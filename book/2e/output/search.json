[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"Welcome website second edition Data Science Command Line Jeroen Janssens, published O’Reilly Media October 2021. website free use. contents licensed Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. can order physical copy Amazon.Want learn Jeroen person? company, Data Science Workshops, Jeroen provides -company training data science command line related topics Python, R, machine learning. Visit Data Science Workshops information.","code":""},{"path":"index.html","id":"description","chapter":"Welcome","heading":"Description","text":"thoroughly revised guide demonstrates flexibility command line can help become efficient productive data scientist. ’ll learn combine small yet powerful command-line tools quickly obtain, scrub, explore, model data. get started, author Jeroen Janssens provides Docker image packed 100 Unix power tools—useful whether work Windows, macOS, Linux.’ll quickly discover command line agile, scalable, extensible technology. Even ’re comfortable processing data Python R, ’ll learn greatly improve data science workflow leveraging command line’s power. book ideal data scientists, analysts, engineers, system administrators, researchers.Obtain data websites, APIs, databases, spreadsheetsPerform scrub operations text, CSV, HTML, XML, JSON filesExplore data, compute descriptive statistics, create visualizationsManage data science workflowCreate tools one-liners existing Python R codeParallelize distribute data-intensive pipelinesModel data dimensionality reduction, regression, classification algorithmsLeverage command line Python, Jupyter, R, RStudio, Apache Spark","code":""},{"path":"index.html","id":"praise","chapter":"Welcome","heading":"Praise","text":"\nTraditional computer data science curricula often mistake command line obsolete relic instead teaching modern vital toolset . well career come grasp elegance power command line easily exploring messy datasets even creating reproducible data pipelines work. first edition Data Science Command Line one comprehensive clear references novice art, now second edition, ’m learning new tools applications .\n\nDan Nguyen, data scientist, former news application developer ProPublica, former Lorry . Lokey Visiting Professor Professional Journalism Stanford University\n\nUnix philosophy simple tools, one job well, cleverly piped together, embodied command line. Jeroen expertly discusses bring philosophy work data science, illustrating command line world file input/output, also world data manipulation, exploration, even modeling.\n\nChris H. Wiggins, associate professor department applied physics applied mathematics Columbia University, chief data scientist New York Times\n\nbook explains integrate common data science tasks coherent workflow. ’s just tactics breaking problems, ’s also strategies assembling pieces solution.\n\nJohn D. Cook, consultant applied mathematics, statistics, technical computing\n\nDespite may hear, practical data science still focused interesting visualizations insights derived flat files. Jeroen’s book leans reality, helps reduce complexity data practitioners showing time-tested command-line tools can repurposed data science.\n\nPaige Bailey, principal product manager code intelligence Microsoft, GitHub\n\n’s amazing fast much data work can performed command line ever pulling data R, Python, database. Older technologies like sed awk still incredibly powerful versatile. read Data Science Command Line, heard tools never saw full power. Thanks Jeroen, ’s like now secret weapon working large data.\n\nJared Lander, chief data scientist Lander Analytics, organizer New York Open Statistical Programming Meetup, author R Everyone\n\ncommand line essential tool every data scientist’s toolbox, knowing well makes easy translate questions data real-time insights. Jeroen explains basic Unix philosophy chain together single-purpose tools arrive simple solutions complex problems, also introduces new command-line tools data cleaning, analysis, visualization, modeling.\n\nJake Hofman, senior principal researcher Microsoft Research, adjunct assistant professor department applied mathematics Columbia University\n","code":""},{"path":"index.html","id":"dedication","chapter":"Welcome","heading":"Dedication","text":"wife, Esther. Without continued encouragement, support,\npatience, second edition surely ended /dev/null.","code":""},{"path":"index.html","id":"about-the-author","chapter":"Welcome","heading":"About the Author","text":"Jeroen Janssens independent data science consultant instructor. enjoys visualizing data, implementing machine learning models, building solutions using Python, R, JavaScript, Bash. Jeroen manages Data Science Workshops, training coaching firm organizes open enrollment workshops, -company courses, inspiration sessions, hackathons, meetups. Previously, \nassistant professor Jheronimus Academy Data Science data scientist Elsevier Amsterdam various startups New York City. Jeroen holds PhD machine learning Tilburg University MSc artificial intelligence Maastricht University. lives wife two kids Rotterdam, Netherlands.\ncan find Jeroen Twitter, GitHub, LinkedIn.","code":""},{"path":"index.html","id":"colophon","chapter":"Welcome","heading":"Colophon","text":"animal cover Data Science Command Line wreathed hornbill (Rhytidoceros undulatus). Also known bar-pouched wreathed hornbill, species found forests mainland Southeast Asia northeastern India Bhutan. Hornbills named casques form upper part birds’ bills. single obvious purpose exists hollow, keratizined structures, may serve means recognition members species, amplifier birds’ calls, —males often exhibit larger casques females species—gender recognition. Wreathed hornbills can distinguished plain-pouched hornbills, closely related otherwise similar appearance, dark bar lower part wreathed hornbills’ throats.Wreathed hornbills roost flocks four hundred mate monogamous, lifelong partnerships. help males, females seal tree cavities behind dung mud lay eggs brood. slit large enough beak alone, male feeds mate young four months. diet animal prey becomes predominantly fruit females young leave nest. Hornbill couples known return nest many nine years.Many animals O’Reilly covers endangered; important world.color illustration Karen Montgomery, based black white engraving Braukhaus’s Lexicon. cover fonts Gilroy Semibold Guardian Sans. text heading font Source Sans Pro code font Fira Mono.","code":""},{"path":"foreword.html","id":"foreword","chapter":"Foreword","heading":"Foreword","text":"love first sight.must around 1981 1982 got first taste Unix.\ncommand-line shell, uses language single commands complex programs, changed world, never looked back.writer discovered joys computing, regular expressions gateway drug.\n’d first tried text editor HP’s RTE operating system, came Unix philosophy small cooperating tools command line shell glue tied together fully understood power.\nRegular expressions ed, ex, vi (now vim), emacs powerful, sure, wasn’t saw ex scripts unbound became sed, Unix stream editor, AWK, allowed bind programmed actions regular expressions, shell scripts let build pipelines existing tools new ones ’d written , really got .\nProgramming speak computers, tell want , just , ways persist, ways can varied like human language, repeatable structure different verbs objects.beginner, forms programming seemed like recipes followed exactly, careful incantations get everything right, like waiting teacher grade essay ’d written.\nshell programming, compilation waiting.\nlike conversation friend.\nfriend didn’t understand, easily try .\n’s , something simple say, just say one word.\nalready words whole lot things might want say.\nweren’t, easily make new words.\nstring words learned words made gradually complex sentences, paragraphs, eventually get persuasive essays.Almost every programming language powerful shell associated tools, least, none provides easier pathway programming mindset, none provides better environment kind everyday conversation machines ask help us work.\nBrian Kernighan, one creators AWK well co-author marvelous book Unix Programming Environment, said 2019 interview Lex Fridman, “[Unix] meant environment really easy write programs.” [00:23:10]\nwent explain often still uses AWK rather writing Python program ’s exploring data.\n“doesn’t scale big programs, pretty darn well little things just want see somethings something.” [00:37:01]Data Science Command Line, Jeroen Janssens demonstrates just powerful Unix/Linux approach command line even today.\nJeroen hadn’t already done , ’d write essay just command line sweet powerful match kinds tasks often encountered data science.\nalready starts book explaining .\n’ll just say : use command line, often find coming back easiest way much work.\nwhether ’re shell newbie, just someone hasn’t thought much great fit shell programming data science, book come treasure.\nJeroen great teacher, material covers priceless.\n—Tim O’Reilly\nMay 2021\n","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"Data science exciting field work .\n’s also still relatively young.\nUnfortunately, many people, many companies well, believe need new technology tackle problems posed data science.\nHowever, book demonstrates, many things can accomplished using command line instead, sometimes much efficient way.PhD program, gradually switched using Microsoft Windows using Linux.\ntransition bit scary first, started operating systems installed next (known dual-boot).\nurge switch back forth Microsoft Windows Linux eventually faded, point even tinkering around Arch Linux, allows build custom Linux machine scratch.\n’re given command line, ’s make .\nnecessity, quickly became comfortable using command line.\nEventually, spare time got precious, settled Linux distribution known Ubuntu ease use large community.\nHowever, command line still ’m spending time.actually wasn’t long ago realized command line just installing software, configuring systems, searching files.\nstarted learning tools cut, sort, sed.\nexamples command-line tools take data input, something , print result.\nUbuntu comes quite .\nunderstood potential combining small tools, hooked.earning PhD, became data scientist, wanted use approach data science much possible.\nThanks couple new, open source command-line tools including xml2json, jq, json2csv, even able use command line tasks scraping websites processing lots JSON data.September 2013, decided write blog post titled Seven Command-line Tools Data Science.\nsurprise, blog post got quite attention, received lot suggestions command-line tools.\nstarted wondering whether blog post turned book.\npleased , 10 months later, help many talented people (see acknowledgments), answer yes.sharing personal story much think know book came , want know learn command line well.\ncommand line different using graphical user interface, can seem scary first.\nlearn , can well.\nmatter current operating system matter currently work data, reading book able data science command line.\n’re already familiar command line, even ’re already dreaming shell scripts, chances ’ll still discover interesting tricks command-line tools use next data science project.","code":""},{"path":"preface.html","id":"what-to-expect-from-this-book","chapter":"Preface","heading":"What to Expect from This Book","text":"book, ’re going obtain, scrub, explore, model data—lot .\nbook much become better data science tasks.\nalready great resources available discuss, example, apply statistical test data can best visualized.\nInstead, practical book aims make efficient productive teaching perform data science tasks command line.book discusses 90 command-line tools, ’s tools matter .\ncommand-line tools around long time, others replaced better ones.\nNew command-line tools created even ’re reading .\nyears, discovered many amazing command-line tools.\nUnfortunately, discovered late included book.\nshort, command-line tools come go.\n’s OK.matters underlying idea working tools, pipes, data.\ncommand-line tools one thing well.\npart Unix philosophy, makes several appearances throughout book.\nbecome familiar command line, know combine command-line tools, can even create new ones, developed invaluable skill.","code":""},{"path":"preface.html","id":"changes-for-the-second-edition","chapter":"Preface","heading":"Changes for the Second Edition","text":"command line technology way working timeless, tools discussed first edition either superseded newer tools (e.g., csvkit largely replaced xsv) abandoned developers (e.g., drake), ’ve suboptimal choices (e.g., weka).\nlearned lot since first edition published October 2014, either experience result useful feedback readers.\nEven though book quite niche lies intersection two subjects, remains steady interest data science community, evidenced many positive messages receive almost every day.\nupdating first edition, hope keep book relevant least another five years.\n’s nonexhaustive list changes made:replaced csvkit xsv much possible. xsv much faster alternative working CSV files.Section 2.2 3.2 replaced VirtualBox image Docker image. Docker faster lightweight way running isolated environment VirtualBox.now use pup instead scrape work HTML. scrape Python tool created . pup much faster, features, easier install.Chapter 6 rewritten scratch. Instead drake now use make project management. drake longer maintained make much mature popular developers.replaced Rio rush. Rio clunky Bash script created . rush R package much stable flexible way using R command line.Chapter 9 replaced Weka BigML Vowpal Wabbit (vw). Weka old way used command line clunky. BigML commercial API longer want rely. Vowpal Wabbit mature machine learning tool, developed Yahoo! now Microsoft.Chapter 10 entirely new chapter integrating command line existing workflows, including Python, R, Apache Spark. first edition mentioned command line can easily integrated existing workflows, never got . chapter fixes .","code":""},{"path":"preface.html","id":"how-to-read-this-book","chapter":"Preface","heading":"How to Read This Book","text":"general, advise read book linear fashion.\nconcept command-line tool introduced, chances employ later chapter.\nexample, Chapter 9, make heavy use parallel, introduced extensively Chapter 8.Data science broad field intersects many fields programming, data visualization, machine learning.\nresult, book touches many interesting topics unfortunately discussed full length.\nThroughout book, end chapter, suggestions exploration.\n’s required read material order follow along book, interested, know ’s much learn.","code":""},{"path":"preface.html","id":"who-this-book-is-for","chapter":"Preface","heading":"Who This Book Is For","text":"book makes just one assumption : work data.\ndoesn’t matter programming language statistical computing environment ’re currently using.\nbook explains necessary concepts beginning.also doesn’t matter whether operating system Microsoft Windows, macOS, flavor Linux.\nbook comes Docker image, easy--install virtual environment.\nallows run command-line tools follow along code examples environment book written.\ndon’t waste time figuring install command-line tools dependencies.book contains code Bash, Python, R, ’s helpful programming experience, ’s means required follow along examples.","code":""},{"path":"preface.html","id":"conventions-used-in-this-book","chapter":"Preface","heading":"Conventions Used in This Book","text":"following typographical conventions used book:ItalicIndicates new terms, URLs, directory names, filenames.\nIndicates new terms, URLs, directory names, filenames.Constant widthUsed code commands, well within paragraphs refer command-line tools options.\nUsed code commands, well within paragraphs refer command-line tools options.Constant width boldShows commands text typed literally user.\nShows commands text typed literally user.","code":""},{"path":"preface.html","id":"acknowledgments","chapter":"Preface","heading":"Acknowledgments","text":"","code":""},{"path":"preface.html","id":"acknowledgments-for-the-second-edition-2021","chapter":"Preface","heading":"Acknowledgments for the Second Edition (2021)","text":"Seven years passed since first edition came .\ntime, especially last 13 months, many people helped .\nWithout , never able write second edition.blessed three wonderful editors O’Reilly.\nlike thank Sarah “Embrace deadline” Grey, Jess “Pedal metal” Haberman, Kate “Let go” Galloway. middle names say . incredible help, able embrace deadlines, put pedal metal mattered, eventually let go.\n’d also like thank colleagues Angela Rufino, Arthur Johnson, Cassandra Furtado, David Futato, Helen Monroe, Karen Montgomery, Kate Dullea, Kristen Brown, Marie Beaugureau, Marsee Henon, Nick Adams, Regina Wilkinson, Shannon Cutt, Shannon Turlington, Yasmina Greco, making collaboration O’Reilly pleasure.Despite automated process execute code paste back results (thanks R Markdown Docker), number mistakes able make impressive.\nThank Aaditya Maruthi, Brian Eoff, Caitlin Hudon, Julia Silge, Mike Dewar, Shane Reustle reducing number immensely.\ncourse, mistakes left responsibility.Marc Canaleta deserves special thank .\nOctober 2014, shortly first edition came , Marc invited give one-day workshop Data Science Command Line team Social Point Barcelona.\nLittle know many workshops follow.\neventually led start company: Data Science Workshops.\nEvery time teach, learn something new.\nprobably don’t know , student impact, one way another, book.\nsay: thank .\nhope can teach long time.Captivating conversations, splendid suggestions, passionate pull requests.\ngreatly appreciate every contribution following generous people:\nAdam Johnson,\nAndre Manook,\nAndrea Borruso,\nAndres Lowrie,\nAndrew Berisha,\nAndrew Gallant,\nAndrew Sanchez,\nAnicet Ebou,\nAnthony Egerton,\nBen Isenhart,\n[.keep-together]#Chris Wiggins#,\nChrys Wu,\nDan Nguyen,\nDarryl Amatsetam,\nDmitriy Rozhkov,\nDoug Needham,\nEdgar Manukyan,\nErik Swan,\nFelienne Hermans,\nGeorge Kampolis,\nGiel van Lankveld,\nGreg Wilson,\nHay Kranen,\nIoannis Cherouvim,\nJake Hofman,\nJannes Muenchow,\nJared Lander,\nJay Roaf,\nJeffrey Perkel,\nJim Hester,\nJoachim Hagege,\nJoel Grus,\nJohn Cook,\nJohn Sandall,\nJoost Helberg,\nJoost van Dijk,\nJoyce Robbins,\nJulian Hatwell,\nKarlo Guidoni,\nKarthik Ram,\nLissa Hyacinth,\nLonghow Lam,\nLui Pillmann,\nLukas Schmid,\nLuke Reding,\nMaarten van Gompel,\nMartin Braun,\nMax Schelker,\nMax Shron,\nNathan Furnal,\nNoah Chase,\nOscar Chic,\nPaige Bailey,\nPeter Saalbrink,\nRich Pauloo,\nRichard Groot,\nRico Huijbers,\nRob Doherty,\nRobbert van Vlijmen,\nRussell Scudder,\nSylvain Lapoix,\nTJ Lavelle,\nTan Long,\nThomas Stone,\nTim O’Reilly,\nVincent Warmerdam, \nYihui Xie.Throughout book, especially footnotes appendix, ’ll find hundreds names.\nnames belong authors many tools, books, resources book stands.\n’m incredibly grateful hard work, regardless whether work done 50 years 50 days ago., like thank wife Esther, daughter Florien, son Olivier reminding daily truly matters.\npromise ’ll years start writing third edition.","code":""},{"path":"preface.html","id":"acknowledgments-for-the-first-edition-2014","chapter":"Preface","heading":"Acknowledgments for the First Edition (2014)","text":"First , ’d like thank Mike Dewar Mike Loukides believing blog post Seven Command-Line Tools Data Science, wrote September 2013, expanded book.Special thanks technical reviewers Mike Dewar, Brian Eoff, Shane Reustle reading various drafts, meticulously testing commands, providing invaluable feedback.\nefforts improved book greatly.\nremaining errors entirely responsibility.privilege working together three amazing editors, namely: Ann Spencer, Julie Steele, Marie Beaugureau.\nThank guidance great liaisons many talented people O’Reilly.\npeople include: Laura Baldwin, Huguette Barriere, Sophia DeMartini, Yasmina Greco, Rachel James, Ben Lorica, Mike Loukides, Christopher Pappas.\nmany others haven’t met operating behind scenes.\nTogether ensured working O’Reilly truly pleasure.book discusses 80 command-line tools.\nNeedless say, without tools, book wouldn’t existed first place.\n’m therefore extremely grateful authors created contributed tools.\ncomplete list authors unfortunately long include ; mentioned Appendix.\nThanks especially Aaron Crow, Jehiah Czebotar, Christoph Groskopf, Dima Kogan, Sergey Lisitsyn, Francisco J.\nMartin, Ole Tange providing help amazing command-line tools.Eric Postma Jaap van den Herik, supervised PhD program, deserve special thank .\ncourse five years taught many lessons.\nAlthough writing technical book quite different writing PhD thesis, many lessons proved helpful past nine months well.Finally, ’d like thank colleagues YPlan, friends, family, especially wife Esther supporting pulling away command line just right times.","code":""},{"path":"chapter-1-introduction.html","id":"chapter-1-introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"book data science command line.\naim make efficient productive data scientist teaching leverage power command line.terms data science command line title requires explanation.\ncan technology 50 years old1 use field years young?Today, data scientists can choose overwhelming collection exciting technologies programming languages.\nPython, R, Julia, Apache Spark examples.\nmay already experience one .\n, still care command line data science?\ncommand line offer technologies programming languages ?valid questions.\nfirst chapter answer questions follows.\nFirst, provide practical definition data science act backbone book.\nSecond, ’ll list five important advantages command line.\nend chapter hope convinced command line indeed worth learning data science.","code":""},{"path":"chapter-1-introduction.html","id":"data-science-is-osemn","chapter":"1 Introduction","heading":"1.1 Data Science is OSEMN","text":"field data science still infancy, , exist various definitions encompasses.\nThroughout book employ practical definition Hilary Mason Chris H. Wiggins2.\ndefine data science according following five steps: (1) obtaining data, (2) scrubbing data, (3) exploring data, (4) modeling data, (5) interpreting data.\nTogether, steps form OSEMN model (pronounced awesome).\ndefinition serves backbone book step, (except step 5, interpreting data, explain ) chapter.Although five steps discussed linear incremental fashion, practice common move back forth perform multiple steps time.\nFigure 1.1 illustrates data science iterative non-linear process.\nexample, modeled data, look results, may decide go back scrubbing step adjust features dataset.\nFigure 1.1: data science iterative non-linear process\nexplain step entails.","code":""},{"path":"chapter-1-introduction.html","id":"obtaining-data","chapter":"1 Introduction","heading":"1.1.1 Obtaining Data","text":"Without data, little data science can .\nfirst step obtaining data.\nUnless fortunate enough already possess data, may need one following:Download data another location (e.g., webpage server)Query data database API (e.g., MySQL Twitter)Extract data another file (e.g., HTML file spreadsheet)Generate data (e.g., reading sensors taking surveys)Chapter 3, discuss several methods obtaining data using command line.\nobtained data likely either plain text, CSV, JSON, HTML, XML format.\nnext step scrub data.","code":""},{"path":"chapter-1-introduction.html","id":"scrubbing-data","chapter":"1 Introduction","heading":"1.1.2 Scrubbing Data","text":"uncommon obtained data missing values, inconsistencies, errors, weird characters, uninteresting columns.\ncase, scrub, clean, data can anything interesting .\nCommon scrubbing operations include:Filtering linesExtracting certain columnsReplacing valuesExtracting wordsHandling missing values duplicatesConverting data one format anotherWhile data scientists love create exciting data visualizations insightful models (steps 3 4), usually much effort goes obtaining scrubbing required data first (steps 1 2).\nData Jujitsu, DJ Patil3 states “80% work data project cleaning data.”\nChapter 5, demonstrate command line can help accomplish data scrubbing operations.","code":""},{"path":"chapter-1-introduction.html","id":"exploring-data","chapter":"1 Introduction","heading":"1.1.3 Exploring Data","text":"scrubbed data, ready explore .\ngets interesting ’re exploring, truly get know data.\nChapter 7 show command line can used :Look dataDerive statistics dataCreate insightful visualizationsCommand-line tools introduced Chapter 7 include: csvstat4 rush5.","code":""},{"path":"chapter-1-introduction.html","id":"modeling-data","chapter":"1 Introduction","heading":"1.1.4 Modeling Data","text":"want explain data predict happen, probably want create statistical model data.\nTechniques create model include clustering, classification, regression, dimensionality reduction.\ncommand line suitable programming new type model scratch.\n, however, useful able build model command line.\nChapter 9 introduce several command-line tools either build model locally employ API perform computation cloud.","code":""},{"path":"chapter-1-introduction.html","id":"interpreting-data","chapter":"1 Introduction","heading":"1.1.5 Interpreting Data","text":"final perhaps important step OSEMN model interpreting data.\nstep involves:Drawing conclusions dataEvaluating results meanCommunicating resultTo honest, computer little use , command line really come play stage.\nreached step, ’s .\nstep OSEMN model chapter.\nInstead, refer book Thinking Data Max Shron6.","code":""},{"path":"chapter-1-introduction.html","id":"intermezzo-chapters","chapter":"1 Introduction","heading":"1.2 Intermezzo Chapters","text":"Besides chapters cover OSEMN steps, four intermezzo chapters.\ndiscusses general topic concerning data science, command line employed .\ntopics applicable step data science process.Chapter 4, discuss create reusable tools command line.\npersonal tools can come long commands typed command line, existing code written , say, Python R.\nable create tools allows become efficient productive.command line interactive environment data science, can become challenging keep track workflow.\nChapter 6, demonstrate command-line tool called make, allows define data science workflow terms tasks dependencies .\ntool increases reproducibility workflow, also colleagues peers.Chapter 8, explain commands tools can sped running parallel.\nUsing command-line tool called GNU Parallel7, can apply command-line tools large datasets run multiple cores even remote machines.Chapter 10, discuss employ power command line environments programming languages R, RStudio, Python, Jupyter Notebooks, even Apache Spark.","code":""},{"path":"chapter-1-introduction.html","id":"what-is-the-command-line","chapter":"1 Introduction","heading":"1.3 What is the Command Line?","text":"discuss use command line data science, let’s take peek command line actually looks like (may already familiar ).\nFigure 1.2 Figure 1.3 show screenshot command line appears default macOS Ubuntu, respectively.\nUbuntu particular distribution GNU/Linux, ’s one ’ll using book.\nFigure 1.2: Command line macOS\n\nFigure 1.3: Command line Ubuntu\nwindow shown two screenshots called terminal.\nprogram enables interact shell.\nshell executes commands type .\nChapter 2, explain two terms detail.Typing commands different way interacting computer graphical user interface (GUI).\nmostly used processing data , say, Microsoft Excel, approach may seem intimidating first.\nDon’t afraid.\nTrust say ’ll get used working command line quickly.book, commands type output generate displayed text.\nexample, contents terminal two screenshots look like :’ll also notice command preceded dollar sign ($).\ncalled prompt.\nprompt two screenshots showed information, namely username, date, penguin.\n’s convention show dollar sign examples, prompt (1) can change session (go different directory), (2) can customized user (e.g., can also show time current git8 branch ’re working ), (3) irrelevant commands .next chapter ’ll explain much essential command-line concepts.\nNow ’s time first explain learn use command line data science.","code":"$ whoami\ndst\n \n$ date\nTue Dec 14 11:43:30 AM CET 2021\n \n$ echo 'The command line is awesome!' | cowsay -f tux\n ______________________________\n< The command line is awesome! >\n ------------------------------\n   \\\n    \\\n        .--.\n       |o_o |\n       |:_/ |\n      //   \\ \\\n     (|     | )\n    /'\\_   _/`\\\n    \\___)=(___/\n \n \n$"},{"path":"chapter-1-introduction.html","id":"why-data-science-at-the-command-line","chapter":"1 Introduction","heading":"1.4 Why Data Science at the Command Line?","text":"command line many great advantages can really make efficient productive data scientist.\nRoughly grouping advantages, command line : agile, augmenting, scalable, extensible, ubiquitous.\nelaborate advantage .","code":""},{"path":"chapter-1-introduction.html","id":"the-command-line-is-agile","chapter":"1 Introduction","heading":"1.4.1 The Command Line is Agile","text":"first advantage command line allows agile.\nData science interactive exploratory nature, environment work needs allow .\ncommand line achieves two means.First, command line provides -called read-eval-print-loop (REPL).\nmeans type command, press Enter, command evaluated immediately.\nREPL often much convenient data science edit-compile-run-debug cycle associated scripts, large programs, , say, Hadoop jobs.\ncommands executed immediately, may stopped , can changed quickly.\nshort iteration cycle really allows play data.Second, command line close file system.\ndata main ingredient data science, important able work easily files contain dataset.\ncommand line offers many convenient tools .","code":""},{"path":"chapter-1-introduction.html","id":"the-command-line-is-augmenting","chapter":"1 Introduction","heading":"1.4.2 The Command Line is Augmenting","text":"command line integrates well technologies.\nWhatever technology data science workflow currently includes (whether ’s R, Python, Excel), please know ’m suggesting abandon workflow.\nInstead, consider command line augmenting technology amplifies technologies ’re currently employing.\ncan three ways.First, command line can act glue many different data science tools.\nOne way glue tools connecting output first tool input second tool.\nChapter 2 explain works.Second, can often delegate tasks command line environment.\nexample, Python, R, Apache Spark allow run command-line tools capture output.\ndemonstrate examples Chapter 10.Third, can convert code (e.g., Python R script) reusable command-line tool.\nway, doesn’t matter anymore language ’s written.\nNow, can used command line directly environment integrates command line mentioned previous paragraph.\nexplain Chapter 4.end, every technology strengths weaknesses, ’s good know several use whichever appropriate task hand.\nSometimes means using R, sometimes command line, sometimes even pen paper.\nend book ’ll solid understanding use command line, ’re better continuing favorite programming language statistical computing environment.","code":""},{"path":"chapter-1-introduction.html","id":"the-command-line-is-scalable","chapter":"1 Introduction","heading":"1.4.3 The Command Line is Scalable","text":"’ve said , working command line different using GUI.\ncommand line things typing, whereas GUI, things pointing clicking mouse.Everything type manually command line can also automated scripts tools.\nmakes easy re-run commands case made mistake, input data changed, colleague wants perform analysis.\nMoreover, commands can run specific intervals, remote server, parallel many chunks data (Chapter 8).command line automatable, becomes scalable repeatable.\n’s straightforward automate pointing clicking, makes GUI less suitable environment scalable repeatable data science.","code":""},{"path":"chapter-1-introduction.html","id":"the-command-line-is-extensible","chapter":"1 Introduction","heading":"1.4.4 The Command Line is Extensible","text":"command line invented 50 years ago.\ncore functionality largely remained unchanged, tools, workhorses command-line, developed daily basis.command line language-agnostic.\nallows command-line tools written many different programming languages.\nopen source community producing many free high-quality command-line tools can use data science.command-line tools can work together, makes command line flexible.\ncan also create tools, allowing extending effective functionality command line.","code":""},{"path":"chapter-1-introduction.html","id":"the-command-line-is-ubiquitous","chapter":"1 Introduction","heading":"1.4.5 The Command Line is Ubiquitous","text":"command line comes Unix-like operating system, including Ubuntu Linux macOS, can found many places.\nPlus, 100% top 500 supercomputers running Linux.9\n, ever get hands one supercomputers (ever find Jurassic Park doorlocks working), better know way around command line!Linux runs supercomputers.\nalso runs servers, laptops, embedded systems.\ndays, many companies offer cloud computing, can easily launch new machines fly.\never log machine (server general), ’s almost certain ’ll arrive command line.’s also important note command line isn’t just hype.\ntechnology around five decades, ’m convinced ’s stay another five.\nLearning use command line (data science general) therefore worthwhile investment.","code":""},{"path":"chapter-1-introduction.html","id":"summary","chapter":"1 Introduction","heading":"1.5 Summary","text":"chapter introduced OSEMN model data science, use guide throughout book.\nprovided background Unix command line hopefully convinced ’s suitable environment data science.\nnext chapter ’m going show get started installing datasets tools explaining fundamental concepts.","code":""},{"path":"chapter-1-introduction.html","id":"for-further-exploration","chapter":"1 Introduction","heading":"1.6 For Further Exploration","text":"book UNIX: History Memoir Brian W. Kernighan tells story Unix, explaining , developed, matters.2018 gave presentation titled 50 Reasons Learn Shell Data Science Strata London. can read slides need even convincing.short sweet book Thinking Data Max Shron focuses instead provides framework defining data science project help ask right questions solve right problems.","code":""},{"path":"chapter-2-getting-started.html","id":"chapter-2-getting-started","chapter":"2 Getting Started","heading":"2 Getting Started","text":"chapter ’m going make sure prerequisites data science command line.\nprerequisites fall three parts: (1) data sets use book, (2) proper environment command-line tools use throughout book, (3) understanding essential concepts come play using command line.First, describe download datasets.\nSecond, explain install Docker image, virtual environment based Ubuntu Linux contains necessary command-line tools.\nSubsequently, go essential Unix concepts examples.end chapter, ’ll everything need order continue first step data science, namely obtaining data.","code":""},{"path":"chapter-2-getting-started.html","id":"getting-the-data","chapter":"2 Getting Started","heading":"2.1 Getting the Data","text":"datasets use book can downloaded follows:Download ZIP file https://www.datascienceatthecommandline.com/2e/data.zip.Create new directory. can give directory name want, recommend stick lowercase letters, numbers, maybe hyphen underscore ’s easier work command line. example: dsatcl2e-data. Remember directory .Move ZIP file new directory unpack .directory now contains one subdirectory per chapter.next section explain install environment containing command-line tools work data.","code":""},{"path":"chapter-2-getting-started.html","id":"docker-image","chapter":"2 Getting Started","heading":"2.2 Installing the Docker Image","text":"book use many different command-line tools.\nUnix often comes lot command-line tools pre-installed offers many packages contain relevant tools.\nInstalling packages often difficult.\nHowever, ’ll also use tools available packages require manual, involved installation.\norder acquire necessary command-line tools without go installation process , encourage , whether ’re Windows, macOS, Linux, install Docker image created specifically book.Docker image bundle one applications together dependencies.\nDocker container isolated environment runs image.\ncan manage Docker images containers docker command-line tool (’ll ) Docker GUI.\nway, Docker container like virtual machine, Docker container uses far fewer resources.\nend chapter suggest resources learn Docker.install Docker image, first need download install Docker Docker website.\nDocker installed, invoke following command terminal command prompt download Docker image (don’t type dollar sign):can run Docker image follows:’re now inside isolated environment—known Docker container—necessary command-line tools installed.\nfollowing command produces enthusiastic cow, know everything working correctly:want get data container, can add volume, means local directory gets mapped directory inside container.\nrecommend first create new directory, navigate new directory, run following ’re macOS Linux:following ’re Windows using Command Prompt (also known cmd):following ’re using Windows PowerShell:commands, option -v instructs docker map current directory /data directory inside container, place get data Docker container.’re done, can shut Docker container typing exit.","code":"$ docker pull datasciencetoolbox/dsatcl2e$ docker run --rm -it datasciencetoolbox/dsatcl2e$ cowsay \"Let's moove\\!\"\n ______________\n< Let's moove! >\n --------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||$ docker run --rm -it -v \"$(pC:\\> docker run --rm -it -v \"%cd%\":/data datasciencetoolbox/dsatcl2ePS C:\\> docker run --rm -it -v ${PWD}:/data datasciencetoolbox/dsatcl2e"},{"path":"chapter-2-getting-started.html","id":"essential-concepts","chapter":"2 Getting Started","heading":"2.3 Essential Unix Concepts","text":"Chapter 1, briefly showed command line .\nNow running Docker image, can really get started.\nsection, discuss several concepts tools need know order feel comfortable data science command line.\n, now, mainly working graphical user interfaces, might quite change.\ndon’t worry, ’ll start beginning, gradually go advanced topics.","code":""},{"path":"chapter-2-getting-started.html","id":"the-environment","chapter":"2 Getting Started","heading":"2.3.1 The Environment","text":"’ve just logged brand new environment.\nanything, ’s worthwhile get high-level understanding environment.\n’s roughly defined four layers, briefly discuss top .Command-line toolsFirst foremost, command-line tools work .\nuse typing corresponding commands.\ndifferent types command-line tools, discuss next section.\nExamples tools : ls10, cat11, jq12.\nFirst foremost, command-line tools work .\nuse typing corresponding commands.\ndifferent types command-line tools, discuss next section.\nExamples tools : ls10, cat11, jq12.TerminalThe terminal, second layer, application type commands . see following text mentioned book:\n$ seq 3\n1\n2\n3\ntype seq 3 terminal press Enter.\n(command-line tool seq13, can see, generates sequence numbers.) type dollar sign.\n’s just tell command can type terminal.\ndollar sign known prompt.\ntext seq 3 output command.\nterminal, second layer, application type commands . see following text mentioned book:type seq 3 terminal press Enter.\n(command-line tool seq13, can see, generates sequence numbers.) type dollar sign.\n’s just tell command can type terminal.\ndollar sign known prompt.\ntext seq 3 output command.ShellThe third layer shell. typed command pressed Enter, terminal sends command shell. shell program interprets command. use Z shell, many others available Bash Fish.\nthird layer shell. typed command pressed Enter, terminal sends command shell. shell program interprets command. use Z shell, many others available Bash Fish.Operating systemThe fourth layer operating system, GNU/Linux case. Linux name kernel, heart operating system. kernel direct contact CPU, disks, hardware. kernel also executes command-line tools. GNU, stands GNU’s UNIX, refers set basic tools. Docker image based particular GNU/Linux distribution called Ubuntu.\nfourth layer operating system, GNU/Linux case. Linux name kernel, heart operating system. kernel direct contact CPU, disks, hardware. kernel also executes command-line tools. GNU, stands GNU’s UNIX, refers set basic tools. Docker image based particular GNU/Linux distribution called Ubuntu.","code":"$ seq 3\n1\n2\n3"},{"path":"chapter-2-getting-started.html","id":"executing-a-command-line-tool","chapter":"2 Getting Started","heading":"2.3.2 Executing a Command-line Tool","text":"Now basic understanding environment, high time try commands.\nType following terminal (without dollar sign) press Enter:just executed command contained single command-line tool.\ntool pwd14 outputs name directory currently .\ndefault, login, home directory.command-line tool cd, Z shell builtin, allows navigate different directory:➊ Navigate directory /data/ch02.\n➋ Print current directory.\n➌ Navigate parent directory.\n➍ Print current directory .\n➎ Navigate subdirectory ch02.part cd specifies directory want navigate .\nValues come command called command-line arguments options.\nTwo dots refer parent directory.\nOne dot, way, refers current directory.\ncd . wouldn’t effect, ’ll still see one dot used places.\nLet’s try different command:pass three command-line arguments head15.\nfirst one option.\nused short option -n.\nSometimes short option long variant, --lines case.\nsecond one value belongs option.\nthird one filename.\nparticular command outputs first three lines file /data/ch02/movies.txt.","code":"$ pwd\n/home/dst$ cd /data/ch02 ➊\n \n$ pwd ➋\n/data/ch02\n \n$ cd .. ➌\n \n$ pwd ➍\n/data\n \n$ cd ch02 ➎$ head -n 3 movies.txt\nMatrix\nStar Wars\nHome Alone"},{"path":"chapter-2-getting-started.html","id":"five-types-of-command-line-tools","chapter":"2 Getting Started","heading":"2.3.3 Five Types of Command-line Tools","text":"use term command-line tool lot, far, haven’t yet explained actually mean .\nuse umbrella term anything can executed command line (see Figure 2.1).\nhood, command-line tool one following five types:binary executableA shell builtinAn interpreted scriptA shell functionAn alias\nFigure 2.1: use term command-line tool umbrella term\n’s good know difference types.\ncommand-line tools come pre-installed Docker image mostly comprise first two types (binary executable shell builtin).\nthree types (interpreted script, shell function, alias) allow us build data science toolbox become efficient productive data scientists.Binary ExecutableBinary executables programs classical sense. binary executable created compiling source code machine code. means open file text editor read .\nBinary executables programs classical sense. binary executable created compiling source code machine code. means open file text editor read .Shell BuiltinShell builtins command-line tools provided shell, Z shell (zsh) case. Examples include cd pwd. Shell builtins may differ shells. Like binary executables, easily inspected changed.\nShell builtins command-line tools provided shell, Z shell (zsh) case. Examples include cd pwd. Shell builtins may differ shells. Like binary executables, easily inspected changed.Interpreted ScriptAn interpreted script text file executed binary executable. Examples include: Python, R, Bash scripts. One great advantage interpreted script can read change . script interpreted Python file extension .py, first line script defines binary execute .\n$ bat fac.py\n───────┬──────────────────────────────────────────────────────────────\n       │ File: fac.py\n───────┼──────────────────────────────────────────────────────────────\n   1   │ #!/usr/bin/env python\n   2   │\n   3   │ def factorial(x):\n   4   │     result = 1\n   5   │     range(2, x + 1):\n   6   │         result *= \n   7   │     return result\n   8   │\n   9   │ __name__ == \"__main__\":\n  10   │     import sys\n  11   │     x = int(sys.argv[1])\n  12   │     sys.stdout.write(f\"{factorial(x)}\\n\")\n───────┴──────────────────────────────────────────────────────────────\nscript computes factorial integer pass parameter. can invoked command line follows:\n$ ./fac.py 5\n120\nChapter 4, ’ll discuss great detail create reusable command-line tools using interpreted scripts.\ninterpreted script text file executed binary executable. Examples include: Python, R, Bash scripts. One great advantage interpreted script can read change . script interpreted Python file extension .py, first line script defines binary execute .script computes factorial integer pass parameter. can invoked command line follows:Chapter 4, ’ll discuss great detail create reusable command-line tools using interpreted scripts.Shell FunctionA shell function function , case, executed zsh. provide similar functionality script, usually (necessarily) smaller scripts. also tend personal. following command defines function called fac, , just like interpreted Python script , computes factorial integer pass parameter. generating list numbers using seq, putting numbers one line * delimiter using paste16, passing equation bc17, evaluates outputs result.\n$ fac() { (echo 1; seq $1) | paste -s -d\\* - | bc; }\n\n$ fac 5\n120\nfile ~/.zshrc, configuration file Z shell, good place define shell functions, always available.\nshell function function , case, executed zsh. provide similar functionality script, usually (necessarily) smaller scripts. also tend personal. following command defines function called fac, , just like interpreted Python script , computes factorial integer pass parameter. generating list numbers using seq, putting numbers one line * delimiter using paste16, passing equation bc17, evaluates outputs result.file ~/.zshrc, configuration file Z shell, good place define shell functions, always available.AliasAliases like macros. often find executing certain command parameters (part ), can define alias save time. Aliases also useful continue misspell certain command (Chris Wiggins maintains useful list aliases). following command defines alias:\n$ alias l='ls --color -lhF --group-directories-first'\n\n$ alias les=less\nNow, type following command line, shell replace alias finds value:\n$ cd /data\n\n$ l\ntotal 40K\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch02/\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch03/\ndrwxr-xr-x 3 dst dst 4.0K Dec 14 11:43 ch04/\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch05/\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch06/\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch07/\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch08/\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch09/\ndrwxr-xr-x 4 dst dst 4.0K Dec 14 11:43 ch10/\ndrwxr-xr-x 3 dst dst 4.0K Dec 14 11:43 csvconf/\n\n$ cd ch02\nAliases simpler shell functions don’t allow parameters. function fac defined using alias parameter. Still, aliases allow save lots keystrokes. Like shell functions, aliases often defined file .zshrc, located home directory. see aliases currently defined, run alias without arguments. Try . see?\nAliases like macros. often find executing certain command parameters (part ), can define alias save time. Aliases also useful continue misspell certain command (Chris Wiggins maintains useful list aliases). following command defines alias:Now, type following command line, shell replace alias finds value:Aliases simpler shell functions don’t allow parameters. function fac defined using alias parameter. Still, aliases allow save lots keystrokes. Like shell functions, aliases often defined file .zshrc, located home directory. see aliases currently defined, run alias without arguments. Try . see?book ’ll focus mostly last three types command-line tools: interpreted scripts, shell functions, aliases.\ncan easily changed.\npurpose command-line tool make life easier, make productive efficient data scientist.\ncan find type command-line tool type (shell builtin):type returns three command-line tools pwd.\ncase, first reported command-line tool used type pwd.\nnext section ’ll look combine command-line tools.","code":"$ bat fac.py\n───────┬──────────────────────────────────────────────────────────────\n       │ File: fac.py\n───────┼──────────────────────────────────────────────────────────────\n   1   │ #!/usr/bin/env python\n   2   │\n   3   │ def factorial(x):\n   4   │     result = 1\n   5   │     for i in range(2, x + 1):\n   6   │         result *= i\n   7   │     return result\n   8   │\n   9   │ if __name__ == \"__main__\":\n  10   │     import sys\n  11   │     x = int(sys.argv[1])\n  12   │     sys.stdout.write(f\"{factorial(x)}\\n\")\n───────┴──────────────────────────────────────────────────────────────$ ./fac.py 5\n120$ fac() { (echo 1; seq $1) | paste -s -d\\* - | bc; }\n\n$ fac 5\n120$ alias l='ls --color -lhF --group-directories-first'\n\n$ alias les=less$ cd /data\n\n$ l\ntotal 40K\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch02/\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch03/\ndrwxr-xr-x 3 dst dst 4.0K Dec 14 11:43 ch04/\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch05/\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch06/\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch07/\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch08/\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch09/\ndrwxr-xr-x 4 dst dst 4.0K Dec 14 11:43 ch10/\ndrwxr-xr-x 3 dst dst 4.0K Dec 14 11:43 csvconf/\n\n$ cd ch02$ type -a pwd\npwd is a shell builtin\npwd is /usr/bin/pwd\npwd is /bin/pwd\n \n$ type -a cd\ncd is a shell builtin\n \n$ type -a fac\nfac is a shell function\n \n$ type -a l\nl is an alias for ls --color -lhF --group-directories-first"},{"path":"chapter-2-getting-started.html","id":"combining-command-line-tools","chapter":"2 Getting Started","heading":"2.3.4 Combining Command-line Tools","text":"command-line tools adhere Unix philosophy18, designed thing, really well.\nexample, command-line tool grep19 can filter lines, wc20 can count lines, sort21 can sort lines.\npower command line comes ability combine small, yet powerful command-line tools.power made possible managing communication streams tools.\ntool three standard communication streams: standard input, standard output, standard error.\noften abbreviated stdin, stdout, stderr.standard output standard error , default, redirected terminal, normal output error messages printed screen.\nFigure 2.2 illustrates pwd rev22.\nrun rev, ’ll see nothing happens.\n’s rev expects input, default, ’s keys pressed keyboard.\nTry typing sentence press Enter.\nrev immediately responds input reverse.\ncan stop sending input pressing Ctrl-D rev stop.\nFigure 2.2: Every tool three standard streams: standard input (stdin), standard output (stdout), standard error (stderr)\npractice, ’ll use keyboard source input, output generated tools contents files.\nexample, curl can download book Alice’s Adventures Wonderland Lewis Carrol pipe next tool.\n(’ll discuss curl detail Chapter 3.)\ndone using pipe operator (|).\nFigure 2.3: output tool can piped another tool\ncan pipe output curl grep filter lines pattern.\nImagine want see chapters listed table contents:\ncan combine curl grep follows:wanted know many chapters book , can use wc, good counting things:➊ option -l specifies wc output number lines pass . default also returns number characters words.can think piping automated copy paste.\nget hang combining tools using pipe operator, ’ll find virtually limits .","code":"$ curl -s \"https://www.gutenberg.org/files/11/11-0.txt\" | grep \" CHAPTER\"\n CHAPTER I.     Down the Rabbit-Hole\n CHAPTER II.    The Pool of Tears\n CHAPTER III.   A Caucus-Race and a Long Tale\n CHAPTER IV.    The Rabbit Sends in a Little Bill\n CHAPTER V.     Advice from a Caterpillar\n CHAPTER VI.    Pig and Pepper\n CHAPTER VII.   A Mad Tea-Party\n CHAPTER VIII.  The Queen’s Croquet-Ground\n CHAPTER IX.    The Mock Turtle’s Story\n CHAPTER X.     The Lobster Quadrille\n CHAPTER XI.    Who Stole the Tarts?\n CHAPTER XII.   Alice’s Evidence$ curl -s \"https://www.gutenberg.org/files/11/11-0.txt\" |\n> grep \" CHAPTER\" |\n> wc -l ➊\n12"},{"path":"chapter-2-getting-started.html","id":"redirecting-input-and-output","chapter":"2 Getting Started","heading":"2.3.5 Redirecting Input and Output","text":"Besides piping output one tool another tool, can also save file.\nfile saved current directory, unless full path given.\ncalled output redirection, works follows:, save output grep file named chapters.txt directory /data/ch02.\nfile exist yet, created.\nfile already exists, contents overwritten.\nFigure 2.4 illustrates output redirection works conceptually.\nNote standard error still redirected terminal.\nFigure 2.4: output tool can redirected file\ncan also append output file >>, meaning output added original contents:tool echo outputs value specify.\n-n option, stands newline, specifies echo output trailing newline.Saving output file useful need store intermediate results, example continue analysis later stage.\nuse contents file greeting.txt , can use cat, reads file prints .➊ -w option indicates wc count words.result can achieved using smaller--sign (<):way, directly passing file standard input wc without running additional process23.\nFigure 2.5 illustrates two ways work.\n, final output .\nFigure 2.5: Two ways use contents file input\nLike many command-line tools, wc allows one filenames specified arguments.\nexample:Note case, wc also outputs name files.can suppress output tool redirecting special file called /dev/null.\noften suppress error messages (see Figure 2.6 illustration).\nfollowing causes cat produce error message find file 404.txt:can redirect standard error /dev/null follows:➊ 2 refers standard error.\nFigure 2.6: Redirecting stderr /dev/null\ncareful read write file.\n, ’ll end empty file.\n’s tool output redirected, immediately opens file writing, thereby emptying .\ntwo workarounds : (1) write different file rename afterwards mv (2) use sponge24, soaks input writing file.\nFigure 2.7 illustrates works.\nFigure 2.7: Unless use sponge, read write file one pipeline\nexample, imagine used dseq25 generate file dates.txt now ’d like add line numbers using nl26.\nrun following, file dates.txt end empty.Instead, can use one workarounds just described:","code":"$ curl \"https://www.gutenberg.org/files/11/11-0.txt\" | grep \" CHAPTER\" > chapter\ns.txt\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  170k  100  170k    0     0   202k      0 --:--:-- --:--:-- --:--:--  203k\n \n$ cat chapters.txt\n CHAPTER I.     Down the Rabbit-Hole\n CHAPTER II.    The Pool of Tears\n CHAPTER III.   A Caucus-Race and a Long Tale\n CHAPTER IV.    The Rabbit Sends in a Little Bill\n CHAPTER V.     Advice from a Caterpillar\n CHAPTER VI.    Pig and Pepper\n CHAPTER VII.   A Mad Tea-Party\n CHAPTER VIII.  The Queen’s Croquet-Ground\n CHAPTER IX.    The Mock Turtle’s Story\n CHAPTER X.     The Lobster Quadrille\n CHAPTER XI.    Who Stole the Tarts?\n CHAPTER XII.   Alice’s Evidence$ echo -n \"Hello\" > greeting.txt\n \n$ echo \" World\" >> greeting.txt$ cat greeting.txt\nHello World\n \n$ cat greeting.txt | wc -w ➊\n2$ < greeting.txt wc -w\n2$ wc -w greeting.txt movies.txt\n 2 greeting.txt\n11 movies.txt\n13 total$ cat movies.txt 404.txt\nMatrix\nStar Wars\nHome Alone\nIndiana Jones\nBack to the Future\n/usr/bin/cat: 404.txt: No such file or directory$ cat movies.txt 404.txt 2> /dev/null ➊\nMatrix\nStar Wars\nHome Alone\nIndiana Jones\nBack to the Future$ dseq 5 > dates.txt\n \n$ < dates.txt nl > dates.txt\n \n$ bat dates.txt\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: dates.txt   <EMPTY>\n───────┴────────────────────────────────────────────────────────────────────────$ dseq 5 > dates.txt\n\n$ < dates.txt nl > dates-nl.txt\n \n$ bat dates-nl.txt\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: dates-nl.txt\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │      1  2021-12-15\n   2   │      2  2021-12-16\n   3   │      3  2021-12-17\n   4   │      4  2021-12-18\n   5   │      5  2021-12-19\n───────┴────────────────────────────────────────────────────────────────────────\n \n$ dseq 5 > dates.txt\n\n$ < dates.txt nl | sponge dates.txt\n \n$ bat dates.txt\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: dates.txt\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │      1  2021-12-15\n   2   │      2  2021-12-16\n   3   │      3  2021-12-17\n   4   │      4  2021-12-18\n   5   │      5  2021-12-19\n───────┴────────────────────────────────────────────────────────────────────────"},{"path":"chapter-2-getting-started.html","id":"working-with-files-and-directories","chapter":"2 Getting Started","heading":"2.3.6 Working With Files and Directories","text":"data scientists, work lot data.\ndata often stored files.\nimportant know work files (directories live ) command line.\nEvery action can using GUI, can command-line tools (much ).\nsection introduce important ones list, create, move, copy, rename, delete files directories.Listing contents directory can done ls.\ndon’t specify directory, lists contents current directory.\nprefer ls long listing format directories grouped files.\nInstead typing corresponding options time, use alias l.already seen can create new files redirecting output either > >>.\nneed move file different directory can use mv27:can also rename files mv:can also rename move entire directories.\nlonger need file, delete (remove) rm28:want remove entire directory contents, specify -r option, stands recursive:want copy file, use cp29.\nuseful creating backups:can create directories using mkdir30:command-line tools accept -v option, stands verbose, output ’s going .\nexample:tools mkdir also accept -option, stands interactive, causes tools ask confirmation.\nexample:","code":"$ ls /data/ch10\nalice.txt  count.py  count.R  __pycache__  Untitled1337.ipynb\n \n$ alias l\nl='ls --color -lhF --group-directories-first'\n \n$ l /data/ch10\ntotal 180K\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 __pycache__/\n-rw-r--r-- 1 dst dst 164K Dec 14 11:43 alice.txt\n-rwxr--r-- 1 dst dst  408 Dec 14 11:43 count.py*\n-rw-r--r-- 1 dst dst  460 Dec 14 11:43 count.R\n-rw-r--r-- 1 dst dst 1.7K Dec 14 11:43 Untitled1337.ipynb$ mv hello.txt /data/ch02$ cd data\n$ mv hello.txt bye.txt$ rm bye.txt$ rm -r /data/ch02/old$ cp server.log server.log.bak$ cd /data\n \n$ mkdir logs\n \n$ l\ntotal 44K\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch02/\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch03/\ndrwxr-xr-x 3 dst dst 4.0K Dec 14 11:43 ch04/\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch05/\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch06/\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch07/\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch08/\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:43 ch09/\ndrwxr-xr-x 4 dst dst 4.0K Dec 14 11:43 ch10/\ndrwxr-xr-x 3 dst dst 4.0K Dec 14 11:43 csvconf/\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 11:44 logs/$ mkdir -v backup\n/usr/bin/mkdir: created directory 'backup'\n \n$ cp -v * backup\n/usr/bin/cp: -r not specified; omitting directory 'backup'\n/usr/bin/cp: -r not specified; omitting directory 'ch02'\n/usr/bin/cp: -r not specified; omitting directory 'ch03'\n/usr/bin/cp: -r not specified; omitting directory 'ch04'\n/usr/bin/cp: -r not specified; omitting directory 'ch05'\n/usr/bin/cp: -r not specified; omitting directory 'ch06'\n/usr/bin/cp: -r not specified; omitting directory 'ch07'\n/usr/bin/cp: -r not specified; omitting directory 'ch08'\n/usr/bin/cp: -r not specified; omitting directory 'ch09'\n/usr/bin/cp: -r not specified; omitting directory 'ch10'\n/usr/bin/cp: -r not specified; omitting directory 'csvconf'\n/usr/bin/cp: -r not specified; omitting directory 'logs'$ rm -i *\nzsh: sure you want to delete all 12 files in /data [yn]? n"},{"path":"chapter-2-getting-started.html","id":"managing-output","chapter":"2 Getting Started","heading":"2.3.7 Managing Output","text":"Sometimes tools sequence tools produces much output include book.\nInstead manually altering output, prefer transparent piping helper tool.\ndon’t necessarily , especially ’re interested complete output.tools use making output manageable:often use trim limit output given height width.\ndefault, output trimmed 10 lines width terminal.\nPass negative number disable trimming height width.\nexample:tools use massage output : head, tail, fold, paste, column.\nappendix contains examples .output comma-separated values, often pipe throughcsvlook turn nice-looking table.\nrun csvlook, ’ll see complete table.\nredefined csvlook table shortened trim:use bat show contents file line numbers syntax highlighting matters.\nexample source code:Sometimes add -option want explicitly point spaces, tabs, newlines file.Sometimes ’s useful write intermediate output file.\nallows inspect step pipeline completed.\ncan insert tool tee often like pipeline.\noften use inspect portion final output, writing complete output file (see Figure 2.8.\n, complete output written even.txt first 5 lines printed using trim:\nFigure 2.8: tee, can write intermediate output file\nLastly, insert images generated command-line tools (every image except screenshots diagrams) use display.\nrun display ’ll find doesn’t work.\nChapter 7, explain four options display generated images command line.","code":"$ cat /data/ch07/tips.csv | trim 5 25\nbill,tip,sex,smoker,day,…\n16.99,1.01,Female,No,Sun…\n10.34,1.66,Male,No,Sun,D…\n21.01,3.5,Male,No,Sun,Di…\n23.68,3.31,Male,No,Sun,D…\n… with 240 more lines$ which csvlook\ncsvlook () {\n        /usr/bin/csvlook \"$@\" | trim | sed 's/- | -/──┼──/g;s/| -/├──/g;s/- |/──\n┤/;s/|/│/g;2s/-/─/g'\n}\n \n$ csvlook /data/ch07/tips.csv\n│  bill │   tip │ sex    │ smoker │ day  │ time   │ size │\n├───────┼───────┼────────┼────────┼──────┼────────┼──────┤\n│ 16.99 │  1.01 │ Female │  False │ Sun  │ Dinner │    2 │\n│ 10.34 │  1.66 │ Male   │  False │ Sun  │ Dinner │    3 │\n│ 21.01 │  3.50 │ Male   │  False │ Sun  │ Dinner │    3 │\n│ 23.68 │  3.31 │ Male   │  False │ Sun  │ Dinner │    2 │\n│ 24.59 │  3.61 │ Female │  False │ Sun  │ Dinner │    4 │\n│ 25.29 │  4.71 │ Male   │  False │ Sun  │ Dinner │    4 │\n│  8.77 │  2.00 │ Male   │  False │ Sun  │ Dinner │    2 │\n│ 26.88 │  3.12 │ Male   │  False │ Sun  │ Dinner │    4 │\n… with 236 more lines$ bat /data/ch04/stream.py\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: /data/ch04/stream.py\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ #!/usr/bin/env python\n   2   │ from sys import stdin, stdout\n   3   │ while True:\n   4   │     line = stdin.readline()\n   5   │     if not line:\n   6   │         break\n   7   │     stdout.write(\"%d\\n\" % int(line)**2)\n   8   │     stdout.flush()\n───────┴────────────────────────────────────────────────────────────────────────$ seq 0 2 100 | tee even.txt | trim 5\n0\n2\n4\n6\n8\n… with 46 more lines"},{"path":"chapter-2-getting-started.html","id":"help","chapter":"2 Getting Started","heading":"2.3.8 Help!","text":"’re finding way around command-line, may happen need help.\nEven seasoned users need help point.\nimpossible remember different command-line tools possible arguments.\nFortunately, command line offers severals ways get help.important command get help perhaps man31, short manual.\ncontains information command-line tools.\ncase forgot options tool tar, happens time, just access manual page using:every command-line tool manual page.\nTake cd example:shell builtins like cd can consult zshbuiltins manual page:can search pressing / exit pressing q.\nTry find appropriate section cd.Newer command-line tools often lack manual page well.\ncase, best bet invoke tool --help (-h) option.\nexample:Specifying --help option also works command-line tools cat.\nHowever, corresponding man page often provides information.\n, trying three approaches, still stuck, perfectly acceptable consult Internet.\nappendix, ’s list command-line tools used book.\nBesides command-line tool can installed, also shows can get help.Manual pages can quite verbose difficult read.\ntool tldr32 collection community-maintained help pages command-line tools, aims simpler, approachable complement traditional manual pages.\n’s example showing tldr page tar:can see, rather listing many options alphabetically like man often , tldr cuts chase giving list practical examples.","code":"$ man tar | trim 20\nTAR(1)                          GNU TAR Manual                          TAR(1)\n \nNAME\n       tar - an archiving utility\n \nSYNOPSIS\n   Traditional usage\n       tar {A|c|d|r|t|u|x}[GnSkUWOmpsMBiajJzZhPlRvwo] [ARG...]\n \n   UNIX-style usage\n       tar -A [OPTIONS] ARCHIVE ARCHIVE\n \n       tar -c [-f ARCHIVE] [OPTIONS] [FILE...]\n \n       tar -d [-f ARCHIVE] [OPTIONS] [FILE...]\n \n       tar -t [-f ARCHIVE] [OPTIONS] [MEMBER...]\n \n       tar -r [-f ARCHIVE] [OPTIONS] [FILE...]\n \n… with 1147 more lines$ man cd\nNo manual entry for cd$ man zshbuiltins | trim\nZSHBUILTINS(1)              General Commands Manual             ZSHBUILTINS(1)\n \nNAME\n       zshbuiltins - zsh built-in commands\n \nSHELL BUILTIN COMMANDS\n       Some shell builtin commands take options as described in individual en‐\n       tries; these are often referred to in the  list  below  as  `flags'  to\n       avoid  confusion  with  shell options, which may also have an effect on\n       the behaviour of builtin commands.  In this introductory section,  `op‐\n… with 2735 more lines$ jq --help | trim\njq - commandline JSON processor [version 1.6]\n \nUsage:  /usr/bin/jq [options] <jq filter> [file...]\n        /usr/bin/jq [options] --args <jq filter> [strings...]\n        /usr/bin/jq [options] --jsonargs <jq filter> [JSON_TEXTS...]\n \njq is a tool for processing JSON inputs, applying the given filter to\nits JSON text inputs and producing the filter's results as JSON on\nstandard output.\n \n… with 37 more lines$ tldr tar | trim 20\n \n  tar\n \n  Archiving utility.\n  Often combined with a compression method, such as gzip or bzip2.\n  More information: https://www.gnu.org/software/tar.\n \n  - [c]reate an archive and write it to a [f]ile:\n    tar cf target.tar file1 file2 file3\n \n  - [c]reate a g[z]ipped archive and write it to a [f]ile:\n    tar czf target.tar.gz file1 file2 file3\n \n  - [c]reate a g[z]ipped archive from a directory using relative paths:\n    tar czf target.tar.gz --directory=path/to/directory .\n \n  - E[x]tract a (compressed) archive [f]ile into the current directory [v]erbos…\n    tar xvf source.tar[.gz|.bz2|.xz]\n \n  - E[x]tract a (compressed) archive [f]ile into the target directory:\n… with 12 more lines"},{"path":"chapter-2-getting-started.html","id":"summary-1","chapter":"2 Getting Started","heading":"2.4 Summary","text":"chapter learned get required command-line tools installing Docker image.\nalso went essential command-line concepts get help.\nNow necessary ingredients, ’re ready first step OSEMN model data science: obtaining data.","code":""},{"path":"chapter-2-getting-started.html","id":"for-further-exploration-1","chapter":"2 Getting Started","heading":"2.5 For Further Exploration","text":"subtitle book pays homage epic Unix Power Tools Jerry Peek, Shelley Powers, Tim O’Reilly, Mike Loukides. rightly . 51 chapters thousand pages, covers just everything know Unix. weighs 4 pounds, might want consider getting ebook.website explainshell parses command sequence commands provides short explanation part. Useful quickly understanding new command option without skim relevant manual pages.Docker truly brilliant piece software. chapter ’ve briefly explained download Docker image run Docker container, might worthwhile learn create Docker images. book Docker: & Running Sean Kane Karl Matthias good resource well.","code":""},{"path":"chapter-3-obtaining-data.html","id":"chapter-3-obtaining-data","chapter":"3 Obtaining Data","heading":"3 Obtaining Data","text":"chapter deals first step OSEMN model: obtaining data.\n, without data, much data science can .\nassume data need solve data science problem already exists.\nfirst task get data onto computer (possibly also inside Docker container) form can work .According Unix philosophy, text universal interface.\nAlmost every command-line tool takes text input, produces text output, .\nmain reason command-line tools can work well together.\nHowever, ’ll see, even just text can come multiple forms.Data can obtained several ways—example downloading server, querying database, connecting Web API.\nSometimes, data comes compressed form binary format Microsoft Excel Spreadsheet.\nchapter, discuss several tools help tackle command line, including: curl33, in2csv34, sql2csv35, tar36.","code":""},{"path":"chapter-3-obtaining-data.html","id":"overview","chapter":"3 Obtaining Data","heading":"3.1 Overview","text":"chapter, ’ll learn :Copy local files Docker imageDownload data InternetDecompress filesExtract data spreadsheetsQuery relational databasesCall web APIsThis chapter starts following files:instructions get files Chapter 2.\nfiles either downloaded generated using command-line tools.","code":"$ cd /data/ch03\n \n$ l\ntotal 924K\n-rw-r--r-- 1 dst dst 627K Dec 14 11:46 logs.tar.gz\n-rw-r--r-- 1 dst dst 189K Dec 14 11:46 r-datasets.db\n-rw-r--r-- 1 dst dst  149 Dec 14 11:46 tmnt-basic.csv\n-rw-r--r-- 1 dst dst  148 Dec 14 11:46 tmnt-missing-newline.csv\n-rw-r--r-- 1 dst dst  181 Dec 14 11:46 tmnt-with-header.csv\n-rw-r--r-- 1 dst dst  91K Dec 14 11:46 top2000.xlsx"},{"path":"chapter-3-obtaining-data.html","id":"copying-local-files-to-the-docker-container","chapter":"3 Obtaining Data","heading":"3.2 Copying Local Files to the Docker Container","text":"common situation already necessary files computer.\nsection explains can get files Docker container.mentioned Chapter 2 Docker container isolated virtual environment.\nLuckily one exception : files can transferred Docker container.\nlocal directory ran docker run, mapped directory Docker container.\ndirectory called /data.\nNote home directory, /home/dst.one files local computer, want apply command-line tools , copy move files mapped directory.\nLet’s assume file called logs.csv Downloads directory.’re running Windows, open command prompt PowerShell run following two commands:running Linux macOS, open terminal execute following command operating system (inside Docker container):can also drag--drop file right directory using graphical file manager Windows Explorer macOS Finder.","code":"> cd %UserProfile%\\Downloads\n> copy logs.csv MyDataScienceToolbox\\$ cp ~/Downloads/logs.csv ~/my-data-science-toolbox"},{"path":"chapter-3-obtaining-data.html","id":"downloading-from-the-internet","chapter":"3 Obtaining Data","heading":"3.3 Downloading from the Internet","text":"Internet provides, without doubt, largest resource interesting data.\ncommand-line tool curl can considered command line’s Swiss Army knife comes downloading data Internet.","code":""},{"path":"chapter-3-obtaining-data.html","id":"introducing-curl","chapter":"3 Obtaining Data","heading":"3.3.1 Introducing curl","text":"browse URL, stands uniform resource locator, browser interprets data downloads.\nexample, browser renders HTML files, plays video files automatically, shows PDF files.\nHowever, use curl access URL, downloads data , default, prints standard output.\ncurl doesn’t interpretation, luckily command-line tools can used process data .easiest invocation curl specify URL command-line argument.\nLet’s, try downloading article Wikipedia:➊ Remember, trim used make output fit nicely book.can see, curl downloads raw HTML returned Wikipedia’s server; interpretation done contents immediately printed standard output.\nURL, ’d think article list windmills Netherlands.\nHowever, apparently many windmills left province page.\nFascinating.default, curl outputs progress meter shows download rate expected time completion.\noutput isn’t written standard output, separate channel, known standard error, doesn’t interfere add another tool pipeline.\ninformation can useful downloading large files, usually find distracting, specify -s option silence output.➊ ’ll discuss pup37, handy tool scraping websites, detail Chapter 5.know, apparently 234 windmills province Friesland alone!","code":"$ curl \"https://en.wikipedia.org/wiki/List_of_windmills_in_the_Netherlands\" |\n> trim ➊\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0<!\nDOCTYPE html>\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n<head>\n<meta charset=\"UTF-8\"/>\n<title>List of windmills in the Netherlands - Wikipedia<\/title>\n<script>document.documentElement.className=\"client-js\";RLCONF={\"wgBreakFrames\":…\n\"wgRelevantPageName\":\"List_of_windmills_in_the_Netherlands\",\"wgRelevantArticleI…\n,\"site.styles\":\"ready\",\"user.styles\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"us…\n\"ext.growthExperiments.SuggestedEditSession\"];<\/script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement(\"user.options@…\n100  249k    0  249k    0     0   853k      0 --:--:-- --:--:-- --:--:--  867k\n… with 1754 more lines$ curl -s \"https://en.wikipedia.org/wiki/List_of_windmills_in_Friesland\" |\n> pup -n 'table.wikitable tr' ➊\n234"},{"path":"chapter-3-obtaining-data.html","id":"saving","chapter":"3 Obtaining Data","heading":"3.3.2 Saving","text":"can let curl save output file adding -O option.\nfilename based last part URL.don’t like filename can use -o option together filename redirect output file :","code":"$ curl -s \"https://en.wikipedia.org/wiki/List_of_windmills_in_Friesland\" -O\n \n$ l\ntotal 1.4M\n-rw-r--r-- 1 dst dst 432K Dec 14 11:46 List_of_windmills_in_Friesland\n-rw-r--r-- 1 dst dst 627K Dec 14 11:46 logs.tar.gz\n-rw-r--r-- 1 dst dst 189K Dec 14 11:46 r-datasets.db\n-rw-r--r-- 1 dst dst  149 Dec 14 11:46 tmnt-basic.csv\n-rw-r--r-- 1 dst dst  148 Dec 14 11:46 tmnt-missing-newline.csv\n-rw-r--r-- 1 dst dst  181 Dec 14 11:46 tmnt-with-header.csv\n-rw-r--r-- 1 dst dst  91K Dec 14 11:46 top2000.xlsx$ curl -s \"https://en.wikipedia.org/wiki/List_of_windmills_in_Friesland\" > fries\nland.html"},{"path":"chapter-3-obtaining-data.html","id":"other-protocols","chapter":"3 Obtaining Data","heading":"3.3.3 Other Protocols","text":"total, curl supports 20 protocols.\ndownload FTP server, stands File Transfer Protocol, use curl way.\ndownload file welcome.msg ftp.gnu.org:specified URL directory, curl list contents directory.\nURL password protected, can specify username password follows -u option.DICT protocol, allows access various dictionaries lookup definitions.\n’s definition “windmill” according Collaborative International Dictionary English:downloading data Internet, however, protocol likely HTTP, URL start either http:// https://.","code":"$ curl -s \"ftp://ftp.gnu.org/welcome.msg\" | trim\nNOTICE (Updated October 15 2021):\n \nIf you maintain scripts used to access ftp.gnu.org over FTP,\nwe strongly encourage you to change them to use HTTPS instead.\n \nEventually we hope to shut down FTP protocol access, but plan\nto give notice here and other places for several months ahead\nof time.\n \n--\n… with 19 more lines$ curl -s \"dict://dict.org/d:windmill\" | trim\n220 dict.dict.org dictd 1.12.1/rf on Linux 4.19.0-10-amd64 <auth.mime> <8291886…\n250 ok\n150 1 definitions retrieved\n151 \"Windmill\" gcide \"The Collaborative International Dictionary of English v.0…\nWindmill \\Wind\"mill`\\, n.\n   A mill operated by the power of the wind, usually by the\n   action of the wind upon oblique vanes or sails which radiate\n   from a horizontal shaft. --Chaucer.\n   [1913 Webster]\n.\n… with 2 more lines"},{"path":"chapter-3-obtaining-data.html","id":"following-redirects","chapter":"3 Obtaining Data","heading":"3.3.4 Following Redirects","text":"access shortened URL, ones start http://bit.ly/ http://t.co/, browser automatically redirects correct location.\ncurl, however, need specify -L --location option order redirected.\ndon’t, can get something like:Sometimes get nothing back, just like follow URL mentioned :specifying ---head option, curl fetches HTTP header response, allows inspect status code information returned server.first line shows protocol followed HTTP status code, 303 case.\ncan also see location URL redirects .\nInspecting header getting status code useful debugging tool case curl give expected result.\ncommon HTTP status codes include 404 (found) 403 (forbidden).\nWikipedia page lists HTTP status codes.summary, curl useful command-line tool downloading data Internet.\nthree common options -s silence progress meter, -u specify username password, -L automatically follow redirects.\nSee man page information (make head spin):","code":"$ curl -s \"https://bit.ly/2XBxvwK\"\n<html>\n<head><title>Bitly<\/title><\/head>\n<body><a href=\"https://youtu.be/dQw4w9WgXcQ\">moved here<\/a><\/body>\n<\/html>%$ curl -s \"https://youtu.be/dQw4w9WgXcQ\"$ curl -sI \"https://youtu.be/dQw4w9WgXcQ\" | trim\nHTTP/2 303\ncontent-type: application/binary\nx-content-type-options: nosniff\ncache-control: no-cache, no-store, max-age=0, must-revalidate\npragma: no-cache\nexpires: Mon, 01 Jan 1990 00:00:00 GMT\ndate: Tue, 14 Dec 2021 10:46:22 GMT\nlocation: https://www.youtube.com/watch?v=dQw4w9WgXcQ&feature=youtu.be\ncontent-length: 0\nx-frame-options: SAMEORIGIN\n… with 11 more lines$ man curl | trim 20\ncurl(1)                           Curl Manual                          curl(1)\n \nNAME\n       curl - transfer a URL\n \nSYNOPSIS\n       curl [options / URLs]\n \nDESCRIPTION\n       curl  is  a tool to transfer data from or to a server, using one of the\n       supported protocols (DICT, FILE, FTP, FTPS, GOPHER, HTTP, HTTPS,  IMAP,\n       IMAPS,  LDAP,  LDAPS,  MQTT, POP3, POP3S, RTMP, RTMPS, RTSP, SCP, SFTP,\n       SMB, SMBS, SMTP, SMTPS, TELNET and TFTP). The command  is  designed  to\n       work without user interaction.\n \n       curl offers a busload of useful tricks like proxy support, user authen‐\n       tication, FTP upload, HTTP post, SSL connections, cookies, file  trans‐\n       fer  resume,  Metalink,  and more. As you will see below, the number of\n       features will make your head spin!\n \n… with 3986 more lines"},{"path":"chapter-3-obtaining-data.html","id":"decompressing-files","chapter":"3 Obtaining Data","heading":"3.4 Decompressing Files","text":"original dataset large ’s collection many files, file may compressed archive.\nDatasets contain many repeated values (words text file keys JSON file) especially well suited compression.Common file extensions compressed archives : .tar.gz, .zip, .rar.\ndecompress , use command-line tools tar, unzip38, unrar39, respectively.\n(, though less common, file extensions need yet tools.)Let’s take tar.gz (pronounced “gzipped tarball”) example.\norder extract archive named logs.tar.gz, use following incantation:➊ ’s common combine three short options, like , can also specify separately -x -z -f.\nfact, many command-tools allow combine options consist single character.Indeed, tar notorious many command-line arguments.\ncase, three options -x, -z, -f specify tar extract files archive, use gzip decompression algorithm use file logs.tar.gz.However, since ’re yet familiar archive, ’s good idea first examine contents.\ncan done -t option (instead -x option):seems archive contains lot files, inside directory.\norder keep current directory clean, ’s good idea first create new directory using mkdir extract files using -C option.Let’s verify number files contents:Excellent.\nNow, understand ’d like scrub explore log files, ’s later Chapter 5 Chapter 7.time, ’ll get used options, ’d like show alternative option, might convenient.\nRather remembering different command-line tools options, ’s handy script called unpack40, decompress many different formats.\nunpack looks extension file want decompress, calls appropriate command-line tool.\nNow, order decompress file, run:","code":"$ tar -xzf logs.tar.gz ➊ $ tar -tzf logs.tar.gz | trim\nE1FOSPSAYDNUZI.2020-09-01-00.0dd00628\nE1FOSPSAYDNUZI.2020-09-01-00.b717c457\nE1FOSPSAYDNUZI.2020-09-01-01.05f904a4\nE1FOSPSAYDNUZI.2020-09-01-02.36588daf\nE1FOSPSAYDNUZI.2020-09-01-02.6cea8b1d\nE1FOSPSAYDNUZI.2020-09-01-02.be4bc86d\nE1FOSPSAYDNUZI.2020-09-01-03.16f3fa32\nE1FOSPSAYDNUZI.2020-09-01-03.1c0a370f\nE1FOSPSAYDNUZI.2020-09-01-03.76df64bf\nE1FOSPSAYDNUZI.2020-09-01-04.0a1ade1b\n… with 2427 more lines$ mkdir logs\n \n$ tar -xzf logs.tar.gz -C logs$ ls logs | wc -l\n2437\n \n$ cat logs/* | trim\n#Version: 1.0\n#Fields: date time x-edge-location sc-bytes c-ip cs-method cs(Host) cs-uri-stem…\n2020-09-01      00:51:54        SEA19-C1        391     206.55.174.150  GET    …\n2020-09-01      00:54:59        CPH50-C2        384     82.211.213.95   GET    …\n#Version: 1.0\n#Fields: date time x-edge-location sc-bytes c-ip cs-method cs(Host) cs-uri-stem…\n2020-09-01      00:04:28        DFW50-C1        391     2a03:2880:11ff:9::face:…\n#Version: 1.0\n#Fields: date time x-edge-location sc-bytes c-ip cs-method cs(Host) cs-uri-stem…\n2020-09-01      01:04:14        ATL56-C4        385     2600:1700:2760:da20:548…\n… with 10279 more lines$ unpack logs.tar.gz"},{"path":"chapter-3-obtaining-data.html","id":"converting-microsoft-excel-spreadsheets-to-csv","chapter":"3 Obtaining Data","heading":"3.5 Converting Microsoft Excel Spreadsheets to CSV","text":"many people, Microsoft Excel offers intuitive way work small datasets perform calculations .\nresult, lot data embedded Microsoft Excel spreadsheets.\nspreadsheets , depending extension filename, stored either proprietary binary format (.xls) collection compressed XML files (.xlsx).\ncases, data readily usable command-line tools.\nshame use valuable datasets just stored way.Especially ’re just starting command line, might tempted convert spreadsheet CSV opening Microsoft Excel open source variant LibreOffice Calc, manually exporting CSV.\nworks one-solution, disadvantage scale well multiple files automated.\nFurthermore, ’re working server, chances don’t application available.\nTrust , ’ll get hang .Luckily, command-line tool called in2csv, converts Microsoft Excel spreadsheets CSV files.\nCSV stands comma-separated values.\nWorking CSV can tricky lacks formal specification.\n\nYakov Shafranovich defines CSV format according following three points:41Each record located separate line, delimited line break (␊). Take, example, following CSV file crucial information Teenage Mutant Ninja Turtles:➊ -option makes bat show non-printable characters like spaces, tabs, newlines.last record file may may ending line break. example:may header appearing first line file format normal record lines. header contain names corresponding fields file contain number fields records rest file. example:can see, CSV default readable.\ncan pipe data tool called csvlook42, nicely format table.\nCSV data header, like tmnt-missing-newline.csv need add -H option, otherwise first line interpreted header.➊ -H option specifies CSV file header.Let’s demonstrate in2csv using spreadsheet contains 2000 popular songs according annual Dutch marathon radio program Top 2000.\nextract data, invoke in2csv follows:Danny Vera? popular song supposed Bohemian Rhapsody, course.\nWell, least Queen appears plenty times Top 2000 can’t really complain:➊ value --regex options regular expression (regex). ’s special syntax defining patterns. , want match artists exactly match “Queen,” use caret (^) dollar sign ($) match start end values column ARTIEST.way, tools in2csv, csvgrep, csvlook part CSVkit, collection command-line tools work CSV data.format file automatically determined extension, .xlsx case.\npipe data in2csv, specify format explicitly.spreadsheet can contain multiple worksheets.\nin2csv extracts, default, first worksheet.\nextract different worksheet, need pass name worksheet --sheet option.\n’re sure ’s worksheet called, can use --names option, prints names worksheets.\nsee top2000.xlsx one sheet, named Blad1 (Dutch Sheet1).","code":"$ bat -A tmnt-basic.csv ➊\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: tmnt-basic.csv\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ Leonardo,Leo,blue,two·ninjakens␊\n   2   │ Raphael,Raph,red,pair·of·sai␊\n   3   │ Michelangelo,Mikey·or·Mike,orange,pair·of·nunchaku␊\n   4   │ Donatello,Donnie·or·Don,purple,staff␊\n───────┴────────────────────────────────────────────────────────────────────────$ bat -A tmnt-missing-newline.csv\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: tmnt-missing-newline.csv\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ Leonardo,Leo,blue,two·ninjakens␊\n   2   │ Raphael,Raph,red,pair·of·sai␊\n   3   │ Michelangelo,Mikey·or·Mike,orange,pair·of·nunchaku␊\n   4   │ Donatello,Donnie·or·Don,purple,staff\n───────┴────────────────────────────────────────────────────────────────────────$ bat -A tmnt-with-header.csv\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: tmnt-with-header.csv\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ name,nickname,mask_color,weapon␊\n   2   │ Leonardo,Leo,blue,two·ninjakens␊\n   3   │ Raphael,Raph,red,pair·of·sai␊\n   4   │ Michelangelo,Mikey·or·Mike,orange,pair·of·nunchaku␊\n   5   │ Donatello,Donnie·or·Don,purple,staff␊\n───────┴────────────────────────────────────────────────────────────────────────$ csvlook tmnt-with-header.csv\n│ name         │ nickname      │ mask_color │ weapon           │\n├──────────────┼───────────────┼────────────┼──────────────────┤\n│ Leonardo     │ Leo           │ blue       │ two ninjakens    │\n│ Raphael      │ Raph          │ red        │ pair of sai      │\n│ Michelangelo │ Mikey or Mike │ orange     │ pair of nunchaku │\n│ Donatello    │ Donnie or Don │ purple     │ staff            │\n \n$ csvlook tmnt-basic.csv\n│ Leonardo     │ Leo           │ blue   │ two ninjakens    │\n├──────────────┼───────────────┼────────┼──────────────────┤\n│ Raphael      │ Raph          │ red    │ pair of sai      │\n│ Michelangelo │ Mikey or Mike │ orange │ pair of nunchaku │\n│ Donatello    │ Donnie or Don │ purple │ staff            │\n \n$ csvlook -H tmnt-missing-newline.csv ➊\n│ a            │ b             │ c      │ d                │\n├──────────────┼───────────────┼────────┼──────────────────┤\n│ Leonardo     │ Leo           │ blue   │ two ninjakens    │\n│ Raphael      │ Raph          │ red    │ pair of sai      │\n│ Michelangelo │ Mikey or Mike │ orange │ pair of nunchaku │\n│ Donatello    │ Donnie or Don │ purple │ staff            │$ curl \"https://www.nporadio2.nl/data/download/TOP-2000-2020.xlsx\" > top2000.xls\nx\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 41744  100 41744    0     0  46024      0 --:--:-- --:--:-- --:--:-- 46228\n \n$ in2csv top2000.xlsx | tee top2000.csv | trim\nBadZipFile: File is not a zip file$ csvgrep top2000.csv --columns ARTIEST --regex '^Queen$' | csvlook -I ➊\nStopIteration:\nStopIteration:$ in2csv --names top2000.xlsx\nBadZipFile: File is not a zip file\nsys:1: ResourceWarning: unclosed file <_io.BufferedReader name='top2000.xlsx'>"},{"path":"chapter-3-obtaining-data.html","id":"querying-relational-databases","chapter":"3 Obtaining Data","heading":"3.6 Querying Relational Databases","text":"Many companies store data relational database.\nJust spreadsheets, great obtain data command line.Examples relational databases MySQL, PostgreSQL, SQLite.\ndatabases slightly different way interfacing .\nprovide command-line tool command-line interface, others .\nMoreover, consistent comes usage output.Fortunately, command-line tool called sql2csv, part CSVkit suite.\nworks many different databases common interface, including MySQL, Oracle, PostgreSQL, SQLite, Microsoft SQL Server, Sybase.\noutput sql2csv , name suggests, CSV format.can obtain data relational databases executing SELECT query .\n(sql2csv also support INSERT, UPDATE, DELETE queries, ’s purpose chapter.)sql2csv needs two arguments: --db, specifies database URL, typical form : dialect+driver://username:password@host:port/database --query, contains SELECT query.\nexample, given SQLite database contains standard datasets R43 , can select rows table mtcars sort mpg column follows:SQLite database local file, don’t need specify username, password host.\nwish query database employer, ’d course need know can access ’d need permission .","code":"$ sql2csv --db 'sqlite:///r-datasets.db' \\\n> --query 'SELECT row_names AS car, mpg FROM mtcars ORDER BY mpg' | csvlook\n│ car                 │  mpg │\n├─────────────────────┼──────┤\n│ Cadillac Fleetwood  │ 10.4 │\n│ Lincoln Continental │ 10.4 │\n│ Camaro Z28          │ 13.3 │\n│ Duster 360          │ 14.3 │\n│ Chrysler Imperial   │ 14.7 │\n│ Maserati Bora       │ 15.0 │\n│ Merc 450SLC         │ 15.2 │\n│ AMC Javelin         │ 15.2 │\n… with 24 more lines"},{"path":"chapter-3-obtaining-data.html","id":"calling-web-apis","chapter":"3 Obtaining Data","heading":"3.7 Calling Web APIs","text":"previous section explained download files Internet.\nAnother way data can come Internet web API, stands Application Programming Interface.\nnumber APIs offered growing increasing rate, means lot interesting data us data scientists.Web APIs meant presented nice layout, websites.\nInstead, web APIs return data structured format, JSON XML.\ndata structured form advantage data can easily processed tools, jq.\nexample, API Ice Fire, contains lot information George R.R. Martin’s fictional world, Game Thrones book TV shows take place, returns data following JSON structure:➊ Spoiler alert: data entirely --date.data piped command-line tool jq just display nice way.\njq many scrubbing exploring possibilities explore Chapter 5 Chapter 7.","code":"$ curl -s \"https://anapioficeandfire.com/api/characters/583\" | jq '.'\n{\n  \"url\": \"https://anapioficeandfire.com/api/characters/583\",\n  \"name\": \"Jon Snow\",\n  \"gender\": \"Male\",\n  \"culture\": \"Northmen\",\n  \"born\": \"In 283 AC\",\n  \"died\": \"\", ➊\n  \"titles\": [\n    \"Lord Commander of the Night's Watch\"\n  ],\n  \"aliases\": [\n    \"Lord Snow\",\n    \"Ned Stark's Bastard\",\n    \"The Snow of Winterfell\",\n    \"The Crow-Come-Over\",\n    \"The 998th Lord Commander of the Night's Watch\",\n    \"The Bastard of Winterfell\",\n    \"The Black Bastard of the Wall\",\n    \"Lord Crow\"\n  ],\n  \"father\": \"\",\n  \"mother\": \"\",\n  \"spouse\": \"\",\n  \"allegiances\": [\n    \"https://anapioficeandfire.com/api/houses/362\"\n  ],\n  \"books\": [\n    \"https://anapioficeandfire.com/api/books/5\"\n  ],\n  \"povBooks\": [\n    \"https://anapioficeandfire.com/api/books/1\",\n    \"https://anapioficeandfire.com/api/books/2\",\n    \"https://anapioficeandfire.com/api/books/3\",\n    \"https://anapioficeandfire.com/api/books/8\"\n  ],\n  \"tvSeries\": [\n    \"Season 1\",\n    \"Season 2\",\n    \"Season 3\",\n    \"Season 4\",\n    \"Season 5\",\n    \"Season 6\"\n  ],\n  \"playedBy\": [\n    \"Kit Harington\"\n  ]\n}"},{"path":"chapter-3-obtaining-data.html","id":"authentication","chapter":"3 Obtaining Data","heading":"3.7.1 Authentication","text":"web APIs require authenticate (, prove identity) can consume output.\nseveral ways .\nweb APIs use API keys others use OAuth protocol.\nNews API, independent source headlines news articles, great example.\nLet’s see happens try access API without API key:Well, expected.\npart question mark, way, pass query parameters.\n’s also place need specify API key.\n’d like keep API key secret, insert reading file /data/.secret/newsapi.org_apikey using command substitution.can get API key News API’s website.","code":"$ curl -s \"http://newsapi.org/v2/everything?q=linux\" | jq .\n{\n  \"status\": \"error\",\n  \"code\": \"apiKeyMissing\",\n  \"message\": \"Your API key is missing. Append this to the URL with the apiKey pa\nram, or use the x-api-key HTTP header.\"\n}$ curl -s \"http://newsapi.org/v2/everything?q=linux&apiKey=$(< /data/.secret/new\nsapi.org_apikey)\" |\n> jq '.' | trim 30\n{\n  \"status\": \"ok\",\n  \"totalResults\": 9653,\n  \"articles\": [\n    {\n      \"source\": {\n        \"id\": \"engadget\",\n        \"name\": \"Engadget\"\n      },\n      \"author\": \"Igor Bonifacic\",\n      \"title\": \"'Arma 3' and 'DayZ' add BattlEye anti-cheat support through Val…\n      \"description\": \"While there are still many unknowns about Steam Deck\\r\\n\n      \"url\": \"https://www.engadget.com/arma-3-dayz-proton-battleye-support-2246…\n      \"urlToImage\": \"https://s.yimg.com/os/creatr-uploaded-images/2021-12/bcb0f…\n      \"publishedAt\": \"2021-12-03T22:46:25Z\",\n      \"content\": \"While there are still many unknowns about Steam Deck\\r\\ns lib…\n    },\n    {\n      \"source\": {\n        \"id\": null,\n        \"name\": \"Slashdot.org\"\n      },\n      \"author\": \"EditorDavid\",\n      \"title\": \"AWS Embraces Fedora Linux for Its Cloud-Based 'Amazon Linux'\",\n      \"description\": \"ZDNet reports:\\n\\nBy and large, the public cloud runs on …\n      \"url\": \"https://linux.slashdot.org/story/21/11/27/0328223/aws-embraces-fe…\n      \"urlToImage\": \"https://a.fsdn.com/sd/topics/cloud_64.png\",\n      \"publishedAt\": \"2021-11-27T16:34:00Z\",\n      \"content\": \"By and large, the public cloud runs on Linux. Most users, eve…\n    },\n… with 236 more lines"},{"path":"chapter-3-obtaining-data.html","id":"streaming-apis","chapter":"3 Obtaining Data","heading":"3.7.2 Streaming APIs","text":"web APIs return data streaming manner.\nmeans connect , data continue pour , connection closed.\nwell-known example Twitter “firehose,” constantly streams tweets sent around world.\nLuckily, command-line tools also operate streaming matter.Let’s take 10 second sample one Wikimedia’s streaming APIs, example:particular API returns changes made Wikipedia properties Wikimedia.\ncommand-line tool sample used close connection 10 seconds.\nconnection can also closed manually pressing Ctrl-C send interrupt.\noutput saved file wikimedia-stream-sample.\nLet’s take peek using trim:little bit sed jq can scrub data get glimpse changes happening English version Wikipedia:➊ sed expression prints lines start data: prints part semicolon, happens JSON.\n➋ jq expression prints title key JSON objects certain type server_name.Speaking streaming, know stream Star Wars: Episode IV – New Hope free using telnet44?time, see Han Solo shoot first!Sure, ’s probably good source data, ’s nothing wrong enjoying old classic training machine learning models45.","code":"$ curl -s \"https://stream.wikimedia.org/v2/stream/recentchange\" |\n> sample -s 10 > wikimedia-stream-sample$ < wikimedia-stream-sample trim\n:ok\n \nevent: message\nid: [{\"topic\":\"eqiad.mediawiki.recentchange\",\"partition\":0,\"timestamp\":16101133…\ndata: {\"$schema\":\"/mediawiki/recentchange/1.0.0\",\"meta\":{\"uri\":\"https://en.wiki…\n \nevent: message\nid: [{\"topic\":\"eqiad.mediawiki.recentchange\",\"partition\":0,\"timestamp\":16101133…\ndata: {\"$schema\":\"/mediawiki/recentchange/1.0.0\",\"meta\":{\"uri\":\"https://www.wik…\n \n… with 1078 more lines$ < wikimedia-stream-sample sed -n 's/^data: //p' | ➊\n> jq 'select(.type == \"edit\" and .server_name == \"en.wikipedia.org\") | .title' ➋\n\"Odion Ighalo\"\n\"Hold Up (song)\"\n\"Talk:Royal Bermuda Yacht Club\"\n\"Jenna Ushkowitz\"\n\"List of films released by Yash Raj Films\"\n\"SP.A\"\n\"Odette (musician)\"\n\"Talk:Pierre Avoi\"\n\"User:Curlymanjaro/sandbox3\"\n\"List of countries by electrification rate\"\n\"Grieg (crater)\"\n\"Gorman, Edmonton\"\n\"Khabza\"\n\"QAnon\"\n\"Khaw Boon Wan\"\n\"Draft:Oggy and the Cockroaches (1975 TV series)\"\n\"Renzo Reggiardo\"\n\"Greer, Arizona\"\n\"National Curriculum for England\"\n\"Mod DB\"\n\"Jordanian Pro League\"\n\"List of foreign Serie A players\"$ telnet towel.blinkenlights.nl \n \n                       -===                    `\"',\n       I'll bet you   \"\"o o                    O O|)\n           have!      _\\ -/_                  _\\o/ _\n                     || || |*                /|\\ / |\\\n                     \\\\ || ***              //| |  |\\\\\n                      \\\\o=***********      // | |  | ||\n                      |\\(#'***\\\\        -==#  | |  | ||\n                      |====|*  ')         '\\  |====| /#\n                      |/|| |                  | || |  \"\n                      ( )( )                  | || |\n                      |-||-|                  | || |\n                      | || |                  | || |\n      ________________[_][__\\________________/__)(_)_____________________\n "},{"path":"chapter-3-obtaining-data.html","id":"summary-2","chapter":"3 Obtaining Data","heading":"3.8 Summary","text":"Congratulations, finished first step OSEMN model.\n’ve learned variety ways obtain data, ranging downloading querying relational database.\nnext chapter, intermezzo chapter, ’ll teach create command-line tools.\nFeel free skip go Chapter 5 (second step OSEMN model) wait learn scrubbing data.","code":""},{"path":"chapter-3-obtaining-data.html","id":"for-further-exploration-2","chapter":"3 Obtaining Data","heading":"3.9 For Further Exploration","text":"Looking dataset practice ? GitHub repository Awesome Public Datasets lists hundreds high-quality datasets publicly available.perhaps ’d rather practice API? GitHub repository Public APIs lists many free APIs. City Bikes One API among favorites.Writing SQL queries obtain data relational database important skill. first 15 lessons book SQL 10 Minutes Day Ben Forta teach SELECT statement filtering, grouping, sorting capabilities.","code":""},{"path":"chapter-4-creating-command-line-tools.html","id":"chapter-4-creating-command-line-tools","chapter":"4 Creating Command-line Tools","heading":"4 Creating Command-line Tools","text":"Throughout book, ’ll introduce many commands pipelines basically fit one line.\nknown one-liners pipelines.\nable perform complex tasks just one-liner makes command line powerful.\n’s different experience writing using traditional programs.tasks perform , perform often.\ntasks specific others can generalized.\nneed repeat certain one-liner regular basis, ’s worthwhile turn command-line tool .\n, one-liners command-line tools uses.\nRecognizing opportunity requires practice skill.\nadvantages command-line tool don’t remember entire one-liner improves readability include pipeline.\nsense, can think command-line tool similar function programming language.benefit working programming language, however, code one file.\nmeans can easily edit reuse code.\ncode parameters can even generalized re-applied problems follow similar pattern.Command-line tools best worlds: can used command line, accept parameters, created .\nchapter, ’re going get familiar creating command-line tools two ways.\nFirst, explain turn one-liners reusable command-line tools.\nadding parameters commands, can add flexibility programming language offers.\nSubsequently, demonstrate create reusable command-line tools code ’s written programming language.\nfollowing Unix philosophy, code can combined command-line tools, may written entirely different language.\nchapter, focus three programming languages: Bash, Python, R.believe creating reusable command-line tools makes efficient productive data scientist long run.\ngradually build data science toolbox can draw existing tools apply problems encountered previously.\nrequires practice recognize opportunity turn one-liner existing code command-line tool.","code":""},{"path":"chapter-4-creating-command-line-tools.html","id":"overview-1","chapter":"4 Creating Command-line Tools","heading":"4.1 Overview","text":"chapter, ’ll learn :Convert one-liners parameterized shell scriptsTurn existing Python R code reusable command-line toolsThis chapter starts following files:instructions get files Chapter 2.\nfiles either downloaded generated using command-line tools.","code":"$ cd /data/ch04\n \n$ l\ntotal 32K\n-rwxr-xr-x 1 dst dst 400 Dec 14 11:46 fizzbuzz.py*\n-rwxr-xr-x 1 dst dst 391 Dec 14 11:46 fizzbuzz.R*\n-rwxr-xr-x 1 dst dst 182 Dec 14 11:46 stream.py*\n-rwxr-xr-x 1 dst dst 147 Dec 14 11:46 stream.R*\n-rwxr-xr-x 1 dst dst 105 Dec 14 11:46 top-words-4.sh*\n-rwxr-xr-x 1 dst dst 128 Dec 14 11:46 top-words-5.sh*\n-rwxr-xr-x 1 dst dst 647 Dec 14 11:46 top-words.py*\n-rwxr-xr-x 1 dst dst 584 Dec 14 11:46 top-words.R*"},{"path":"chapter-4-creating-command-line-tools.html","id":"converting-one-liners-into-shell-scripts","chapter":"4 Creating Command-line Tools","heading":"4.2 Converting One-liners into Shell Scripts","text":"section ’m going explain turn one-liner reusable command-line tool.\nLet’s say like get top frequent words used piece text.\nTake book Alice’s Adventures Wonderland Lewis Carroll, , like many great books, freely available Project Gutenberg.following sequence tools pipeline job:➊ Downloading ebook using curl.\n➋ Converting entire text lowercase using tr47.\n➌ Extracting words using grep48 put word separate line.\n➍ Sort words alphabetical order using sort49.\n➎ Remove duplicates count often word appears list using uniq50.\n➏ Sort list unique words count descending order using sort.\n➐ Keep top 10 lines (.e., words) using head.words indeed appear often text.\nwords (apart word “alice”) appear frequently many English texts, carry little meaning.\nfact, known stopwords.\nget rid , keep frequent words related text.’s list stopwords ’ve found:grep can filter stopwords right start counting:➊ Obtain patterns file (stopwords case), one per line, -f. Interpret patterns fixed strings -F. Select lines containing matches form whole words -w. Select non-matching lines -v.nothing wrong running one-liner just .\nHowever, imagine wanted top 10 words every e-book Project Gutenberg.\nimagine wanted top 10 words news website hourly basis.\ncases, best one-liner separate building block can part something bigger.\nadd flexibility one-liner terms parameters, let’s turn shell script.allows us take one-liner starting point, gradually improve .\nturn one-liner reusable command-line tool, ’ll walk following six steps:Copy paste one-liner file.Add execute permissions.Define -called shebang.Remove fixed input part.Add parameter.Optionally extend PATH.","code":"$ curl -sL \"https://www.gutenberg.org/files/11/11-0.txt\" | trim\n﻿The Project Gutenberg eBook of Alice’s Adventures in Wonderland, by Lewis …\n \nThis eBook is for the use of anyone anywhere in the United States and\nmost other parts of the world at no cost and with almost no restrictions\nwhatsoever. You may copy it, give it away or re-use it under the terms\nof the Project Gutenberg License included with this eBook or online at\nwww.gutenberg.org. If you are not located in the United States, you\nwill have to check the laws of the country where you are located before\nusing this eBook.\n \n… with 3751 more lines$ curl -sL \"https://www.gutenberg.org/files/11/11-0.txt\" | ➊\n> tr '[:upper:]' '[:lower:]' | ➋\n> grep -oE \"[a-z\\']{2,}\" | ➌\n> sort | ➍\n> uniq -c | ➎\n> sort -nr | ➏\n> head -n 10 ➐\n   1839 the\n    942 and\n    811 to\n    638 of\n    610 it\n    553 she\n    486 you\n    462 said\n    435 in\n    403 alice$ curl -sL \"https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/\nstopwords-en.txt\" |\n> sort | tee stopwords | trim 20\n10\n39\na\nable\nableabout\nabout\nabove\nabroad\nabst\naccordance\naccording\naccordingly\nacross\nact\nactually\nad\nadded\nadj\nadopted\nae\n… with 1278 more lines$ curl -sL \"https://www.gutenberg.org/files/11/11-0.txt\" |\n> tr '[:upper:]' '[:lower:]' |\n> grep -oE \"[a-z\\']{2,}\" |\n> sort |\n> grep -Fvwf stopwords | ➊\n> uniq -c |\n> sort -nr |\n> head -n 10\n    403 alice\n     98 gutenberg\n     88 project\n     76 queen\n     71 time\n     63 king\n     60 turtle\n     57 mock\n     56 hatter\n     55 gryphon"},{"path":"chapter-4-creating-command-line-tools.html","id":"step-1-create-file","chapter":"4 Creating Command-line Tools","heading":"4.2.1 Step 1: Create File","text":"first step create new file.\ncan open favorite text editor copy paste one-liner.\nLet’s name file top-words-1.sh indicate first step towards new command-line tool like stay command line, can use builtin fc, stands fix command, allows fix edit last-run command.Running fc invokes default text editor, stored environment variable EDITOR.\nDocker container, set nano, straightforward text editor.\ncan see, file contains one-liner:Let’s give temporary file proper name pressing Ctrl-O, removing temporary filename, typing top-words-1.sh:Press Enter:Press Y confirm want save different filename:Press Ctrl-X exit nano go back whence came.using file extension .sh make clear creating shell script.\nHowever, command-line tools don’t need extension.\nfact, command-line tools rarely extensions.Confirm contents file:can now use bash51 interpret execute commands file:saves typing one-liner next time.However, file executed , ’s yet real command-line tool.\nLet’s change next step.","code":"$ fc  GNU nano 5.4                     /tmp/zshpu0emE                               \ncurl -sL \"https://www.gutenberg.org/files/11/11-0.txt\" |                        \ntr '[:upper:]' '[:lower:]' |            \ngrep -oE \"[a-z\\']{2,}\" |                \nsort |              \ngrep -Fvwf stopwords |                  \nuniq -c |           \nsort -nr |          \nhead -n 10          \n \n \n \n \n                                [ Read 8 lines ]                                \n^G Help      ^O Write Out ^W Where Is  ^K Cut       ^T Execute   ^C Location    \n^X Exit      ^R Read File ^\\ Replace   ^U Paste     ^J Justify   ^_ Go To Line    GNU nano 5.4                     /tmp/zshpu0emE                               \ncurl -sL \"https://www.gutenberg.org/files/11/11-0.txt\" |                        \ntr '[:upper:]' '[:lower:]' |            \ngrep -oE \"[a-z\\']{2,}\" |                \nsort |              \ngrep -Fvwf stopwords |                  \nuniq -c |           \nsort -nr |          \nhead -n 10          \n \n \n \n \nFile Name to Write: top-words-1.sh                                              \n^G Help             M-D DOS Format      M-A Append          M-B Backup File     \n^C Cancel           M-M Mac Format      M-P Prepend         ^T Browse             GNU nano 5.4                     /tmp/zshpu0emE                               \ncurl -sL \"https://www.gutenberg.org/files/11/11-0.txt\" |                        \ntr '[:upper:]' '[:lower:]' |            \ngrep -oE \"[a-z\\']{2,}\" |                \nsort |              \ngrep -Fvwf stopwords |                  \nuniq -c |           \nsort -nr |          \nhead -n 10          \n \n \n \n \nSave file under DIFFERENT NAME?                                                 \n Y Yes                                                                          \n N No           ^C Cancel                                                         GNU nano 5.4                     top-words-1.sh                               \ncurl -sL \"https://www.gutenberg.org/files/11/11-0.txt\" |                        \ntr '[:upper:]' '[:lower:]' |            \ngrep -oE \"[a-z\\']{2,}\" |                \nsort |              \ngrep -Fvwf stopwords |                  \nuniq -c |           \nsort -nr |          \nhead -n 10          \n \n \n \n \n                               [ Wrote 8 lines ]                                \n^G Help      ^O Write Out ^W Where Is  ^K Cut       ^T Execute   ^C Location    \n^X Exit      ^R Read File ^\\ Replace   ^U Paste     ^J Justify   ^_ Go To Line  $ pwd\n/data/ch04\n \n$ l\ntotal 44K\n-rwxr-xr-x 1 dst dst  400 Dec 14 11:46 fizzbuzz.py*\n-rwxr-xr-x 1 dst dst  391 Dec 14 11:46 fizzbuzz.R*\n-rw-r--r-- 1 dst dst 7.5K Dec 14 11:47 stopwords\n-rwxr-xr-x 1 dst dst  182 Dec 14 11:46 stream.py*\n-rwxr-xr-x 1 dst dst  147 Dec 14 11:46 stream.R*\n-rw-r--r-- 1 dst dst  173 Dec 14 11:47 top-words-1.sh\n-rwxr-xr-x 1 dst dst  105 Dec 14 11:46 top-words-4.sh*\n-rwxr-xr-x 1 dst dst  128 Dec 14 11:46 top-words-5.sh*\n-rwxr-xr-x 1 dst dst  647 Dec 14 11:46 top-words.py*\n-rwxr-xr-x 1 dst dst  584 Dec 14 11:46 top-words.R*\n \n$ bat top-words-1.sh\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: top-words-1.sh\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ curl -sL \"https://www.gutenberg.org/files/11/11-0.txt\" |\n   2   │ tr '[:upper:]' '[:lower:]' |\n   3   │ grep -oE \"[a-z\\']{2,}\" |\n   4   │ sort |\n   5   │ grep -Fvwf stopwords |\n   6   │ uniq -c |\n   7   │ sort -nr |\n   8   │ head -n 10\n───────┴────────────────────────────────────────────────────────────────────────$ bash top-words-1.sh\n    403 alice\n     98 gutenberg\n     88 project\n     76 queen\n     71 time\n     63 king\n     60 turtle\n     57 mock\n     56 hatter\n     55 gryphon"},{"path":"chapter-4-creating-command-line-tools.html","id":"step-2-give-permission-to-execute","chapter":"4 Creating Command-line Tools","heading":"4.2.2 Step 2: Give Permission to Execute","text":"reason execute file directly don’t correct access permissions.\nparticular, , user, need permission execute file.\nsection change access permissions file.order compare differences steps, copy file top-words-2.sh using cp -v top-words-{1,2}.sh.change access permissions file, need use command-line tool called chmod52, stands change mode.\nchanges file mode bits specific file.\nfollowing command gives user, , permission execute top-words-2.sh:argument u+x consists three characters: (1) u indicates want change permissions user owns file, , created file; (2) + indicates want add permission; (3) x, indicates permissions execute.Let’s now look access permissions files:first column shows access permissions file.\ntop-words-2.sh, -rwxrw-r--.\nfirst character - (hyphen) indicates file type.\n- means regular file d means directory.\nnext three characters, rwx, indicate access permissions user owns file.\nr w mean read write, respectively.\n(can see, top-words-1.sh - instead x, means execute file.) next three characters rw- indicate access permissions members group owns file.\nFinally, last three characters column, r--, indicate access permissions users.Now can execute file follows:try execute file don’t correct access permissions, top-words-1.sh, see following error message:","code":"$ cp -v top-words-{1,2}.sh\n'top-words-1.sh' -> 'top-words-2.sh'\n \n$ chmod u+x top-words-2.sh$ l top-words-{1,2}.sh\n-rw-r--r-- 1 dst dst 173 Dec 14 11:47 top-words-1.sh\n-rwxr--r-- 1 dst dst 173 Dec 14 11:47 top-words-2.sh*$ ./top-words-2.sh\n    403 alice\n     98 gutenberg\n     88 project\n     76 queen\n     71 time\n     63 king\n     60 turtle\n     57 mock\n     56 hatter\n     55 gryphon$ ./top-words-1.sh\nzsh: permission denied: ./top-words-1.sh"},{"path":"chapter-4-creating-command-line-tools.html","id":"step-3-define-shebang","chapter":"4 Creating Command-line Tools","heading":"4.2.3 Step 3: Define Shebang","text":"Although can already execute file , add -called shebang file.\nshebang special line script instructs system executable use interpret commands.name shebang comes first two characters: hash () exclamation mark (bang): #!.\n’s good idea leave , done previous step, shell different default executable.\nZ shell, one ’re using throughout book, uses executable /bin/sh default shebang defined.\ncase ’d like bash interpret commands give us functionality sh., ’re free use whatever editor like, ’m going stick nano53, installed Docker image.Go ahead type #!/usr/bin/env/bash press Enter.\n’re ready, press Ctrl-X save exit.Press Y indicate want save file.Let’s confirm top-words-3.sh looks like:’s exactly need: original pipeline shebang front .Sometimes come across scripts shebang form !/usr/bin/bash !/usr/bin/python (case Python, see next section).\ngenerally works, bash python54 executables installed different location /usr/bin, script work anymore.\nbetter use form present , namely !/usr/bin/env bash !/usr/bin/env python, env55 executable aware bash python installed.\nshort, using env makes scripts portable.","code":"$ cp -v top-words-{2,3}.sh\n'top-words-2.sh' -> 'top-words-3.sh'\n \n$ nano top-words-3.sh  GNU nano 5.4                     top-words-3.sh                               \ncurl -sL \"https://www.gutenberg.org/files/11/11-0.txt\" |                        \ntr '[:upper:]' '[:lower:]' |            \ngrep -oE \"[a-z\\']{2,}\" |                \nsort |              \ngrep -Fvwf stopwords |                  \nuniq -c |           \nsort -nr |          \nhead -n 10          \n \n \n \n \n                                [ Read 8 lines ]                                \n^G Help      ^O Write Out ^W Where Is  ^K Cut       ^T Execute   ^C Location    \n^X Exit      ^R Read File ^\\ Replace   ^U Paste     ^J Justify   ^_ Go To Line    GNU nano 5.4                     top-words-3.sh *                             \n#!/usr/bin/env bash \ncurl -sL \"https://www.gutenberg.org/files/11/11-0.txt\" |                        \ntr '[:upper:]' '[:lower:]' |            \ngrep -oE \"[a-z\\']{2,}\" |                \nsort |              \ngrep -Fvwf stopwords |                  \nuniq -c |           \nsort -nr |          \nhead -n 10          \n \n \n \nSave modified buffer?                                                           \n Y Yes                                                                          \n N No           ^C Cancel                                                         GNU nano 5.4                     top-words-3.sh *                             \n#!/usr/bin/env bash \ncurl -sL \"https://www.gutenberg.org/files/11/11-0.txt\" |                        \ntr '[:upper:]' '[:lower:]' |            \ngrep -oE \"[a-z\\']{2,}\" |                \nsort |              \ngrep -Fvwf stopwords |                  \nuniq -c |           \nsort -nr |          \nhead -n 10          \n \n \n \nFile Name to Write: top-words-3.sh                                              \n^G Help             M-D DOS Format      M-A Append          M-B Backup File     \n^C Cancel           M-M Mac Format      M-P Prepend         ^T Browse           $ bat top-words-3.sh\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: top-words-3.sh\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ #!/usr/bin/env bash \n   2   │ curl -sL \"https://www.gutenberg.org/files/11/11-0.txt\" |\n   3   │ tr '[:upper:]' '[:lower:]' |\n   4   │ grep -oE \"[a-z\\']{2,}\" |\n   5   │ sort |\n   6   │ grep -Fvwf stopwords |\n   7   │ uniq -c |\n   8   │ sort -nr |\n   9   │ head -n 10\n───────┴────────────────────────────────────────────────────────────────────────"},{"path":"chapter-4-creating-command-line-tools.html","id":"step-4-remove-fixed-input","chapter":"4 Creating Command-line Tools","heading":"4.2.4 Step 4: Remove Fixed Input","text":"know valid command-line tool can execute command line.\ncan better .\ncan make command-line tool reusable.\nfirst command file curl, downloads text wish obtain top 10 -used words.\n, data operations combined one.wanted obtain top 10 -used words another e-book, text matter? input data fixed within tools .\nbetter separate data command-line tool.assume user command-line tool provide text, tool become generally applicable.\n, solution remove curl command script.\nupdated script named top-words-4.sh:works script starts command needs data standard input, like tr, take input given command-line tools.\nexample:","code":"$ cp -v top-words-{3,4}.sh\n'top-words-3.sh' -> 'top-words-4.sh'\n \n$ sed -i '2d' top-words-4.sh\n \n$ bat top-words-4.sh\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: top-words-4.sh\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ #!/usr/bin/env bash \n   2   │ tr '[:upper:]' '[:lower:]' |\n   3   │ grep -oE \"[a-z\\']{2,}\" |\n   4   │ sort |\n   5   │ grep -Fvwf stopwords |\n   6   │ uniq -c |\n   7   │ sort -nr |\n   8   │ head -n 10\n───────┴────────────────────────────────────────────────────────────────────────$ curl -sL 'https://www.gutenberg.org/files/11/11-0.txt' | ./top-words-4.sh\n    403 alice\n     98 gutenberg\n     88 project\n     76 queen\n     71 time\n     63 king\n     60 turtle\n     57 mock\n     56 hatter\n     55 gryphon\n \n$ curl -sL 'https://www.gutenberg.org/files/12/12-0.txt' | ./top-words-4.sh\n    469 alice\n    189 queen\n     98 gutenberg\n     88 project\n     72 time\n     71 red\n     70 white\n     67 king\n     63 head\n     59 knight\n \n$ man bash | ./top-words-4.sh\n    585 command\n    332 set\n    313 word\n    313 option\n    304 file\n    300 variable\n    298 bash\n    258 list\n    257 expansion\n    238 history"},{"path":"chapter-4-creating-command-line-tools.html","id":"step-5-add-arguments","chapter":"4 Creating Command-line Tools","heading":"4.2.5 Step 5: Add Arguments","text":"one step make command-line tool even reusable: parameters.\ncommand-line tool number fixed command-line arguments, example -nr sort -n 10 head.\nprobably best keep former argument fixed.\nHowever, useful allow different values head command.\nallow end user set number -often used words output.\nshows file top-words-5.sh looks like:variable NUM_WORDS set value $1, special variable Bash. holds value first command-line argument passed command-line tool. table lists special variables Bash offers. value specified, take value “10.”Note order use value $NUM_WORDS variable, need put dollar sign front . set , don’t write dollar sign.also used $1 directly argument head bother creating extra variable NUM_WORDS.\nHowever, larger scripts command-line arguments $2 $3, code becomes readable use named variables.Now wanted see top 20 -used words text, invoke command-line tool follows:user specify number, script show top 10 common words:","code":"$ bat top-words-5.sh\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: top-words-5.sh\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ #!/usr/bin/env bash\n   2   │\n   3   │ NUM_WORDS=\"${1:-10}\"\n   4   │\n   5   │ tr '[:upper:]' '[:lower:]' |\n   6   │ grep -oE \"[a-z\\']{2,}\" |\n   7   │ sort |\n   8   │ grep -Fvwf stopwords |\n   9   │ uniq -c |\n  10   │ sort -nr |\n  11   │ head -n \"${NUM_WORDS}\"\n───────┴────────────────────────────────────────────────────────────────────────$ curl -sL \"https://www.gutenberg.org/files/11/11-0.txt\" > alice.txt\n \n$ < alice.txt ./top-words-5.sh 20\n    403 alice\n     98 gutenberg\n     88 project\n     76 queen\n     71 time\n     63 king\n     60 turtle\n     57 mock\n     56 hatter\n     55 gryphon\n     53 rabbit\n     50 head\n     48 voice\n     45 looked\n     44 mouse\n     42 duchess\n     40 tone\n     40 dormouse\n     37 cat\n     34 march$ < alice.txt ./top-words-5.sh\n    403 alice\n     98 gutenberg\n     88 project\n     76 queen\n     71 time\n     63 king\n     60 turtle\n     57 mock\n     56 hatter\n     55 gryphon"},{"path":"chapter-4-creating-command-line-tools.html","id":"step-6-extend-your-path","chapter":"4 Creating Command-line Tools","heading":"4.2.6 Step 6: Extend Your PATH","text":"previous five steps finally finished building reusable command-line tool.\n, however, one step can useful.\noptional step going ensure can execute command-line tools everywhere.Currently, want execute command-line tool, either navigate directory include full path name shown step 2.\nfine command-line tool specifically built , say, certain project.\nHowever, command-line tool applied multiple situations, useful able execute everywhere, just like command-line tools come Ubuntu.accomplish , Bash needs know look command-line tools.\ntraversing list directories stored environment variable called PATH.\nfresh Docker container, PATH looks like :directories delimited colons.\ncan print list directories translating colons newlines:change PATH permanently, ’ll need edit .bashrc .profile file located home directory.\nput custom command-line tools one directory, say, ~/tools, change PATH .\nNow, longer need add ./, can just use filename.\nMoreover, longer need remember command-line tool located.","code":"$ echo $PATH\n/usr/local/lib/R/site-library/rush/exec:/usr/bin/dsutils:/home/dst/.local/bin:/u\nsr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin$ echo $PATH | tr ':' '\\n'\n/usr/local/lib/R/site-library/rush/exec\n/usr/bin/dsutils\n/home/dst/.local/bin\n/usr/local/sbin\n/usr/local/bin\n/usr/sbin\n/usr/bin\n/sbin\n/bin$ cp -v top-words{-5.sh,}\n'top-words-5.sh' -> 'top-words'\n \n$ export PATH=\"${PATH}:/data/ch04\"\n \n$ echo $PATH\n/usr/local/lib/R/site-library/rush/exec:/usr/bin/dsutils:/home/dst/.local/bin:/u\nsr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/data/ch04\n \n$ curl \"https://www.gutenberg.org/files/11/11-0.txt\" |\n> top-words 10\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  170k  100  170k    0     0   115k      0  0:00:01  0:00:01 --:--:--  116k\n    403 alice\n     98 gutenberg\n     88 project\n     76 queen\n     71 time\n     63 king\n     60 turtle\n     57 mock\n     56 hatter\n     55 gryphon"},{"path":"chapter-4-creating-command-line-tools.html","id":"creating-command-line-tools-with-python-and-r","chapter":"4 Creating Command-line Tools","heading":"4.3 Creating Command-line Tools with Python and R","text":"command-line tool created previous section written Bash.\n(Sure, every feature Bash programming language employed, interpreter still bash.) know now, command line language agnostic, don’t necessarily use Bash creating command-line tools.section ’m going demonstrate command-line tools can created programming languages well.\n’ll focus Python R two popular programming languages within data science community.\noffer complete introduction either language, assume familiarity Python R.\nprogramming languages Java, Go, Julia, follow similar pattern comes creating command-line tools.three main reasons creating command-line tools another programming language Bash.\nFirst, may already code ’d like able use command line.\nSecond, command-line tool end encompassing hundred lines Bash code.\nThird, command-line tool needs safe robust (Bash lacks many features type checking).six steps discussed previous section roughly apply creating command-line tools programming languages well.\nfirst step, however, copy pasting command line, rather copy pasting relevant code new file.\nCommand-line tools written Python R need specify python Rscript56, respectively, interpreter shebang.comes creating command-line tools using Python R, two aspects deserve special attention.\nFirst, processing standard input, comes natural shell scripts, taken care explicitly Python R.\nSecond, command-line tools written Python R tend complex, may also want offer user ability specify elaborate command-line arguments.","code":""},{"path":"chapter-4-creating-command-line-tools.html","id":"porting-the-shell-script","chapter":"4 Creating Command-line Tools","heading":"4.3.1 Porting The Shell Script","text":"starting point, let’s see port shell script just created Python R.\nwords, Python R code gives us top -often used words standard input? first show two files top-words.py top-words.R discuss differences shell code.\nPython, code look something like:Note Python example doesn’t use third-party packages.\nwant advanced text processing, recommend check NLTK package57.\n’re going work lot numerical data, recommend use Pandas package58.R code look something like:Let’s check three implementations (.e., Bash, Python, R) return top 5 words counts:Wonderful! Sure, output exciting.\n’s exciting can accomplish task multiple languages.\nLet’s look differences approaches.First, ’s immediately obvious difference amount code.\nspecific task, Python R require much code Bash.\nillustrates , tasks, better use command line.\ntasks, may better using programming language.\ngain experience command line, start recognize use approach.\neverything command-line tool, can even split task subtasks, combine Bash command-line tool , say, Python command-line tool.\nWhichever approach works best task hand.","code":"$ cd /data/ch04\n \n$ bat top-words.py\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: top-words.py\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ #!/usr/bin/env python\n   2   │ import re\n   3   │ import sys\n   4   │\n   5   │ from collections import Counter\n   6   │ from urllib.request import urlopen\n   7   │\n   8   │ def top_words(text, n):\n   9   │     with urlopen(\"https://raw.githubusercontent.com/stopwords-iso/stopw\n       │ ords-en/master/stopwords-en.txt\") as f:\n  10   │         stopwords = f.read().decode(\"utf-8\").split(\"\\n\")\n  11   │\n  12   │     words = re.findall(\"[a-z']{2,}\", text.lower())\n  13   │     words = (w for w in words if w not in stopwords)\n  14   │\n  15   │     for word, count in Counter(words).most_common(n):\n  16   │         print(f\"{count:>7} {word}\")\n  17   │\n  18   │\n  19   │ if __name__ == \"__main__\":\n  20   │     text = sys.stdin.read()\n  21   │\n  22   │     try:\n  23   │         n = int(sys.argv[1])\n  24   │     except:\n  25   │         n = 10\n  26   │\n  27   │     top_words(text, n)\n───────┴────────────────────────────────────────────────────────────────────────$ bat top-words.R\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: top-words.R\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ #!/usr/bin/env Rscript\n   2   │ n <- as.integer(commandArgs(trailingOnly = TRUE))\n   3   │ if (length(n) == 0) n <- 10\n   4   │\n   5   │ f_stopwords <- url(\"https://raw.githubusercontent.com/stopwords-iso/sto\n       │ pwords-en/master/stopwords-en.txt\")\n   6   │ stopwords <- readLines(f_stopwords, warn = FALSE)\n   7   │ close(f_stopwords)\n   8   │\n   9   │ f_text <- file(\"stdin\")\n  10   │ lines <- tolower(readLines(f_text))\n  11   │\n  12   │ words <- unlist(regmatches(lines, gregexpr(\"[a-z']{2,}\", lines)))\n  13   │ words <- words[is.na(match(words, stopwords))]\n  14   │\n  15   │ counts <- sort(table(words), decreasing = TRUE)\n  16   │ cat(sprintf(\"%7d %s\\n\", counts[1:n], names(counts[1:n])), sep = \"\")\n  17   │ close(f_text)\n───────┴────────────────────────────────────────────────────────────────────────$ time < alice.txt top-words 5\n    403 alice\n     98 gutenberg\n     88 project\n     76 queen\n     71 time\ntop-words 5 < alice.txt  0.38s user 0.04s system 149% cpu 0.278 total\n \n$ time < alice.txt top-words.py 5\n    403 alice\n     98 gutenberg\n     88 project\n     76 queen\n     71 time\ntop-words.py 5 < alice.txt  1.36s user 0.03s system 95% cpu 1.454 total\n \n$ time < alice.txt top-words.R 5\n    403 alice\n     98 gutenberg\n     88 project\n     76 queen\n     71 time\ntop-words.R 5 < alice.txt  1.48s user 0.14s system 91% cpu 1.774 total"},{"path":"chapter-4-creating-command-line-tools.html","id":"processing-streaming-data-from-standard-input","chapter":"4 Creating Command-line Tools","heading":"4.3.2 Processing Streaming Data from Standard Input","text":"previous two code snippets, Python R read complete standard input .\ncommand line, tools pipe data next command-line tool streaming fashion.\ncommand-line tools require complete data write data standard output, like sort.\nmeans pipeline blocked command-line tools.\ndoesn’t problem input data finite, like file.\nHowever, input data non-stop stream, blocking command-line tools useless.Luckily Python R support processing streaming data.\ncan apply function line-per-line basis, example.\ntwo minimal examples demonstrate works Python R, respectively.Python R tool solve , now infamous, Fizz Buzz problem, defined follows: Print numbers 1 100, except number divisible 3, instead print “fizz”; number divisible 5, instead print “buzz”; number divisible 15, instead print “fizzbuzz.” ’s Python code59:’s R code:Let’s test tools (save space pipe output column):output looks correct !\n’s difficult demonstrate two tools actually work streaming manner.\ncan verify piping input data sample -d 100 ’s piped Python R tool.\nway, ’ll add small delay line ’s easier confirm tools don’t wait input data, operate line line basis.","code":"$ bat fizzbuzz.py\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: fizzbuzz.py\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ #!/usr/bin/env python\n   2   │ import sys\n   3   │\n   4   │ CYCLE_OF_15 = [\"fizzbuzz\", None, None, \"fizz\", None,\n   5   │                \"buzz\", \"fizz\", None, None, \"fizz\",\n   6   │                \"buzz\", None, \"fizz\", None, None]\n   7   │\n   8   │ def fizz_buzz(n: int) -> str:\n   9   │     return CYCLE_OF_15[n % 15] or str(n)\n  10   │\n  11   │ if __name__ == \"__main__\":\n  12   │     try:\n  13   │         while (n:= sys.stdin.readline()):\n  14   │             print(fizz_buzz(int(n)))\n  15   │     except:\n  16   │         pass\n───────┴────────────────────────────────────────────────────────────────────────$ bat fizzbuzz.R\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: fizzbuzz.R\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ #!/usr/bin/env Rscript\n   2   │ cycle_of_15 <- c(\"fizzbuzz\", NA, NA, \"fizz\", NA,\n   3   │                  \"buzz\", \"fizz\", NA, NA, \"fizz\",\n   4   │                  \"buzz\", NA, \"fizz\", NA, NA)\n   5   │\n   6   │ fizz_buzz <- function(n) {\n   7   │   word <- cycle_of_15[as.integer(n) %% 15 + 1]\n   8   │   ifelse(is.na(word), n, word)\n   9   │ }\n  10   │\n  11   │ f <- file(\"stdin\")\n  12   │ open(f)\n  13   │ while(length(n <- readLines(f, n = 1)) > 0) {\n  14   │   write(fizz_buzz(n), stdout())\n  15   │ }\n  16   │ close(f)\n───────┴────────────────────────────────────────────────────────────────────────$ seq 30 | fizzbuzz.py | column -x\n1               2               fizz            4               buzz\nfizz            7               8               fizz            buzz\n11              fizz            13              14              fizzbuzz\n16              17              fizz            19              buzz\nfizz            22              23              fizz            buzz\n26              fizz            28              29              fizzbuzz\n \n$ seq 30 | fizzbuzz.R | column -x\n1               2               fizz            4               buzz\nfizz            7               8               fizz            buzz\n11              fizz            13              14              fizzbuzz\n16              17              fizz            19              buzz\nfizz            22              23              fizz            buzz\n26              fizz            28              29              fizzbuzz"},{"path":"chapter-4-creating-command-line-tools.html","id":"summary-3","chapter":"4 Creating Command-line Tools","heading":"4.4 Summary","text":"intermezzo chapter, shown build command-line tool.\nsix steps needed turn code reusable building block.\n’ll find makes much productive.\nadvise keep eye opportunities create tools.\nnext chapter covers second step OSEMN model data science, namely scrubbing data.","code":""},{"path":"chapter-4-creating-command-line-tools.html","id":"for-further-exploration-3","chapter":"4 Creating Command-line Tools","heading":"4.5 For Further Exploration","text":"Adding help documentation tool becomes important tool many options remember, even want share tool others. docopt language-agnostic framework provide help define possible options tool accepts. implementations available just programming language including Bash, Python, R.want learn programming Bash, recommend Classic Shell Programming Arnold Robbins Nelson Beebe Bash Cookbook Carl Albing JP Vossen.Writing robust safe Bash script quite tricky. ShellCheck online tool check Bash code mistakes vulnerabilities. ’s also command-line tool available.book Ten Essays Fizz Buzz Joel Grus insightful fun collection ten different ways solve Fizz Buzz Python.","code":""},{"path":"chapter-5-scrubbing-data.html","id":"chapter-5-scrubbing-data","chapter":"5 Scrubbing Data","heading":"5 Scrubbing Data","text":"Two chapters ago, first step OSEMN model data science, looked obtaining data variety sources.\nchapter second step: scrubbing data.\nsee, ’s quite rare can immediately continue exploring even modeling data.\n’s plethora reasons data first needs cleaning, scrubbing.starters, data might desired format.\nexample, may obtained JSON data API, need CSV format create visualization.\ncommon formats include plain text, HTML, XML.\ncommand-line tools work one two formats, ’s important ’re able convert data one format another.data desired format, still issues like missing values, inconsistencies, weird characters, unnecessary parts.\ncan fix applying filters, replacing values, combining multiple files.\ncommand line especially well-suited kind transformations, many specialized tools available, can handle large amounts data.\nchapter ’ll discuss classic tools grep60 awk61, newer tools jq62 pup63.Sometimes can use command-line tool perform several operations multiple tools perform operation.\nchapter structured like cookbook, focus problems recipes, rather diving deeply command-line tools .","code":""},{"path":"chapter-5-scrubbing-data.html","id":"overview-2","chapter":"5 Scrubbing Data","heading":"5.1 Overview","text":"chapter, ’ll learn :Convert data one format anotherApply SQL queries directly CSVFilter linesExtract replace valuesSplit, merge, extract columnsCombine multiple filesThis chapter starts following files:instructions get files Chapter 2.\nfiles either downloaded generated using command-line tools.dive actual transformations, ’d like demonstrate ubiquity working command line.","code":"$ cd /data/ch05\n \n$ l\ntotal 200K\n-rw-r--r-- 1 dst dst 164K Dec 14 11:47 alice.txt\n-rw-r--r-- 1 dst dst 4.5K Dec 14 11:47 iris.csv\n-rw-r--r-- 1 dst dst  179 Dec 14 11:47 irismeta.csv\n-rw-r--r-- 1 dst dst  160 Dec 14 11:47 names-comma.csv\n-rw-r--r-- 1 dst dst  129 Dec 14 11:47 names.csv\n-rw-r--r-- 1 dst dst 7.8K Dec 14 11:47 tips.csv\n-rw-r--r-- 1 dst dst 5.1K Dec 14 11:47 users.json"},{"path":"chapter-5-scrubbing-data.html","id":"transformations-transformations-everywhere","chapter":"5 Scrubbing Data","heading":"5.2 Transformations, Transformations Everywhere","text":"Chapter 1 mentioned , practice, steps OSEMN model rarely followed linearly.\nvein, although scrubbing second step OSEMN model, want know ’s just obtained data needs scrubbing.\ntransformations ’ll learn chapter can useful part pipeline step OSEMN model.\nGenerally, one command line tool generates output can used immediately next tool, can chain two tools together using pipe operator (|).\nOtherwise, transformation needs applied data first inserting intermediate tool pipeline.Let walk example make concrete.\nImagine obtained first 100 items fizzbuzz sequence (cf. Chapter 4) ’d like visualize often words fizz, buzz, fizzbuzz appear using bar chart.\nDon’t worry example uses tools might familiar yet, ’ll covered detail later.First obtain data generating sequence write fb.seq:➊ custom tool fizzbuzz.py comes Chapter 4.use grep keep lines match pattern fizz buzz count often word appears using sort uniq64:➊ regular expression also matches fizzbuzz.\n➋ Using sort uniq way common way count lines sort descending order. ’s -c option adds counts.Note sort used twice: first uniq assumes input data sorted second sort counts numerically.\nway, intermediate transformation, albeit subtle one.next step visualize counts using rush65.\nHowever, since rush expects input data CSV format, requires less subtle transformation first.\nawk can add header, flip two fields, insert commas single incantation:Now ’re ready use rush create bar chart.\nSee Figure 5.1 result.\n(’ll cover syntax rush detail Chapter 7.)\nFigure 5.1: Counting fizz, buzz, fizzbuzz\nAlthough example bit contrived, reveals pattern common working command line.\nkey tools, ones obtain data, create visualization, train model, often require intermediate transformations order chained pipeline.\nsense, writing pipeline like solving puzzle, key pieces often require helper pieces fit.Now ’ve seen importance scrubbing data, ’re ready learn actual transformations.","code":"$ seq 100 |\n> /data/ch04/fizzbuzz.py | ➊\n> tee fb.seq | trim\n1\n2\nfizz\n4\nbuzz\nfizz\n7\n8\nfizz\nbuzz\n… with 90 more lines$ grep -E \"fizz|buzz\" fb.seq | ➊\n> sort | uniq -c | sort -nr > fb.cnt ➋\n \n$ bat -A fb.cnt\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: fb.cnt\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ ·····27·fizz␊\n   2   │ ·····14·buzz␊\n   3   │ ······6·fizzbuzz␊\n───────┴────────────────────────────────────────────────────────────────────────$ < fb.cnt awk 'BEGIN { print \"value,count\" } { print $2\",\"$1 }' > fb.csv\n \n$ bat fb.csv\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: fb.csv\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ value,count\n   2   │ fizz,27\n   3   │ buzz,14\n   4   │ fizzbuzz,6\n───────┴────────────────────────────────────────────────────────────────────────\n \n$ csvlook fb.csv\n│ value    │ count │\n├──────────┼───────┤\n│ fizz     │    27 │\n│ buzz     │    14 │\n│ fizzbuzz │     6 │$ rush plot -x value -y count --geom col --height 2 fb.csv > fb.png\n \n$ display fb.png"},{"path":"chapter-5-scrubbing-data.html","id":"plain-text","chapter":"5 Scrubbing Data","heading":"5.3 Plain Text","text":"Formally speaking, plain text refers sequence human-readable characters optionally, specific types control characters tabs newlines66.\nExamples logs, e-books, emails, source code.\nPlain text many benefits binary data67, including:can opened, edited, saved using text editorIt’s self-describing independent application created itIt outlive forms data, additional knowledge applications required process itBut importantly, Unix philosophy considers plain text universal interface command-line tools68.\nMeaning, tools accept plain text input produce plain text output.’s reason enough start plain text.\nformats discuss chapter, CSV, JSON, XML, HTML indeed also plain text.\nnow, assume plain text clear tabular structure (like CSV ) nested structure (like JSON, XML, HTML ).\nLater chapter, ’ll introduce tools specifically designed working formats.","code":""},{"path":"chapter-5-scrubbing-data.html","id":"filtering-lines","chapter":"5 Scrubbing Data","heading":"5.3.1 Filtering Lines","text":"first scrubbing operation filtering lines.\nmeans input data, line evaluated whether kept discarded.","code":""},{"path":"chapter-5-scrubbing-data.html","id":"based-on-location","chapter":"5 Scrubbing Data","heading":"5.3.1.1 Based on Location","text":"straightforward way filter lines based location.\nmay useful want inspect, say, top 10 lines file, extract specific row output another command-line tool.\nillustrate filter based location, let’s create dummy file contains 10 lines:can print first 3 lines using either head69, sed70, awk:➊ awk, NR refers total number input records seen far.Similarly, can print last 3 lines using tail71:can also use sed awk , tail much faster.\nRemoving first 3 lines goes follows:Notice tail specify number lines plus one.\nThink line want start printing.\nRemoving last 3 lines can done head:can print specific lines using either sed, awk, combination head tail.\nprint lines 4, 5, 6:can print odd lines sed specifying start step, awk using modulo operator:Printing even lines works similar manner:","code":"$ seq -f \"Line %g\" 10 | tee lines\nLine 1\nLine 2\nLine 3\nLine 4\nLine 5\nLine 6\nLine 7\nLine 8\nLine 9\nLine 10$ < lines head -n 3\nLine 1\nLine 2\nLine 3\n \n$ < lines sed -n '1,3p'\nLine 1\nLine 2\nLine 3\n \n$ < lines awk 'NR <= 3' ➊\nLine 1\nLine 2\nLine 3$ < lines tail -n 3\nLine 8\nLine 9\nLine 10$ < lines tail -n +4\nLine 4\nLine 5\nLine 6\nLine 7\nLine 8\nLine 9\nLine 10\n \n$ < lines sed '1,3d'\nLine 4\nLine 5\nLine 6\nLine 7\nLine 8\nLine 9\nLine 10\n \n$ < lines sed -n '1,3!p'\nLine 4\nLine 5\nLine 6\nLine 7\nLine 8\nLine 9\nLine 10$ < lines head -n -3\nLine 1\nLine 2\nLine 3\nLine 4\nLine 5\nLine 6\nLine 7$ < lines sed -n '4,6p'\nLine 4\nLine 5\nLine 6\n \n$ < lines awk '(NR>=4) && (NR<=6)'\nLine 4\nLine 5\nLine 6\n \n$ < lines head -n 6 | tail -n 3\nLine 4\nLine 5\nLine 6$ < lines sed -n '1~2p'\nLine 1\nLine 3\nLine 5\nLine 7\nLine 9\n \n$ < lines awk 'NR%2'\nLine 1\nLine 3\nLine 5\nLine 7\nLine 9$ < lines sed -n '0~2p'\nLine 2\nLine 4\nLine 6\nLine 8\nLine 10\n \n$ < lines awk '(NR+1)%2'\nLine 2\nLine 4\nLine 6\nLine 8\nLine 10"},{"path":"chapter-5-scrubbing-data.html","id":"based-on-a-pattern","chapter":"5 Scrubbing Data","heading":"5.3.1.2 Based on a Pattern","text":"Sometimes want keep discard lines based contents.\ngrep, canonical command-line tool filtering lines, can print every line matches certain pattern regular expression.\nexample, extract chapter headings Alice’s Adventures Wonderland:➊ -options specifies matching case-insensitive.can also specify regular expression.\nexample, wanted print headings start :Note specify -E option order enable regular expressions.\nOtherwise, grep interprets pattern literal string likely results matches :-v option invert matches, grep prints lines don’t match pattern.\nregular expression matches lines contain white space, .\ninverse, using wc -l, can count number non-empty lines:","code":"$ < alice.txt grep -i chapter ➊\nCHAPTER I. Down the Rabbit-Hole\nCHAPTER II. The Pool of Tears\nCHAPTER III. A Caucus-Race and a Long Tale\nCHAPTER IV. The Rabbit Sends in a Little Bill\nCHAPTER V. Advice from a Caterpillar\nCHAPTER VI. Pig and Pepper\nCHAPTER VII. A Mad Tea-Party\nCHAPTER VIII. The Queen's Croquet-Ground\nCHAPTER IX. The Mock Turtle's Story\nCHAPTER X. The Lobster Quadrille\nCHAPTER XI. Who Stole the Tarts?\nCHAPTER XII. Alice's Evidence$ < alice.txt grep -E '^CHAPTER (.*)\\. The'\nCHAPTER II. The Pool of Tears\nCHAPTER IV. The Rabbit Sends in a Little Bill\nCHAPTER VIII. The Queen's Croquet-Ground\nCHAPTER IX. The Mock Turtle's Story\nCHAPTER X. The Lobster Quadrille$ < alice.txt grep '^CHAPTER (.*)\\. The'$ < alice.txt grep -Ev '^\\s$' | wc -l\n2790"},{"path":"chapter-5-scrubbing-data.html","id":"based-on-randomness","chapter":"5 Scrubbing Data","heading":"5.3.1.3 Based on Randomness","text":"’re process formulating data pipeline lot data, debugging pipeline can cumbersome.\ncase, generating smaller sample data might useful.\nsample72 comes handy.\nmain purpose sample get subset data outputting certain percentage input line--line basis., every input line one percent chance printed.\npercentage can also specified fraction (namely 1/100) probability (namely 0.01).sample two purposes, can useful ’re debugging pipeline.\nFirst, ’s possible add delay output.\ncomes handy input constant stream (example, Wikipedia stream saw Chapter 3), data comes fast see ’s going .\nSecondly, can put timer sample, don’t kill ongoing process manually.\nexample, add 1 second delay line printed run 5 seconds, type:➊ tool ts73 adds timestamp front line.order prevent unnecessary computation, try put sample early possible pipeline.\nfact, argument holds command-line tool reduces data, like head tail.\n’re confident pipeline works, take pipeline.","code":"$ seq -f \"Line %g\" 1000 | sample -r 1%\nLine 45\nLine 223\nLine 355\nLine 369\nLine 438\nLine 807\nLine 813$ seq -f \"Line %g\" 1000 | sample -r 1% -d 1000 -s 5 | ts ➊\nDec 14 11:48:10 Line 58\nDec 14 11:48:11 Line 60\nDec 14 11:48:12 Line 230\nDec 14 11:48:13 Line 250\nDec 14 11:48:14 Line 785\nDec 14 11:48:15 Line 786"},{"path":"chapter-5-scrubbing-data.html","id":"extracting-values","chapter":"5 Scrubbing Data","heading":"5.3.2 Extracting Values","text":"extract actual chapter headings example earlier, can take simple approach piping output grep cut:, line ’s passed cut split spaces fields, third field last field printed.\ntotal number fields can different per input line.\nsed can accomplish task much complex manner:(Since output ’s trimmed three lines.) approach uses regular expression back reference.\n, sed also takes work done grep.\nrecommend using complicated approach simpler one work.\nexample, chapter ever part text just used indicate start new chapter.\ncourse many levels complexity worked around , illustrate extremely strict approach.\npractice, challenge come pipeline strikes good balance complexity flexibility.’s worth noting cut can also split characters positions.\nuseful want extract (remove) set characters per input line:grep great feature outputs every match onto separate line using -o option:wanted create dataset words start end e?\nWell, course ’s pipeline :➊ use tr make text lowercase. ’ll closer look tr next section.two grep commands might combined one, case decided easier reuse adapt previous pipeline.\n’s shame pragmatic order get job done!","code":"$ grep -i chapter alice.txt | cut -d ' ' -f 3-\nDown the Rabbit-Hole\nThe Pool of Tears\nA Caucus-Race and a Long Tale\nThe Rabbit Sends in a Little Bill\nAdvice from a Caterpillar\nPig and Pepper\nA Mad Tea-Party\nThe Queen's Croquet-Ground\nThe Mock Turtle's Story\nThe Lobster Quadrille\nWho Stole the Tarts?\nAlice's Evidence$ sed -rn 's/^CHAPTER ([IVXLCDM]{1,})\\. (.*)$/\\2/p' alice.txt | trim 3\nDown the Rabbit-Hole\nThe Pool of Tears\nA Caucus-Race and a Long Tale\n… with 9 more lines$ grep -i chapter alice.txt | cut -c 9-\nI. Down the Rabbit-Hole\nII. The Pool of Tears\nIII. A Caucus-Race and a Long Tale\nIV. The Rabbit Sends in a Little Bill\nV. Advice from a Caterpillar\nVI. Pig and Pepper\nVII. A Mad Tea-Party\nVIII. The Queen's Croquet-Ground\nIX. The Mock Turtle's Story\nX. The Lobster Quadrille\nXI. Who Stole the Tarts?\nXII. Alice's Evidence$ < alice.txt grep -oE '\\w{2,}' | trim\nProject\nGutenberg\nAlice\nAdventures\nin\nWonderland\nby\nLewis\nCarroll\nThis\n… with 28615 more lines$ < alice.txt tr '[:upper:]' '[:lower:]' | ➊\n> grep -oE '\\w{2,}' |\n> grep -E '^a.*e$' |\n> sort | uniq | sort -nr | trim\navailable\nate\nassistance\naskance\narise\nargue\nare\narchive\napplicable\napple\n… with 25 more lines"},{"path":"chapter-5-scrubbing-data.html","id":"replacing-and-deleting-values","chapter":"5 Scrubbing Data","heading":"5.3.3 Replacing and Deleting Values","text":"can use command-line tool tr74, stands translate, replace delete individual characters.\nexample, spaces can replaced underscores follows:one character needs replaced, can combine :tr can also used delete individual characters specifying argument -d:case, two commands accomplish thing.\nsecond command, however, uses two additional features:\nspecifies range characters (lowercase letters) using square brackets dash ([-]), -c option indicates complement used.\nwords, command keeps lowercase letters.\ncan even use tr convert text uppercase:However, need translate non-ASCII characters, tr may work operates single-byte characters . cases use sed instead:need operate individual characters, may find sed useful.\n’ve already seen example sed extracting chapter headings alice.txt.\nExtracting, deleting, replacing actually operation sed.\njust specify different regular expressions.\nexample, change word, remove repeated spaces, remove leading spaces:➊ Replace hello bye.\n➋ Replace whitespace one space. flag g stands global, meaning substitution can applied line.\n➌ removes leading spaces didn’t specify flag g ., just grep example earlier, three sed commands can combined one:tell , find easier read?","code":"$ echo 'hello world!' | tr ' ' '_'\nhello_world!$ echo 'hello world!' | tr ' !' '_?'\nhello_world?$ echo 'hello world!' | tr -d ' !'\nhelloworld\n \n$ echo 'hello world!' | tr -d -c '[a-z]'\nhelloworld%$ echo 'hello world!' | tr '[a-z]' '[A-Z]'\nHELLO WORLD!\n \n$ echo 'hello world!' | tr '[:lower:]' '[:upper:]'\nHELLO WORLD!$ echo 'hello world!' | tr '[a-z]' '[A-Z]'\nHELLO WORLD!\n \n$ echo 'hallo wêreld!' | tr '[a-z]' '[A-Z]'\nHALLO WêRELD!\n \n$ echo 'hallo wêreld!' | tr '[:lower:]' '[:upper:]'\nHALLO WêRELD!\n \n$ echo 'hallo wêreld!' | sed 's/[[:lower:]]*/\\U&/g'\nHALLO WÊRELD!\n \n$ echo 'helló világ' | tr '[:lower:]' '[:upper:]'\nHELLó VILáG\n \n$ echo 'helló világ' | sed 's/[[:lower:]]*/\\U&/g'\nHELLÓ VILÁG$ echo ' hello     world!' |\n> sed -re 's/hello/bye/' | ➊\n> sed -re 's/\\s+/ /g' | ➋\n> sed -re 's/\\s+//' ➌\nbye world!$ echo ' hello     world!' |\n> sed -re 's/hello/bye/;s/\\s+/ /g;s/\\s+//'\nbye world!"},{"path":"chapter-5-scrubbing-data.html","id":"csv","chapter":"5 Scrubbing Data","heading":"5.4 CSV","text":"","code":""},{"path":"chapter-5-scrubbing-data.html","id":"bodies-and-headers-and-columns-oh-my","chapter":"5 Scrubbing Data","heading":"5.4.1 Bodies and Headers and Columns, Oh My!","text":"command-line tools ’ve used scrub plain text, tr grep, always applied CSV.\nreason command-line tools notion headers, bodies, columns.\nwant filter lines using grep always include header output?\nwant uppercase values specific column using tr leave columns untouched?multi-step workarounds , cumbersome.\nsomething better.\norder leverage ordinary command-line tools CSV, ’d like introduce three command-line tools, aptly named: body75, header76, cols77.Let’s start first command-line tool, body.\nbody can apply command-line tool body CSV file, , everything excluding header.\nexample:assumes header CSV file spans one row.\nworks like :Take one line standard store variable named $header.Print header.Execute command-line arguments passed body remaining data standard .’s another example.\nImagine want count lines following CSV file:wc -l, can count number lines:want consider lines body (everything except header), add body:Note header used also printed output.second command-line tool, header allows manipulate header CSV file.\narguments provided, header CSV file printed:head -n 1.\nheader spans one row, recommended, can specify -n 2.\ncan also add header CSV file:equivalent echo \"count\" | cat - <(seq 5).\nDeleting header done -d option:similar tail -n +2, ’s bit easier remember.\nReplacing header, basically first deleting header adding one look source code, accomplished specifying -r option. , combine body:last least, can apply command just header, similar body command-line tool body.\nexample:third command-line tool called cols, allows apply certain command subset columns.\nexample, wanted uppercase values day column tips data set (without affecting columns header), use cols combination body, follows:Please note passing multiple command-line tools arguments command header -e, body, cols can lead tricky quoting citations.\never run problems, ’s best create separate command-line tool pass command.conclusion, generally preferable use command-line tools specifically made CSV data, body, header, cols also allow apply classic command-line tools CSV files needed.","code":"$ echo -e \"value\\n7\\n2\\n5\\n3\" | body sort -n\nvalue\n2\n3\n5\n7$ seq 5 | header -a count\ncount\n1\n2\n3\n4\n5$ seq 5 | header -a count | wc -l\n6$ seq 5 | header -a count | body wc -l\ncount\n5$ < tips.csv header\nbill,tip,sex,smoker,day,time,size$ seq 5 | header -a count\ncount\n1\n2\n3\n4\n5$ < iris.csv header -d | trim\n5.1,3.5,1.4,0.2,Iris-setosa\n4.9,3.0,1.4,0.2,Iris-setosa\n4.7,3.2,1.3,0.2,Iris-setosa\n4.6,3.1,1.5,0.2,Iris-setosa\n5.0,3.6,1.4,0.2,Iris-setosa\n5.4,3.9,1.7,0.4,Iris-setosa\n4.6,3.4,1.4,0.3,Iris-setosa\n5.0,3.4,1.5,0.2,Iris-setosa\n4.4,2.9,1.4,0.2,Iris-setosa\n4.9,3.1,1.5,0.1,Iris-setosa\n… with 140 more lines$ seq 5 | header -a line | body wc -l | header -r count\ncount\n5$ seq 5 | header -a line | header -e \"tr '[a-z]' '[A-Z]'\"\nLINE\n1\n2\n3\n4\n5$ < tips.csv cols -c day body \"tr '[a-z]' '[A-Z]'\" | head -n 5 | csvlook\n│        day │  bill │  tip │ sex    │ smoker │ time   │ size │\n├────────────┼───────┼──────┼────────┼────────┼────────┼──────┤\n│ 0001-01-07 │ 16.99 │ 1.01 │ Female │  False │ Dinner │    2 │\n│ 0001-01-07 │ 10.34 │ 1.66 │ Male   │  False │ Dinner │    3 │\n│ 0001-01-07 │ 21.01 │ 3.50 │ Male   │  False │ Dinner │    3 │\n│ 0001-01-07 │ 23.68 │ 3.31 │ Male   │  False │ Dinner │    2 │"},{"path":"chapter-5-scrubbing-data.html","id":"performing-sql-queries-on-csv","chapter":"5 Scrubbing Data","heading":"5.4.2 Performing SQL Queries on CSV","text":"case command-line tools mentioned chapter provide enough flexibility, another approach scrub data command line.\ntool csvsql78 allows execute SQL queries directly CSV files.\nSQL powerful language define operations scrubbing data; ’s different way using individual command-line tools.scrubbing tasks , ’ll include several solutions involve csvsql. basic command :pass standard input csvsql, table named stdin.\ntypes column automatically inferred data.\n’ll see later, combining CSV files section, can also specify multiple CSV files.\n\nPlease keep mind csvsql employs SQLite dialect SQL, subtle differences respect SQL standard.\nSQL generally verbose solutions, also much flexible.\nalready know tackle scrubbing problem SQL, use ’re command line?","code":"$ seq 5 | header -a val | csvsql --query \"SELECT SUM(val) AS sum FROM stdin\"\nsum\n15.0"},{"path":"chapter-5-scrubbing-data.html","id":"extracting-and-reordering-columns","chapter":"5 Scrubbing Data","heading":"5.4.3 Extracting and Reordering Columns","text":"Columns can extracted reordered using command-line tool: csvcut79.\nexample, keep columns Iris data set contain numerical values reorder middle two columns:Alternatively, can also specify columns want leave -C option, stands complement:, included columns kept order.\nInstead column names, can also specify indices columns, start 1.\nallows , example, select odd columns (ever need !):’re certain comma’s values, can also use cut extract columns.\naware cut reorder columns, demonstrated following command:can see, matter order specify columns -f option; cut always appear original order.\ncompleteness, let’s also take look SQL approach extracting reordering numerical columns Iris data set:","code":"$ < iris.csv csvcut -c sepal_length,petal_length,sepal_width,petal_width | csvlo\nok\n│ sepal_length │ petal_length │ sepal_width │ petal_width │\n├──────────────┼──────────────┼─────────────┼─────────────┤\n│          5.1 │          1.4 │         3.5 │         0.2 │\n│          4.9 │          1.4 │         3.0 │         0.2 │\n│          4.7 │          1.3 │         3.2 │         0.2 │\n│          4.6 │          1.5 │         3.1 │         0.2 │\n│          5.0 │          1.4 │         3.6 │         0.2 │\n│          5.4 │          1.7 │         3.9 │         0.4 │\n│          4.6 │          1.4 │         3.4 │         0.3 │\n│          5.0 │          1.5 │         3.4 │         0.2 │\n… with 142 more lines$ < iris.csv csvcut -C species | csvlook\n│ sepal_length │ sepal_width │ petal_length │ petal_width │\n├──────────────┼─────────────┼──────────────┼─────────────┤\n│          5.1 │         3.5 │          1.4 │         0.2 │\n│          4.9 │         3.0 │          1.4 │         0.2 │\n│          4.7 │         3.2 │          1.3 │         0.2 │\n│          4.6 │         3.1 │          1.5 │         0.2 │\n│          5.0 │         3.6 │          1.4 │         0.2 │\n│          5.4 │         3.9 │          1.7 │         0.4 │\n│          4.6 │         3.4 │          1.4 │         0.3 │\n│          5.0 │         3.4 │          1.5 │         0.2 │\n… with 142 more lines$ echo 'a,b,c,d,e,f,g,h,i\\n1,2,3,4,5,6,7,8,9' |\n> csvcut -c $(seq 1 2 9 | paste -sd,)\na,c,e,g,i\n1,3,5,7,9$ echo 'a,b,c,d,e,f,g,h,i\\n1,2,3,4,5,6,7,8,9' | cut -d, -f 5,1,3\na,c,e\n1,3,5$ < iris.csv csvsql --query \"SELECT sepal_length, petal_length, \"\\\n> \"sepal_width, petal_width FROM stdin\" | head -n 5 | csvlook\n│ sepal_length │ petal_length │ sepal_width │ petal_width │\n├──────────────┼──────────────┼─────────────┼─────────────┤\n│          5.1 │          1.4 │         3.5 │         0.2 │\n│          4.9 │          1.4 │         3.0 │         0.2 │\n│          4.7 │          1.3 │         3.2 │         0.2 │\n│          4.6 │          1.5 │         3.1 │         0.2 │"},{"path":"chapter-5-scrubbing-data.html","id":"filtering-rows","chapter":"5 Scrubbing Data","heading":"5.4.4 Filtering Rows","text":"difference filtering rows CSV file opposed filtering lines plain text file may want base filtering values certain column, .\nFiltering location essentially , take account first line CSV file usually header.\nRemember can always use body command-line tool want keep header:comes filtering certain pattern within certain column, can use either csvgrep80, awk, , course, csvsql.\nexample, exclude bills party size smaller 5:awk csvsql can also numerical comparisons.\nexample, get bills 40 USD Saturday Sunday:csvsql solution verbose ’s also robust uses names columns instead indexes:Note flexibility clause SQL query easily matched command-line tools, SQL can operate dates sets, form complex combinations clauses.","code":"$ seq 5 | sed -n '3,5p'\n3\n4\n5\n \n$ seq 5 | header -a count | body sed -n '3,5p'\ncount\n3\n4\n5$ csvgrep -c size -i -r \"[1-4]\" tips.csv\nbill,tip,sex,smoker,day,time,size\n29.8,4.2,Female,No,Thur,Lunch,6\n34.3,6.7,Male,No,Thur,Lunch,6\n41.19,5.0,Male,No,Thur,Lunch,5\n27.05,5.0,Female,No,Thur,Lunch,6\n29.85,5.14,Female,No,Sun,Dinner,5\n48.17,5.0,Male,No,Sun,Dinner,6\n20.69,5.0,Male,No,Sun,Dinner,5\n30.46,2.0,Male,Yes,Sun,Dinner,5\n28.15,3.0,Male,Yes,Sat,Dinner,5$ < tips.csv awk -F, 'NR==1 || ($1 > 40.0) && ($5 ~ /^S/)'\nbill,tip,sex,smoker,day,time,size\n48.27,6.73,Male,No,Sat,Dinner,4\n44.3,2.5,Female,Yes,Sat,Dinner,3\n48.17,5.0,Male,No,Sun,Dinner,6\n50.81,10.0,Male,Yes,Sat,Dinner,3\n45.35,3.5,Male,Yes,Sun,Dinner,3\n40.55,3.0,Male,Yes,Sun,Dinner,2\n48.33,9.0,Male,No,Sat,Dinner,4$ csvsql --query \"SELECT * FROM tips WHERE bill > 40 AND day LIKE 'S%'\" tips.csv\n\nbill,tip,sex,smoker,day,time,size\n48.27,6.73,Male,0,Sat,Dinner,4.0\n44.3,2.5,Female,1,Sat,Dinner,3.0\n48.17,5.0,Male,0,Sun,Dinner,6.0\n50.81,10.0,Male,1,Sat,Dinner,3.0\n45.35,3.5,Male,1,Sun,Dinner,3.0\n40.55,3.0,Male,1,Sun,Dinner,2.0\n48.33,9.0,Male,0,Sat,Dinner,4.0"},{"path":"chapter-5-scrubbing-data.html","id":"merging-columns","chapter":"5 Scrubbing Data","heading":"5.4.5 Merging Columns","text":"Merging columns useful values interest spread multiple columns.\nmay happen dates (year, month, day separate columns) names (first name last name separate columns).\nLet’s consider second situation.input CSV list composers.\nImagine task combine first name last name full name.\n’ll present four different approaches task: sed, awk, cols + tr, csvsql.\nLet’s look input CSV:first approach, sed, uses two statements.\nfirst replace header second regular expression back references applied second row onwards:awk approach looks follows:cols approach combination tr:Please note csvsql employ SQLite database execute query || stands concatenation:last_name contain comma? Let’s look raw input CSV clarity sake:Well, appears first three approaches fail; different ways. csvsql able combine first_name full_name:Wait minute! ’s last command? R? Well, matter fact, .\n’s R code evaluated command-line tool called rush. can say moment, also approach succeeds merging two columns.\n’ll discuss nifty command-line tool later.","code":"$ csvlook -I names.csv\n│ id │ last_name │ first_name │ born │\n├────┼───────────┼────────────┼──────┤\n│ 1  │ Williams  │ John       │ 1932 │\n│ 2  │ Elfman    │ Danny      │ 1953 │\n│ 3  │ Horner    │ James      │ 1953 │\n│ 4  │ Shore     │ Howard     │ 1946 │\n│ 5  │ Zimmer    │ Hans       │ 1957 │$ < names.csv sed -re '1s/.*/id,full_name,born/g;2,$s/(.*),(.*),(.*),(.*)/\\1,\\3\n\\2,\\4/g' |\n> csvlook -I\n│ id │ full_name     │ born │\n├────┼───────────────┼──────┤\n│ 1  │ John Williams │ 1932 │\n│ 2  │ Danny Elfman  │ 1953 │\n│ 3  │ James Horner  │ 1953 │\n│ 4  │ Howard Shore  │ 1946 │\n│ 5  │ Hans Zimmer   │ 1957 │$ < names.csv awk -F, 'BEGIN{OFS=\",\"; print \"id,full_name,born\"} {if(NR > 1) {pr\nint $1,$3\" \"$2,$4}}' |\n> csvlook -I\n│ id │ full_name     │ born │\n├────┼───────────────┼──────┤\n│ 1  │ John Williams │ 1932 │\n│ 2  │ Danny Elfman  │ 1953 │\n│ 3  │ James Horner  │ 1953 │\n│ 4  │ Howard Shore  │ 1946 │\n│ 5  │ Hans Zimmer   │ 1957 │$ < names.csv |\n> cols -c first_name,last_name tr \\\",\\\" \\\" \\\" |\n> header -r full_name,id,born |\n> csvcut -c id,full_name,born |\n> csvlook -I\n│ id │ full_name     │ born │\n├────┼───────────────┼──────┤\n│ 1  │ John Williams │ 1932 │\n│ 2  │ Danny Elfman  │ 1953 │\n│ 3  │ James Horner  │ 1953 │\n│ 4  │ Howard Shore  │ 1946 │\n│ 5  │ Hans Zimmer   │ 1957 │$ < names.csv csvsql --query \"SELECT id, first_name || ' ' || last_name \"\\\n> \"AS full_name, born FROM stdin\" | csvlook -I\n│ id  │ full_name     │ born   │\n├─────┼───────────────┼────────┤\n│ 1.0 │ John Williams │ 1932.0 │\n│ 2.0 │ Danny Elfman  │ 1953.0 │\n│ 3.0 │ James Horner  │ 1953.0 │\n│ 4.0 │ Howard Shore  │ 1946.0 │\n│ 5.0 │ Hans Zimmer   │ 1957.0 │$ cat names-comma.csv\nid,last_name,first_name,born\n1,Williams,John,1932\n2,Elfman,Danny,1953\n3,Horner,James,1953\n4,Shore,Howard,1946\n5,Zimmer,Hans,1957\n6,\"Beethoven, van\",Ludwig,1770$ < names-comma.csv sed -re '1s/.*/id,full_name,born/g;2,$s/(.*),(.*),(.*),(.*)/\n\\1,\\3 \\2,\\4/g' | tail -n 1\n6,\"Beethoven,Ludwig  van\",1770$ < names-comma.csv awk -F, 'BEGIN{OFS=\",\"; print \"id,full_name,born\"} {if(NR >\n1) {print $1,$3\" \"$2,$4}}' | tail -n 1\n6, van\" \"Beethoven,Ludwig$ < names-comma.csv | cols -c first_name,last_name tr \\\",\\\" \\\" \\\" |\n> header -r full_name,id,born | csvcut -c id,full_name,born | tail -n 1\n6,\"Ludwig \"\"Beethoven  van\"\"\",1770$ < names-comma.csv csvsql --query \"SELECT id, first_name || ' ' || last_name AS\n full_name, born FROM stdin\" | tail -n 1\n6.0,\"Ludwig Beethoven, van\",1770.0$ < names-comma.csv rush run -t 'unite(df, full_name, first_name, last_name, sep\n = \" \")' - | tail -n 1\n6,\"Ludwig Beethoven, van\",1770"},{"path":"chapter-5-scrubbing-data.html","id":"combining-multiple-csv-files","chapter":"5 Scrubbing Data","heading":"5.4.6 Combining Multiple CSV Files","text":"","code":""},{"path":"chapter-5-scrubbing-data.html","id":"concatenate-horizontally","chapter":"5 Scrubbing Data","heading":"5.4.6.1 Concatenate Horizontally","text":"Let’s say three CSV files want put side side. use tee81 save result csvcut middle pipeline:Assuming rows line , can paste82 files together:, command-line argument -d instructs paste use comma delimiter.","code":"$ < tips.csv csvcut -c bill,tip | tee bills.csv | head -n 3 | csvlook\n│  bill │  tip │\n├───────┼──────┤\n│ 16.99 │ 1.01 │\n│ 10.34 │ 1.66 │\n \n$ < tips.csv csvcut -c day,time | tee datetime.csv |\n> head -n 3 | csvlook -I\n│ day │ time   │\n├─────┼────────┤\n│ Sun │ Dinner │\n│ Sun │ Dinner │\n \n$ < tips.csv csvcut -c sex,smoker,size | tee customers.csv |\n> head -n 3 | csvlook\n│ sex    │ smoker │ size │\n├────────┼────────┼──────┤\n│ Female │  False │    2 │\n│ Male   │  False │    3 │$ paste -d, {bills,customers,datetime}.csv | head -n 3 | csvlook -I\n│ bill  │ tip  │ sex    │ smoker │ size │ day │ time   │\n├───────┼──────┼────────┼────────┼──────┼─────┼────────┤\n│ 16.99 │ 1.01 │ Female │ No     │ 2    │ Sun │ Dinner │\n│ 10.34 │ 1.66 │ Male   │ No     │ 3    │ Sun │ Dinner │"},{"path":"chapter-5-scrubbing-data.html","id":"joining","chapter":"5 Scrubbing Data","heading":"5.4.6.2 Joining","text":"Sometimes data combined vertical horizontal concatenation.\ncases, especially relational databases, data spread multiple tables (files) order minimize redundancy.\nImagine wanted extend Iris data set information three types Iris flowers, namely USDA identifier.\nhappens separate CSV file identifiers:data set Iris data set common species column.\ncan use csvjoin83 join two data sets:course can also use SQL approach using csvsql, , per usual, bit longer (potentially much flexible):","code":"$ csvlook irismeta.csv\n│ species         │ wikipedia_url                                │ usda_id │\n├─────────────────┼──────────────────────────────────────────────┼─────────┤\n│ Iris-versicolor │ http://en.wikipedia.org/wiki/Iris_versicolor │ IRVE2   │\n│ Iris-virginica  │ http://en.wikipedia.org/wiki/Iris_virginica  │ IRVI    │\n│ Iris-setosa     │                                              │ IRSE    │$ csvjoin -c species iris.csv irismeta.csv | csvcut -c sepal_length,sepal_width,\nspecies,usda_id | sed -n '1p;49,54p' | csvlook\n│ sepal_length │ sepal_width │ species         │ usda_id │\n├──────────────┼─────────────┼─────────────────┼─────────┤\n│          4.6 │         3.2 │ Iris-setosa     │ IRSE    │\n│          5.3 │         3.7 │ Iris-setosa     │ IRSE    │\n│          5.0 │         3.3 │ Iris-setosa     │ IRSE    │\n│          7.0 │         3.2 │ Iris-versicolor │ IRVE2   │\n│          6.4 │         3.2 │ Iris-versicolor │ IRVE2   │\n│          6.9 │         3.1 │ Iris-versicolor │ IRVE2   │$ csvsql --query 'SELECT i.sepal_length, i.sepal_width, i.species, m.usda_id FRO\nM iris i JOIN irismeta m ON (i.species = m.species)' iris.csv irismeta.csv | sed\n -n '1p;49,54p' | csvlook\n│ sepal_length │ sepal_width │ species         │ usda_id │\n├──────────────┼─────────────┼─────────────────┼─────────┤\n│          4.6 │         3.2 │ Iris-setosa     │ IRSE    │\n│          5.3 │         3.7 │ Iris-setosa     │ IRSE    │\n│          5.0 │         3.3 │ Iris-setosa     │ IRSE    │\n│          7.0 │         3.2 │ Iris-versicolor │ IRVE2   │\n│          6.4 │         3.2 │ Iris-versicolor │ IRVE2   │\n│          6.9 │         3.1 │ Iris-versicolor │ IRVE2   │"},{"path":"chapter-5-scrubbing-data.html","id":"working-with-xmlhtml-and-json","chapter":"5 Scrubbing Data","heading":"5.5 Working with XML/HTML and JSON","text":"section ’m going demonstrate couple command-line tools can convert data one format another.\ntwo reasons convert data.First, oftentimes, data needs tabular form, just like database table spreadsheet, many visualization machine learning algorithms depend .\nCSV inherently tabular form, JSON HTML/XML data can deeply nested structure.Second, many command-line tools, especially classic ones cut grep, operate plain text.\ntext regarded universal interface command-line tools.\nMoreover, formats younger. formats can treated plain text, allowing us apply command-line tools formats well.Sometimes can get away applying classic tools structured data.\nexample, treating JSON data plain text, can change attribute gender sex using sed:Like many command-line tools, sed make use structure data.\nBetter either use tool makes use structure data (jq discuss ), first convert data tabular format CSV apply appropriate command-line tool.’m going demonstrate converting XML/HTML JSON CSV real-world use case.\ncommand-line tools ’ll using : curl, pup84, xml2json85, jq json2csv86.Wikpedia holds wealth information. Much information ordered tables, can regarded data sets.\nexample, page contains list countries territories together border length, area, ratio two.Let’s imagine ’re interested analyzing data. section, ’ll walk necessary steps corresponding commands. won’t go every little detail, won’t understand everything right away. Don’t worry, ’re confident ’ll get gist . Remember purpose section demonstrate command line. tools concepts used section () explained subsequent chapters.data set ’re interested , embedded HTML.\ngoal end representation data set can work .\nfirst step download HTML using curl:HTML saved file named wiki.html.\nLet’s see first 10 lines look like:seems order.\nImage ’ve able determine root HTML element ’re interested <table> class wikitable.\nallows look part ’re interest using grep (-option specifies number lines want print matching line):now actually see countries values.\nnext step extract necessary elements HTML file.\ncan use pup:expression passed pup CSS-selector.\nsyntax usually used style web pages, can also use select certain elements HTML.\ncase, want select tbody table wikitable class.\nnext xml2json, converts XML (HTML) JSON.reason convert HTML JSON powerful tool called jq operates JSON data.\nfollowing command extracts certain parts JSON data reshapes form can work :data now form can work .\nquite steps get Wikipedia page CSV data set.\nHowever, combine commands one, see ’s actually really concise expressive.concludes demonstration conversion XML/HTML JSON CSV.\njq can perform many operations, exist specialized tools work XML data, experience, converting data CSV format quickly possible tends work well.\nway can spend time becoming proficient generic command-line tools, rather specific tools.","code":"$ sed -e 's/\"gender\":/\"sex\":/g' users.json | jq | trim\n{\n  \"results\": [\n    {\n      \"sex\": \"male\",\n      \"name\": {\n        \"title\": \"mr\",\n        \"first\": \"leevi\",\n        \"last\": \"kivisto\"\n      },\n      \"location\": {\n… with 260 more lines$ curl -sL 'http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_bo\nrder/area_ratio' > wiki.html$ < wiki.html trim\n<!DOCTYPE html>\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n<head>\n<meta charset=\"UTF-8\"/>\n<title>List of countries and territories by border/area ratio - Wikipedia<\/titl…\n<script>document.documentElement.className=\"client-js\";RLCONF={\"wgBreakFrames\":…\n\"Lists of countries by geography\",\"Lists by area\",\"Border-related lists\"],\"wgPa…\n\"wgGENewcomerTasksGuidanceEnabled\":true,\"wgGEAskQuestionEnabled\":false,\"wgGELin…\n\"ext.cx.eventlogging.campaigns\",\"ext.centralNotice.geoIP\",\"ext.centralNotice.st…\n<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement(\"user.options@…\n… with 3070 more lines$ grep wikitable -A 21 wiki.html\n<table class=\"wikitable sortable\">\n<tbody><tr>\n<th>Rank<\/th>\n<th>Country or territory<\/th>\n<th>Total length of land borders (km)<\/th>\n<th>Total surface area (km<sup>2<\/sup>)<\/th>\n<th>Border/area ratio (km/km<sup>2<\/sup>)\n<\/th><\/tr>\n<tr>\n<td>1\n<\/td>\n<td>Vatican City\n<\/td>\n<td>3.2\n<\/td>\n<td>0.44\n<\/td>\n<td>7.2727273\n<\/td><\/tr>\n<tr>\n<td>2\n<\/td>$ < wiki.html pup 'table.wikitable tbody' | tee table.html | trim\n<tbody>\n <tr>\n  <th>\n   Rank\n  <\/th>\n  <th>\n   Country or territory\n  <\/th>\n  <th>\n   Total length of land borders (km)\n… with 4199 more lines$ < table.html xml2json > table.json\n \n$ jq . table.json | trim 20\n{\n  \"tbody\": {\n    \"tr\": [\n      {\n        \"th\": [\n          {\n            \"$t\": \"Rank\"\n          },\n          {\n            \"$t\": \"Country or territory\"\n          },\n          {\n            \"$t\": \"Total length of land borders (km)\"\n          },\n          {\n            \"$t\": [\n              \"Total surface area (km\",\n              \")\"\n            ],\n            \"sup\": {\n… with 4691 more lines$ < table.json jq -r '.tbody.tr[1:][] | [.td[][\"$t\"]] | @csv' | header -a rank,c\nountry,border,surface,ratio > countries.csv$ csvlook --max-column-width 28 countries.csv\n│ rank │ country                      │    border │       surface │  ratio │\n├──────┼──────────────────────────────┼───────────┼───────────────┼────────┤\n│    1 │ Vatican City                 │      3.20 │          0.44 │ 7.273… │\n│    2 │ Monaco                       │      4.40 │          2.00 │ 2.200… │\n│    3 │ San Marino                   │     39.00 │         61.00 │ 0.639… │\n│    4 │ Liechtenstein                │     76.00 │        160.00 │ 0.465… │\n│    5 │ Sint Maarten  (Netherlands)  │     10.20 │         34.00 │ 0.300… │\n│    6 │ Andorra                      │    120.30 │        468.00 │ 0.257… │\n│    7 │ Gibraltar  (United Kingdom)  │      1.20 │          6.00 │ 0.200… │\n│    8 │ Saint Martin (France)        │     10.20 │         54.00 │ 0.189… │\n… with 238 more lines"},{"path":"chapter-5-scrubbing-data.html","id":"summary-4","chapter":"5 Scrubbing Data","heading":"5.6 Summary","text":"chapter ’ve looked cleaning, scrubbing, data.\n’ve seen single tool can magically get rid messiness data; ’ll often need combine multiple different tools get desired result.\nKeep mind classic command-line tools cut sort can’t interpret structured data.\nLuckily, tools convert one data format, JSON XML, another data format, CSV.\nnext chapter, intermezzo chapter, ’m going show can manage project using make.\n’re free skip chapter can’t wait start exploring visualizing data Chapter 7.","code":""},{"path":"chapter-5-scrubbing-data.html","id":"for-further-exploration-4","chapter":"5 Scrubbing Data","heading":"5.7 For Further Exploration","text":"wish ’ve explained awk. ’s powerful tool programming language. highly recommend take time learn . Two good resources book sed & awk Doherty Robbins online GNU Awk User’s Guide.chapter used regular expressions couple places. tutorial unfortunately beyond scope book. regular expressions can used many different tools, recommend learn . good book Regular Expressions Cookbook Jan Goyvaerts Steven Levithan.","code":""},{"path":"chapter-6-project-management-with-make.html","id":"chapter-6-project-management-with-make","chapter":"6 Project Management with Make","heading":"6 Project Management with Make","text":"hope now come appreciate command line convenient environment working data.\nmay noticed , consequence working command line, :Invoke many different commands.Work various directories.Develop command-line tools.Obtain generate many (intermediate) files.Since exploratory process, workflow tends rather chaotic, makes difficult keep track ’ve done.\n’s important steps can reproduced, us others.\ncontinue project time ago, chances forgotten commands ran, directory, files, parameters, order.\nImagine challenges sharing project collaborator.can recover commands digging output history command, , course, reliable approach.\nsomewhat better approach save commands shell script.\nleast allows collaborators reproduce project.\nshell script , however, also sub-optimal approach :difficult read maintain.Dependencies steps unclear.Every step gets executed every time, inefficient sometimes undesirable.make really shines87. make command-line tool allows :Formalize data workflow steps terms input output dependencies.Run specific steps workflow.Use inline code.Store retrieve data external sources.important, related topic version control, allows track changes project, back project server, collaborate others, retrieve earlier versions things go wrong.\npopular command-line tool version control git89.\n’s often used combination GitHub, online service distributed version control.\nMany open source projects, including book, hosted GitHub.\ntopic version control beyond scope book, highly recommend look , especially start collaborating others.\nend chapter recommend resources learn .","code":""},{"path":"chapter-6-project-management-with-make.html","id":"overview-3","chapter":"6 Project Management with Make","heading":"6.1 Overview","text":"Managing data workflow make main topic chapter.\n, ’ll learn :Defining workflow Makefile.Thinking workflow terms input output dependencies.Running tasks building targets.instructions get files Chapter 2.\nfiles either downloaded generated using command-line tools.","code":"$ cd /data/ch06\n \n$ l\ntotal 28K\n-rw-r--r-- 1 dst dst  37 Dec 14 11:49 Makefile.test\n-rw-r--r-- 1 dst dst  16 Dec 14 11:49 numbers.make\n-rw-r--r-- 1 dst dst  26 Dec 14 11:49 numbers-write.make\n-rw-r--r-- 1 dst dst  21 Dec 14 11:49 numbers-write-var.make\n-rw-r--r-- 1 dst dst 432 Dec 14 11:49 starwars.make\n-rw-r--r-- 1 dst dst 263 Dec 14 11:49 tasks.make\n-rw-r--r-- 1 dst dst  27 Dec 14 11:49 template.make"},{"path":"chapter-6-project-management-with-make.html","id":"introducing-make","chapter":"6 Project Management with Make","heading":"6.2 Introducing Make","text":"make organizes command execution around data dependencies.\ndata processing steps formalized separate text file (workflow).\nstep may inputs outputs.\nmake automatically resolves dependencies determines commands need run order.means , say, SQL query takes ten minutes, executed result missing query changed afterwards.\nAlso, want (re-)run specific step, make re-runs steps step depends.\ncan save lot time.formalized workflow allows easily pick project weeks collaborate others. strongly advise , even think one-project, never know need run certain steps , reuse another project.","code":""},{"path":"chapter-6-project-management-with-make.html","id":"running-tasks","chapter":"6 Project Management with Make","heading":"6.3 Running Tasks","text":"default, make searches configuration file called Makefile current directory.\ncan also named makefile (lower case), recommend calling file Makefile ’s common way appears top directory listing.\nNormally one configuration file per project.\nchapter discusses many different ones, haven given different filename .make extension.\nLet’s start following Makefile:Makefile contains one target called numbers.\ntarget like task.\n’s usually name file ’d like create can also generic .\nline , seq 7, known rule.\nThink rule recipe; one commands specify target built.whitespace front rule single tab character.\nmake picky comes whitespace.\nBeware editors insert spaces press TAB key, known soft tab, cause make produce error.\nfollowing code illustrates expanding tab eight spaces:➊ need add -f option (short --makefile option) configuration file isn’t called Makefile, default.\n➋ One helpful error messages ’ll find command line!now , ’ll rename appropriate file Makefile matches real-world use closely.\n, just run make:see make first prints rule (seq 7), output generated rule.\nprocess known building target.\ndon’t specify name target, make build first target specified Makefile.\npractice though, ’ll often specifying target ’d want build:case, ’re actually building anything, , ’re creating new files.\nmake happily build target numbers , ’s finding file called numbers.\nnext section ’ll go .Sometimes ’s useful target builds regardless whether file name exists.\nThink tasks need perform part project.\n’s good practice declare targets phony using special target called .PHONY top Makefile, followed names phony targets.\n’s example Makefile illustrates use phony targets:➊ Note extra dollar sign front $(pwd). needed make uses single dollar sign refer various special variables, ’ll explain later.taken Makefile use working book.\nsay ’m using make glorified task runner.\nAlthough wasn’t primary purpose make, still provides lot value don’t need remember look incantation used.\nInstead, type make publish latest version book published.\n’s perfectly fine put long-running commands Makefile.make can much us!","code":"$ bat -A numbers.make\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: numbers.make\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ numbers:␊\n   2   │ ├──────┤seq·7␊\n───────┴────────────────────────────────────────────────────────────────────────$ < numbers.make expand > spaces.make\n \n$ bat -A spaces.make\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: spaces.make\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ numbers:␊\n   2   │ ········seq·7␊\n───────┴────────────────────────────────────────────────────────────────────────\n \n$ make -f spaces.make ➊\nspaces.make:2: *** missing separator (did you mean TAB instead of 8 spaces?).  S\ntop. ➋\n \n$ rm spaces.make$ cp numbers.make Makefile\n \n$ make\nseq 7\n1\n2\n3\n4\n5\n6\n7$ make numbers\nseq 7\n1\n2\n3\n4\n5\n6\n7$ bat tasks.make\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: tasks.make\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ .PHONY: clean publish docker-run\n   2   │\n   3   │ clean:\n   4   │         rm book/2e/book.md book/2e/render*.rds\n   5   │\n   6   │ publish:\n   7   │         (cd www && hugo) && netlify deploy --prod --dir www/public\n   8   │\n   9   │ docker-run:\n  10   │         docker run -it --rm -v $$(pwd)/book/2e/data:/data -p 8000:8000\n       │ datasciencetoolbox/dsatcl2e:latest ➊\n───────┴────────────────────────────────────────────────────────────────────────"},{"path":"chapter-6-project-management-with-make.html","id":"building-for-real","chapter":"6 Project Management with Make","heading":"6.4 Building, For Real","text":"Let’s modify Makefile output rule written file numbers.Now can say make actually building something.\n’s , run , see make reports target numbers --date.’s need rebuild target numbers file numbers already exists.\n’s great make saving us time repeating work.make, ’s files.\nkeep mind make cares name target.\ncheck whether file name actually gets created rule.\nwrite file called nummers, Dutch “numbers,” target still called numbers, make always build target. Vice versa, file numbers created process, whether automated manual, make still consider target --date.can avoid repetition using automatic variable $@, gets expanded name target:Let’s verify works removing file numbers calling make :Another reason make rebuild target dependencies, let’s discuss next.","code":"$ cp numbers-write.make Makefile\n \n$ bat Makefile\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: Makefile\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ numbers:\n   2   │         seq 7 > numbers\n───────┴────────────────────────────────────────────────────────────────────────\n \n$ make numbers\nseq 7 > numbers\n \n$ bat numbers\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: numbers\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ 1\n   2   │ 2\n   3   │ 3\n   4   │ 4\n   5   │ 5\n   6   │ 6\n   7   │ 7\n───────┴────────────────────────────────────────────────────────────────────────$ make numbers\nmake: 'numbers' is up to date.$ cp numbers-write-var.make Makefile\n \n$ bat Makefile\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: Makefile\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ numbers:\n   2   │         seq 7 > $@\n───────┴────────────────────────────────────────────────────────────────────────$ rm numbers\n \n$ make numbers\nseq 7 > numbers\n \n$ bat numbers\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: numbers\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ 1\n   2   │ 2\n   3   │ 3\n   4   │ 4\n   5   │ 5\n   6   │ 6\n   7   │ 7\n───────┴────────────────────────────────────────────────────────────────────────"},{"path":"chapter-6-project-management-with-make.html","id":"adding-dependencies","chapter":"6 Project Management with Make","heading":"6.5 Adding Dependencies","text":"far, ’ve looked targets exist isolation.\ntypical data science workflow, many steps depend steps.\norder properly talk dependencies Makefile, let’s consider two tasks work dataset Star Wars characters.’s excerpt dataset:first task computes ten tallest humans:➊ keep lines contain pattern Human.\n➋ Extract first two columns.\n➌ Sort lines second column reverse numeric order.\n➍ default, head prints first 10 lines. can override -n option.second task creates box plot showing distribution heights per species (see Figure 6.1):\nFigure 6.1: Distribution heights per species Star Wars\nLet’s put two tasks Makefile.\nInstead incrementally, ’d first like show complete Makefile looks like explain syntax step step.Let’s go Makefile step step.\nfirst three lines change default settings related make :rules executed shell, default, sh. SHELL variable can change another shell, like bash. way can use everything Bash offer loops.default, every line rule sent separately shell. special target .ONESHELL can override rule target top10 works..SHELLFLAGS line makes Bash strict, considered best practice. example, , pipeline rule target top10 now stops soon error.define custom variable called URL.\nEven though used , find helpful put information like near beginning file, can easily make changes kinds settings.special target .PHONY can indicate targets represented files. case holds targets top10. targets now executed regardless whether directory contains files name.five targets: , data, data/starwars.csv, top10, heights.png.\nFigure 6.1 provides overview targets dependencies .\nFigure 6.2: Dependencies targets\nLet’s discuss target turn:target two dependencies rule. like shortcut execute one targets order specified. case: top10 heights.png. target appears first target Makefile, means run make, target built.target data creates directory data. Earlier said make files. Well, ’s also directories. target executed directory data doesn’t yet exist.target data/starwars.csv depends target data. ’s data directory, first created. dependencies satisfied, rule executed, involves downloading file saving file name target.target top10 marked phony, always built specified. depends data/starwars.csv target. makes use special variable, $< expands name first prerequisite, namely data/starwars.csv.target heights.png, like target top10 depends data/starwars.csv makes use automatic variables ’ve seen chapter. See online documentation ’d like learn automatic variables.Last least, let’s verify Makefile works:surprises . didn’t specify target, target built, , turn, causes top10 heights.png targets built. output former printed standard output latter creates file heights.png. data directory created , just like CSV file downloaded .’s nothing fun just playing data forgetting everything else.\ntrust say ’s worthwhile keep record done using Makefile.\nmake life easier (pun intended), also start thinking data workflow terms steps.\nJust command-line toolbox, expand time, holds make workflows.\nsteps defined, easier gets keep , often can reuse certain steps.\nhope get used make, make life easier.","code":"$ curl -sL 'https://raw.githubusercontent.com/tidyverse/dplyr/master/data-raw/st\narwars.csv' |\n> xsv select name,height,mass,homeworld,species |\n> csvlook\n│ name                  │ height │    mass │ homeworld      │ species        │\n├───────────────────────┼────────┼─────────┼────────────────┼────────────────┤\n│ Luke Skywalker        │    172 │    77.0 │ Tatooine       │ Human          │\n│ C-3PO                 │    167 │    75.0 │ Tatooine       │ Droid          │\n│ R2-D2                 │     96 │    32.0 │ Naboo          │ Droid          │\n│ Darth Vader           │    202 │   136.0 │ Tatooine       │ Human          │\n│ Leia Organa           │    150 │    49.0 │ Alderaan       │ Human          │\n│ Owen Lars             │    178 │   120.0 │ Tatooine       │ Human          │\n│ Beru Whitesun lars    │    165 │    75.0 │ Tatooine       │ Human          │\n│ R5-D4                 │     97 │    32.0 │ Tatooine       │ Droid          │\n… with 79 more lines$ curl -sL 'https://raw.githubusercontent.com/tidyverse/dplyr/master/data-raw/st\narwars.csv' |\n> grep Human | ➊\n> cut -d, -f 1,2 | ➋\n> sort -t, -k2 -nr | ➌\n> head ➍\nDarth Vader,202\nQui-Gon Jinn,193\nDooku,193\nBail Prestor Organa,191\nRaymus Antilles,188\nMace Windu,188\nAnakin Skywalker,188\nGregar Typho,185\nJango Fett,183\nCliegg Lars,183$ curl -sL 'https://raw.githubusercontent.com/tidyverse/dplyr/master/data-raw/st\narwars.csv' |\n> rush plot --x height --y species --geom boxplot > heights.png\n \n$ display heights.png$ cp starwars.make Makefile\n \n$ bat Makefile\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: Makefile\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ SHELL := bash\n   2   │ .ONESHELL:\n   3   │ .SHELLFLAGS := -eu -o pipefail -c\n   4   │\n   5   │ URL = \"https://raw.githubusercontent.com/tidyverse/dplyr/master/data-ra\n       │ w/starwars.csv\"\n   6   │\n   7   │ .PHONY: all top10\n   8   │\n   9   │ all: top10 heights.png\n  10   │\n  11   │ data:\n  12   │         mkdir $@\n  13   │\n  14   │ data/starwars.csv: data\n  15   │         curl -sL $(URL) > $@\n  16   │\n  17   │ top10: data/starwars.csv\n  18   │         grep Human $< |\n  19   │         cut -d, -f 1,2 |\n  20   │         sort -t, -k2 -nr |\n  21   │         head\n  22   │\n  23   │ heights.png: data/starwars.csv\n  24   │         < $< rush plot --x height --y species --geom boxplot > $@\n───────┴────────────────────────────────────────────────────────────────────────$ make\nmkdir data\ncurl -sL \"https://raw.githubusercontent.com/tidyverse/dplyr/master/data-raw/star\nwars.csv\" > data/starwars.csv\ngrep Human data/starwars.csv |\ncut -d, -f 1,2 |\nsort -t, -k2 -nr |\nhead\nDarth Vader,202\nQui-Gon Jinn,193\nDooku,193\nBail Prestor Organa,191\nRaymus Antilles,188\nMace Windu,188\nAnakin Skywalker,188\nGregar Typho,185\nJango Fett,183\nCliegg Lars,183\n< data/starwars.csv rush plot --x height --y species --geom boxplot > heights.pn\ng"},{"path":"chapter-6-project-management-with-make.html","id":"summary-5","chapter":"6 Project Management with Make","heading":"6.6 Summary","text":"One beauties command line allows play data.\ncan easily execute different commands process different data files.\ninteractive iterative process.\n, easy forget steps taken get desired result.\n’s therefore important document steps every .\nway, one colleagues picks project time, result can produced executing steps.chapter ’ve shown just putting every command one Bash script suboptimal.\nInstead, proposed use make command-line tool manage data workflow.\nnext chapter covers third step OSEMN model data science namely exploring data.","code":""},{"path":"chapter-6-project-management-with-make.html","id":"for-further-exploration-5","chapter":"6 Project Management with Make","heading":"6.7 For Further Exploration","text":"book Managing Projects GNU Make Robert Mecklenburg online GNU Make Manual provide comprehensive advanced overview make.exist plenty workflow managers besides make. Although differ syntax features, also use concepts targets, rules, dependencies. Examples include Luigi, Apache Airflow, Nextflow.learn version control, particular git GitHub, recommend book Pro Git Scott Chacon Ben Straub. ’s available free. online GitHub documentation also great starting point.","code":""},{"path":"chapter-7-exploring-data.html","id":"chapter-7-exploring-data","chapter":"7 Exploring Data","heading":"7 Exploring Data","text":"hard work (unless already clean data lying around), ’s time fun.\nNow obtained scrubbed data, can continue third step OSEMN model, explore .Exploring step familiarize data.\nfamiliar data essential want extract value .\nexample, knowing kind features data , means know ones worth exploration ones can use answer questions .Exploring data can done three perspectives.\nfirst perspective inspect data properties.\n, want find things like raw data looks like, many data points dataset , features dataset .second compute descriptive statistics. perspective useful learning individual features.\noutput often brief textual can therefore printed command line.third perspective create visualizations data. perspective can gain insight multiple features interact. ’ll discuss way creating visualizations can printed command line. However, visualizations best suited displayed graphical user interface. advantage data visualizations descriptive statistics flexible can convey much information.","code":""},{"path":"chapter-7-exploring-data.html","id":"overview-4","chapter":"7 Exploring Data","heading":"7.1 Overview","text":"chapter, ’ll learn :Inspect data propertiesCompute descriptive statisticsCreate data visualizations inside outside command lineThis chapter starts following files:instructions get files Chapter 2.\nfiles either downloaded generated using command-line tools.","code":"$ cd /data/ch07\n \n$ l\ntotal 104K\n-rw-r--r-- 1 dst dst  125 Dec 14 11:49 datatypes.csv\n-rw-r--r-- 1 dst dst 7.8K Dec 14 11:49 tips.csv\n-rw-r--r-- 1 dst dst  83K Dec 14 11:49 venture.csv\n-rw-r--r-- 1 dst dst 4.6K Dec 14 11:49 venture-wide.csv"},{"path":"chapter-7-exploring-data.html","id":"inspecting-data-and-its-properties","chapter":"7 Exploring Data","heading":"7.2 Inspecting Data and its Properties","text":"section ’ll demonstrate inspect dataset properties. upcoming visualization modeling techniques expect data rectangular shape, ’ll assume data CSV format. can use techniques described Chapter 5 convert data CSV necessary.simplicity sake, ’ll also assume data header.\nfirst subsection ’ll show way determine whether ’s case.\nknow header, can continue answering following questions:many data points features dataset ?raw data look like?kind features dataset ?Can features treated categorical?","code":""},{"path":"chapter-7-exploring-data.html","id":"header-or-not-here-i-come","chapter":"7 Exploring Data","heading":"7.2.1 Header Or Not, Here I Come","text":"can check whether file header printing first lines using head:lines wrap around, add line numbers using nl:Alternatively, can use trim:case, ’s clear first line header contains uppercase names subsequent lines contain numbers.\nindeed quite subjective process ’s decide whether first line header already first data point.\ndataset contains header, ’re best using header tool (discussed Chapter 5) correct .","code":"$ head -n 5 venture.csv\nFREQ,TIME_FORMAT,TIME_PERIOD,EXPEND,UNIT,GEO,OBS_STATUS,OBS_VALUE,FREQ_DESC,TIME\n_FORMAT_DESC,TIME_PERIOD_DESC,OBS_STATUS_DESC,EXPEND_DESC,UNIT_DESC,GEO_DESC\nA,P1Y,2015,INV_VEN,PC_GDP,CZ,,0.002,Annual,Annual,Year 2015,No data,\"Venture cap\nital investment (seed, start-up and later stage) \",Percentage of GDP,Czechia\nA,P1Y,2007,INV_VEN,PC_GDP,DE,,0.034,Annual,Annual,Year 2007,No data,\"Venture cap\nital investment (seed, start-up and later stage) \",Percentage of GDP,Germany\nA,P1Y,2008,INV_VEN,PC_GDP,DE,,0.039,Annual,Annual,Year 2008,No data,\"Venture cap\nital investment (seed, start-up and later stage) \",Percentage of GDP,Germany\nA,P1Y,2009,INV_VEN,PC_GDP,DE,,0.029,Annual,Annual,Year 2009,No data,\"Venture cap\nital investment (seed, start-up and later stage) \",Percentage of GDP,Germany$ head -n 3 venture.csv | nl\n     1  FREQ,TIME_FORMAT,TIME_PERIOD,EXPEND,UNIT,GEO,OBS_STATUS,OBS_VALUE,FREQ_D\nESC,TIME_FORMAT_DESC,TIME_PERIOD_DESC,OBS_STATUS_DESC,EXPEND_DESC,UNIT_DESC,GEO_\nDESC\n     2  A,P1Y,2015,INV_VEN,PC_GDP,CZ,,0.002,Annual,Annual,Year 2015,No data,\"Ven\nture capital investment (seed, start-up and later stage) \",Percentage of GDP,Cze\nchia\n     3  A,P1Y,2007,INV_VEN,PC_GDP,DE,,0.034,Annual,Annual,Year 2007,No data,\"Ven\nture capital investment (seed, start-up and later stage) \",Percentage of GDP,Ger\nmany$ < venture.csv trim 5\nFREQ,TIME_FORMAT,TIME_PERIOD,EXPEND,UNIT,GEO,OBS_STATUS,OBS_VALUE,FREQ_DESC,TIM…\nA,P1Y,2015,INV_VEN,PC_GDP,CZ,,0.002,Annual,Annual,Year 2015,No data,\"Venture ca…\nA,P1Y,2007,INV_VEN,PC_GDP,DE,,0.034,Annual,Annual,Year 2007,No data,\"Venture ca…\nA,P1Y,2008,INV_VEN,PC_GDP,DE,,0.039,Annual,Annual,Year 2008,No data,\"Venture ca…\nA,P1Y,2009,INV_VEN,PC_GDP,DE,,0.029,Annual,Annual,Year 2009,No data,\"Venture ca…\n… with 536 more lines"},{"path":"chapter-7-exploring-data.html","id":"inspect-all-the-data","chapter":"7 Exploring Data","heading":"7.2.2 Inspect All The Data","text":"want inspect raw data pace, ’s probably good idea use cat, data printed one go.\nrecommend using less90, allows interactively inspect data command line.\ncan prevent long lines (venture.csv) wrapping specifying -S option:greater-signs right indicate can scroll horizontally.\ncan scroll pressing .\nPress Space scroll entire screen.\nScrolling horizontally done pressing Left Right.\nPress g G go start end file, respectively.\nQuitting less done pressing q.\nmanual page lists available key bindings.One advantage less load entire file memory, means ’s fast even viewing large files.","code":"$ less -S venture.csvFREQ,TIME_FORMAT,TIME_PERIOD,EXPEND,UNIT,GEO,OBS_STATUS,OBS_VALUE,FREQ_DESC,TIM>\nA,P1Y,2015,INV_VEN,PC_GDP,CZ,,0.002,Annual,Annual,Year 2015,No data,\"Venture ca>\nA,P1Y,2007,INV_VEN,PC_GDP,DE,,0.034,Annual,Annual,Year 2007,No data,\"Venture ca>\nA,P1Y,2008,INV_VEN,PC_GDP,DE,,0.039,Annual,Annual,Year 2008,No data,\"Venture ca>\nA,P1Y,2009,INV_VEN,PC_GDP,DE,,0.029,Annual,Annual,Year 2009,No data,\"Venture ca>\nA,P1Y,2010,INV_VEN,PC_GDP,DE,,0.029,Annual,Annual,Year 2010,No data,\"Venture ca>\nA,P1Y,2011,INV_VEN,PC_GDP,DE,,0.029,Annual,Annual,Year 2011,No data,\"Venture ca>\nA,P1Y,2012,INV_VEN,PC_GDP,DE,,0.021,Annual,Annual,Year 2012,No data,\"Venture ca>\nA,P1Y,2013,INV_VEN,PC_GDP,DE,,0.023,Annual,Annual,Year 2013,No data,\"Venture ca>\nA,P1Y,2014,INV_VEN,PC_GDP,DE,,0.021,Annual,Annual,Year 2014,No data,\"Venture ca>\nA,P1Y,2015,INV_VEN,PC_GDP,DE,,0.025,Annual,Annual,Year 2015,No data,\"Venture ca>\nA,P1Y,2007,INV_VEN,PC_GDP,DK,,0.092,Annual,Annual,Year 2007,No data,\"Venture ca>\nA,P1Y,2008,INV_VEN,PC_GDP,DK,,0.074,Annual,Annual,Year 2008,No data,\"Venture ca>\nA,P1Y,2009,INV_VEN,PC_GDP,DK,,0.051,Annual,Annual,Year 2009,No data,\"Venture ca>\nA,P1Y,2010,INV_VEN,PC_GDP,DK,,0.059,Annual,Annual,Year 2010,No data,\"Venture ca>\n:                   "},{"path":"chapter-7-exploring-data.html","id":"feature-names-and-data-types","chapter":"7 Exploring Data","heading":"7.2.3 Feature Names and Data Types","text":"column (feature) names may indicate meaning feature.\ncan use following head tr combo :basic command assumes file delimited commas.\nrobust approach use csvcut:can go step just printing column names.\nBesides names columns, useful know type values column contains, string characters, numerical value, date.\nAssume following toy dataset:csvlook interprets follows:already used csvsql Chapter 5 execute SQL queries directly CSV data.\ncommand-line arguments passed, generates necessary SQL statement needed insert data actual database.\ncan use output also inspect inferred column types .\ncolumn NULL string printed data type, column contains missing values.output especially useful use tools within csvkit suite, csvgrep, csvsort csvsql.\nventure.csv, columns inferred follows:","code":"$ < venture.csv head -n 1 | tr , '\\n'\nFREQ\nTIME_FORMAT\nTIME_PERIOD\nEXPEND\nUNIT\nGEO\nOBS_STATUS\nOBS_VALUE\nFREQ_DESC\nTIME_FORMAT_DESC\nTIME_PERIOD_DESC\nOBS_STATUS_DESC\nEXPEND_DESC\nUNIT_DESC\nGEO_DESC$ csvcut -n venture.csv\n  1: FREQ\n  2: TIME_FORMAT\n  3: TIME_PERIOD\n  4: EXPEND\n  5: UNIT\n  6: GEO\n  7: OBS_STATUS\n  8: OBS_VALUE\n  9: FREQ_DESC\n 10: TIME_FORMAT_DESC\n 11: TIME_PERIOD_DESC\n 12: OBS_STATUS_DESC\n 13: EXPEND_DESC\n 14: UNIT_DESC\n 15: GEO_DESC$ bat -A datatypes.csv\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: datatypes.csv\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ a,b,c,d,e,f␊\n   2   │ 1,0.0,FALSE,\"\"\"Yes!\"\"\",2011-11-11·11:00,2012-09-08␊\n   3   │ 42,3.1415,TRUE,\"OK,·good\",2014-09-15,12/6/70␊\n   4   │ 66,,False,2198,,␊\n───────┴────────────────────────────────────────────────────────────────────────$ csvlook datatypes.csv\n│  a │      b │     c │ d        │                   e │          f │\n├────┼────────┼───────┼──────────┼─────────────────────┼────────────┤\n│  1 │ 0.000… │ False │ \"Yes!\"   │ 2011-11-11 11:00:00 │ 2012-09-08 │\n│ 42 │ 3.142… │  True │ OK, good │ 2014-09-15 00:00:00 │ 1970-12-06 │\n│ 66 │        │ False │ 2198     │                     │            │$ csvsql datatypes.csv\nCREATE TABLE datatypes (\n        a DECIMAL NOT NULL,\n        b DECIMAL,\n        c BOOLEAN NOT NULL,\n        d VARCHAR NOT NULL,\n        e TIMESTAMP,\n        f DATE\n);$ csvsql venture.csv\nCREATE TABLE venture (\n        \"FREQ\" VARCHAR NOT NULL,\n        \"TIME_FORMAT\" VARCHAR NOT NULL,\n        \"TIME_PERIOD\" DECIMAL NOT NULL,\n        \"EXPEND\" VARCHAR NOT NULL,\n        \"UNIT\" VARCHAR NOT NULL,\n        \"GEO\" VARCHAR NOT NULL,\n        \"OBS_STATUS\" BOOLEAN,\n        \"OBS_VALUE\" DECIMAL NOT NULL,\n        \"FREQ_DESC\" VARCHAR NOT NULL,\n        \"TIME_FORMAT_DESC\" VARCHAR NOT NULL,\n        \"TIME_PERIOD_DESC\" VARCHAR NOT NULL,\n        \"OBS_STATUS_DESC\" VARCHAR NOT NULL,\n        \"EXPEND_DESC\" VARCHAR NOT NULL,\n        \"UNIT_DESC\" VARCHAR NOT NULL,\n        \"GEO_DESC\" VARCHAR NOT NULL\n);"},{"path":"chapter-7-exploring-data.html","id":"unique-identifiers-continuous-variables-and-factors","chapter":"7 Exploring Data","heading":"7.2.4 Unique Identifiers, Continuous Variables, and Factors","text":"Knowing data type feature enough.\n’s also essential know feature represents.\nknowledge domain useful , may also get context looking data .string integer unique identifier represent category.\nlatter case, used assign color visualization.\ninteger denotes, say, postal code, doesn’t make sense compute average.determine whether feature treated unique identifier categorical variable, count number unique values specific column:can use csvstat91, part csvkit, get number unique values column:’s one unique value (OBS_STATUS), ’s chance can discard column doesn’t provide value.\nwanted automatically discard columns, use following pipeline:➊ -C option deselects columns given locations (names), provided command substitution\n➋ Obtain number unique values column venture.csv\n➌ keep columns contain one unique value\n➍ Extract column location\n➎ Trim white space\n➏ Put column locations one comma-separated line\n➐ show first 10 linesHaving said , ’m going keep columns now.Generally speaking, number unique values low compared total number rows, feature might treated categorical one (GEO case venture.csv).\nnumber equal number rows, might unique identifier might also numerical value.\n’s one way find : need go deeper.","code":"$ wc -l tips.csv\n245 tips.csv\n \n$ < tips.csv csvcut -c day | header -d | sort | uniq | wc -l\n4$ csvstat tips.csv --unique\n  1. bill: 229\n  2. tip: 123\n  3. sex: 2\n  4. smoker: 2\n  5. day: 4\n  6. time: 2\n  7. size: 6\n \n$ csvstat venture.csv --unique\n  1. FREQ: 1\n  2. TIME_FORMAT: 1\n  3. TIME_PERIOD: 9\n  4. EXPEND: 1\n  5. UNIT: 3\n  6. GEO: 20\n  7. OBS_STATUS: 1\n  8. OBS_VALUE: 286\n  9. FREQ_DESC: 1\n 10. TIME_FORMAT_DESC: 1\n 11. TIME_PERIOD_DESC: 9\n 12. OBS_STATUS_DESC: 1\n 13. EXPEND_DESC: 1\n 14. UNIT_DESC: 3\n 15. GEO_DESC: 20$ < venture.csv csvcut -C $( ➊\n>   csvstat venture.csv --unique | ➋\n>   grep ': 1$' | ➌\n>   cut -d. -f 1 | ➍\n>   tr -d ' ' | ➎\n>   paste -sd, ➏\n> ) | trim ➐\nTIME_PERIOD,UNIT,GEO,OBS_VALUE,TIME_PERIOD_DESC,UNIT_DESC,GEO_DESC\n2015,PC_GDP,CZ,0.002,Year 2015,Percentage of GDP,Czechia\n2007,PC_GDP,DE,0.034,Year 2007,Percentage of GDP,Germany\n2008,PC_GDP,DE,0.039,Year 2008,Percentage of GDP,Germany\n2009,PC_GDP,DE,0.029,Year 2009,Percentage of GDP,Germany\n2010,PC_GDP,DE,0.029,Year 2010,Percentage of GDP,Germany\n2011,PC_GDP,DE,0.029,Year 2011,Percentage of GDP,Germany\n2012,PC_GDP,DE,0.021,Year 2012,Percentage of GDP,Germany\n2013,PC_GDP,DE,0.023,Year 2013,Percentage of GDP,Germany\n2014,PC_GDP,DE,0.021,Year 2014,Percentage of GDP,Germany\n… with 531 more lines"},{"path":"chapter-7-exploring-data.html","id":"computing-descriptive-statistics","chapter":"7 Exploring Data","heading":"7.3 Computing Descriptive Statistics","text":"","code":""},{"path":"chapter-7-exploring-data.html","id":"column-statistics","chapter":"7 Exploring Data","heading":"7.3.1 Column Statistics","text":"command-line tool csvstat gives lot information. feature (column), shows:data typeWhether missing values (nulls)number unique valuesVarious descriptive statistics (minimum, maximum, sum, mean, standard deviation, median) features appropriateInvoke csvstat follows:’m showing first 32 lines, produces lot output. might want pipe less.\n’re interested specific statistic, can also use one following options:--max (maximum)--min (minimum)--sum (sum)--mean (mean)--median (median)--stdev (standard deviation)--nulls (whether column contains nulls)--unique (unique values)--freq (frequent values)--len (maximum value length)example:can select subset features -c option, accepts integers column names:nice extra, csvstat outputs, end, number data points (rows).\nNewlines commas inside values handled correctly.\nsee last line, can use tail.\nAlternatively, can use xsv, returns actual number rows.Note two options different using wc -l, counts number newlines (therefore also counts header).","code":"$ csvstat venture.csv | trim 32\n  1. \"FREQ\"\n \n        Type of data:          Text\n        Contains null values:  False\n        Unique values:         1\n        Longest value:         1 characters\n        Most common values:    A (540x)\n \n  2. \"TIME_FORMAT\"\n \n        Type of data:          Text\n        Contains null values:  False\n        Unique values:         1\n        Longest value:         3 characters\n        Most common values:    P1Y (540x)\n \n  3. \"TIME_PERIOD\"\n \n        Type of data:          Number\n        Contains null values:  False\n        Unique values:         9\n        Smallest value:        2,007\n        Largest value:         2,015\n        Sum:                   1,085,940\n        Mean:                  2,011\n        Median:                2,011\n        StDev:                 2.584\n        Most common values:    2,015 (60x)\n                               2,007 (60x)\n                               2,008 (60x)\n                               2,009 (60x)\n                               2,010 (60x)\n… with 122 more lines$ csvstat venture.csv --freq | trim\n  1. FREQ: { \"A\": 540 }\n  2. TIME_FORMAT: { \"P1Y\": 540 }\n  3. TIME_PERIOD: { \"2015\": 60, \"2007\": 60, \"2008\": 60, \"2009\": 60, \"2010\": 60 }\n  4. EXPEND: { \"INV_VEN\": 540 }\n  5. UNIT: { \"PC_GDP\": 180, \"NR_COMP\": 180, \"MIO_EUR\": 180 }\n  6. GEO: { \"CZ\": 27, \"DE\": 27, \"DK\": 27, \"EL\": 27, \"ES\": 27 }\n  7. OBS_STATUS: { \"None\": 540 }\n  8. OBS_VALUE: { \"0\": 28, \"1\": 19, \"2\": 14, \"0.002\": 10, \"0.034\": 7 }\n  9. FREQ_DESC: { \"Annual\": 540 }\n 10. TIME_FORMAT_DESC: { \"Annual\": 540 }\n… with 5 more lines$ csvstat venture.csv -c 3,GEO\n  3. \"TIME_PERIOD\"\n \n        Type of data:          Number\n        Contains null values:  False\n        Unique values:         9\n        Smallest value:        2,007\n        Largest value:         2,015\n        Sum:                   1,085,940\n        Mean:                  2,011\n        Median:                2,011\n        StDev:                 2.584\n        Most common values:    2,015 (60x)\n                               2,007 (60x)\n                               2,008 (60x)\n                               2,009 (60x)\n                               2,010 (60x)\n \n  6. \"GEO\"\n \n        Type of data:          Text\n        Contains null values:  False\n        Unique values:         20\n        Longest value:         2 characters\n        Most common values:    CZ (27x)\n                               DE (27x)\n                               DK (27x)\n                               EL (27x)\n                               ES (27x)\n \nRow count: 540$ csvstat venture.csv | tail -n 1\nRow count: 540\n \n$ xsv count venture.csv\n540"},{"path":"chapter-7-exploring-data.html","id":"r-one-liners-on-the-shell","chapter":"7 Exploring Data","heading":"7.3.2 R One-Liners on the Shell","text":"section ’d like introduce command-line tool called rush92,\nenables leverage statistical programming environment R93 directly command line.\nexplain rush exists, lets talk bit R .R powerful statistical software package data science.\n’s interpreted programming language, extensive collection packages, offers REPL, allows , similar command line, play data.\nNote , start R, ’re interactive session separated Unix command line.Imagine CSV file called tips.csv, like compute tip percentage, save result.\naccomplish R first run, R:➊ use --quiet option suppress rather long startup messageAnd run following code:➊ Load required packages\n➋ Read CSV file assign variable\n➌ Compute new column percent\n➍ Save result disk\n➎ Exit RAfterwards, can continue saved file percent.csv command line.Note third line associated want accomplish specifically.\nlines necessary boilerplate.\nTyping boilerplate order accomplish something simple cumbersome breaks workflow.\nSometimes, want one two things time data.\nWouldn’t great harness power R use command line?rush comes .\nLet’s perform task , now using rush:small one-liners possible rush takes care boilerplate.\ncase ’m using run subcommand. ’s also plot subcommand, ’ll use next section produce data visualizations quickly.\n’re passing input data, default, rush assumes ’s CSV format header comma delimiter.\nMoreover, column names sanitized easier work .\ncan override defaults using ---header (-H), --delimiter (-d), ---clean-names (-C) options, respectively.\nhelp gives good overview available options run subcommand:hood, rush generates R script subsequently executes .\ncan view generated script specifying --dry-run (-n) option:generated script:Writes shebang (#!; see Chapter 4) needed running R script command line.Imports tidyverse glue packages.Loads tips.csv data frame, cleans column names, assigns variable df.Runs specified expression.Prints result standard output.redirect generated script file easily turn new command-line tool shebang.output rush doesn’t CSV format per se. , compute mean tip percent, maximum party size, unique values time column, correlation bill tip. Finally, extract entire column (show first 10 values).last dash means rush read standard input.now, want one two things data set R, can specify one-liner, keep working command line.\nknowledge already R can now used command line. rush, can even create sophisticated visualizations, ’ll show next section.","code":"$ R --quiet ➊\n>> library(tidyverse)                            ➊\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──\n✔ ggplot2 3.3.3     ✔ purrr   0.3.4\n✔ tibble  3.0.6     ✔ dplyr   1.0.4\n✔ tidyr   1.1.2     ✔ stringr 1.4.0\n✔ readr   1.4.0     ✔ forcats 0.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n> df <- read_csv(\"tips.csv\")                    ➋\n \n── Column specification ────────────────────────────────────────────────────────\ncols(\n  bill = col_double(),\n  tip = col_double(),\n  sex = col_character(),\n  smoker = col_character(),\n  day = col_character(),\n  time = col_character(),\n  size = col_double()\n)\n \n> df <- mutate(df, percent = tip / bill * 100)  ➌\n> write_csv(df, \"percent.csv\")                  ➍\n> q(\"no\")                                       ➎\n \n$$ < percent.csv trim 5\nbill,tip,sex,smoker,day,time,size,percent\n16.99,1.01,Female,No,Sun,Dinner,2,5.9446733372572105\n10.34,1.66,Male,No,Sun,Dinner,3,16.054158607350097\n21.01,3.5,Male,No,Sun,Dinner,3,16.658733936220845\n23.68,3.31,Male,No,Sun,Dinner,2,13.97804054054054\n… with 240 more lines$ rm percent.csv\n \n$ rush run -t 'mutate(df, percent = tip / bill * 100)' tips.csv > percent.csv\n \n$ < percent.csv trim 5\nbill,tip,sex,smoker,day,time,size,percent\n16.99,1.01,Female,No,Sun,Dinner,2,5.9446733372572105\n10.34,1.66,Male,No,Sun,Dinner,3,16.054158607350097\n21.01,3.5,Male,No,Sun,Dinner,3,16.658733936220845\n23.68,3.31,Male,No,Sun,Dinner,2,13.97804054054054\n… with 240 more lines$ rush run --help\nrush: Run an R expression\n \nUsage:\n  rush run [options] <expression> [--] [<file>...]\n \nReading options:\n  -d, --delimiter <str>    Delimiter [default: ,].\n  -C, --no-clean-names     No clean names.\n  -H, --no-header          No header.\n \nSetup options:\n  -l, --library <name>     Libraries to load.\n  -t, --tidyverse          Enter the Tidyverse.\n \nSaving options:\n      --dpi <str|int>      Plot resolution [default: 300].\n      --height <int>       Plot height.\n  -o, --output <str>       Output file.\n      --units <str>        Plot size units [default: in].\n  -w, --width <int>        Plot width.\n \nGeneral options:\n  -n, --dry-run            Only print generated script.\n  -h, --help               Show this help.\n  -q, --quiet              Be quiet.\n      --seed <int>         Seed random number generator.\n  -v, --verbose            Be verbose.\n      --version            Show version.$ rush run -n --tidyverse 'mutate(df, percent = tip / bill * 100)' tips.csv\n#!/usr/bin/env Rscript\nlibrary(tidyverse)\nlibrary(glue)\ndf <- janitor::clean_names(readr::read_delim(\"tips.csv\", delim = \",\", col_names\n= TRUE))\nmutate(df, percent = tip/bill * 100)$ < percent.csv rush run 'mean(df$percent)' -\n16.0802581722505\n \n$ < percent.csv rush run 'max(df$size)' -\n6\n \n$ < percent.csv rush run 'unique(df$time)' -\nDinner\nLunch\n \n$ < percent.csv rush run 'cor(df$bill, df$tip)' -\n0.675734109211365\n \n$ < percent.csv rush run 'df$tip' - | trim\n1.01\n1.66\n3.5\n3.31\n3.61\n4.71\n2\n3.12\n1.96\n3.23\n… with 234 more lines"},{"path":"chapter-7-exploring-data.html","id":"creating-visualizations","chapter":"7 Exploring Data","heading":"7.4 Creating Visualizations","text":"section, ’m going show create data visualizations command line.\nUsing rush plot ’ll creating bar charts, scatter plots, box plots.\ndive , though, ’d first like explain can display visualizations.","code":""},{"path":"chapter-7-exploring-data.html","id":"displaying-images-from-the-command-line","chapter":"7 Exploring Data","heading":"7.4.1 Displaying Images from the Command Line","text":"Let’s take image tips.png example.\nTake look Figure 7.1, data visualization created using rush tips.csv dataset.\n(’ll explain rush syntax moment.)\nuse display tool insert image book, run display ’ll find doesn’t\nwork.\n’s displaying images command line actually quite tricky.\nFigure 7.1: Displaying image can tricky\nDepending setup, different options available display images.\nknow four options, advantages disadvantages:\n(1) textual representation,\n(2) inline image,\n(3) using image viewer, \n(4) using browser.\nLet’s go quickly.\nFigure 7.2: Displaying image terminal via ASCII characters ANSI escape sequences (top) via iTerm2 inline images protocol (bottom)\nOption 1 display image inside terminal shown top Figure 7.2.\noutput generated rush standard output redirected file.\n’s based ASCII characters ANSI escape sequences, ’s available every terminal.\nDepending ’re reading book, output get run code might might match screenshot Figure 7.2.see ASCII characters, means medium ’re reading book doesn’t support ANSI escape sequences responsible colors.\nFortunately, run command , look just like screenshot.Option 2, seen bottom Figure 7.2, also displays images inside terminal.\niTerm2 terminal, available macOS uses Inline Images Protocol small script (named display).\nscript included Docker image, can easily install :’re using iTerm2 macOS, might options available display images inline.\nPlease consult favorite search engine.\nFigure 7.3: Displaying image externally via file explorer image viewer (left) via webserver browser (right)\nOption 3 manually open image (tips.csv example) image viewer.\nFigure 7.3 shows, left, file explorer (Finder) image viewer (Preview) macOS.\n’re working locally, option always works.\n’re working inside Docker container, can access generated image OS ’ve mapped local directory using -v option.\nSee Chapter 2 instructions .\nadvantage option image viewers automatically update display image changed, allows quick iterations fine-tune visualization.Option 4 open image browser.\nright side Figure 7.3 screenshot Firefox showing http://localhost:8000/tips.png.\nbrowser , need two prerequisites option work.\nFirst, need made port (port 8000 example) accessible Docker container using -p option.\n(, see Chapter 2 instructions .)\nSecond, need start webserver.\n, Docker container small tool called servewd94, serves current working directory using Python:need run servewd directory (example, /data/) happily run background.\n’ve plotted something, can visit localhost:8000 browser access contents directory subdirectories.\ndefault port 8000, can change specifying argument servewd:Just make sure port accessible.\nservewd runs background, need stop follows:Option 4 can also work remote machine.Now ’ve covered four options display images, let’s move actually creating .","code":"$ rush plot --x bill --y tip --color size --facets '~day' tips.csv              \n                      Fri                               Sat                     \n  10.0                                                                  * #     \n   7.5                                                            *             \n                                                           #            *       \n   5.0          #               *                   ###  *   #  #               \n            # ###  ##  ##                       #####*####+ *    **  #          \n   2.5   # %###                           %  #########*#*## # #                 \nt                                                                          size \ni                     Sun                               Thur                6   \np 10.0                                                                      1   \n   7.5                                                         =                \n                    *    ** *                           #      *                \n   5.0    ##    #  +#*#  +* * #      =             ##*    = = #    +*           \n   2.5      ######## # *  ###   #  #            ## #####*  #                    \n           ## ####*  *  #+                   ###### #                           \n           10    20     30     40    50       10    20     30     40    50      \n                                      bill                                      $ curl -s \"https://iterm2.com/utilities/imgcat\" > display && chmod u+x display$ bat $(which servewd)\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: /usr/bin/dsutils/servewd\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ #!/usr/bin/env bash\n   2   │ ARGS=\"$@\"\n   3   │ python3 -m http.server ${ARGS} 2>/dev/null &\n───────┴────────────────────────────────────────────────────────────────────────$ servewd 9999$ pkill -f http.server"},{"path":"chapter-7-exploring-data.html","id":"plotting-in-a-rush","chapter":"7 Exploring Data","heading":"7.4.2 Plotting in a Rush","text":"comes creating data visualizations, ’s plethora options.\nPersonally, ’m staunch proponent ggplot2, visualization package R.\nunderlying grammar graphics accompanied consistent API allows quickly iteratively create different types beautiful data visualizations rarely consult documentation.\nwelcoming set properties exploring data.’re really rush, also don’t want fiddle much single visualization.\nMoreover, ’d like stay command line much possible.\nLuckily, still rush, allows us ggplot2 command line.\ndata visualization Figure 7.1 created follows:However, may noticed, used different command create tips.png:syntax ggplot2 relatively concise, especially considering flexibility offers, ’s shortcut create basic plots quickly.\nshortcut available plot subcommand rush.\nallows create beautiful basic plots without needing learn R grammar graphics.hood, rush plot uses function qplot ggplot2 package.\n’s first part documentation:agree advice; ’re done reading book, ’ll worthwhile learn ggplot2, especially want upgrade exploratory data visualizations ones suitable communication.\nnow, ’re command line, let’s take shortcut.Figure 7.2 already showed, rush plot can create graphical visualizations (consisting pixels) textual visualizations (consisting ASCII characters ANSI escape sequences) syntax.\nrush detects output piped another command (display redirected file tips.png produce graphical visualization; otherwise produce textual visualization.Let’s take moment read plotting saving options rush plot:important options plotting options take <name> argument.\nexample, --x option allows specify column used determine things placed along x axis.\nholds --y option.\n--color --fill options used specify column want use coloring.\ncan probably guess --size --alpha options .\ncommon options explained throughout sections create various visualizations.\nNote visualization, first show textual representation (ASCII ANSI characters) visual representation (pixels).","code":"$ rush run --library ggplot2 'ggplot(df, aes(x = bill, y = tip, color = size)) +\n geom_point() + facet_wrap(~day)' tips.csv > tips.png$ rush plot --x bill --y tip --color size --facets '~day' tips.csv > tips.png$ R -q -e '?ggplot2::qplot' | trim 14\n> ?ggplot2::qplot\nqplot                 package:ggplot2                  R Documentation\n \nQuick plot\n \nDescription:\n \n     ‘qplot()’ is a shortcut designed to be familiar if you're used to\n     base ‘plot()’. It's a convenient wrapper for creating a number of\n     different types of plots using a consistent calling scheme. It's\n     great for allowing you to produce plots quickly, but I highly\n     recommend learning ‘ggplot()’ as it makes it easier to create\n     complex graphics.\n \n… with 108 more lines$ rush plot --help\nrush: Quick plot\n \nUsage:\n  rush plot [options] [--] [<file>|-]\n \nReading options:\n  -d, --delimiter <str>    Delimiter [default: ,].\n  -C, --no-clean-names     No clean names.\n  -H, --no-header          No header.\n \nSetup options:\n  -l, --library <name>     Libraries to load.\n  -t, --tidyverse          Enter the Tidyverse.\n \nPlotting options:\n      --aes <key=value>    Additional aesthetics.\n  -a, --alpha <name>       Alpha column.\n  -c, --color <name>       Color column.\n      --facets <formula>   Facet specification.\n  -f, --fill <name>        Fill column.\n  -g, --geom <geom>        Geometry [default: auto].\n      --group <name>       Group column.\n      --log <x|y|xy>       Variables to log transform.\n      --margins            Display marginal facets.\n      --post <code>        Code to run after plotting.\n      --pre <code>         Code to run before plotting.\n      --shape <name>       Shape column.\n      --size <name>        Size column.\n      --title <str>        Plot title.\n  -x, --x <name>           X column.\n      --xlab <str>         X axis label.\n  -y, --y <name>           Y column.\n      --ylab <str>         Y axis label.\n  -z, --z <name>           Z column.\n \nSaving options:\n      --dpi <str|int>      Plot resolution [default: 300].\n      --height <int>       Plot height.\n  -o, --output <str>       Output file.\n      --units <str>        Plot size units [default: in].\n  -w, --width <int>        Plot width.\n \nGeneral options:\n  -n, --dry-run            Only print generated script.\n  -h, --help               Show this help.\n  -q, --quiet              Be quiet.\n      --seed <int>         Seed random number generator.\n  -v, --verbose            Be verbose.\n      --version            Show version."},{"path":"chapter-7-exploring-data.html","id":"creating-bar-charts","chapter":"7 Exploring Data","heading":"7.4.3 Creating Bar Charts","text":"Bar charts especially useful displaying value counts categorical feature.\n’s textual visualization time feature tips dataset:Figure 7.4 shows graphical visualization, created rush plot output redirected file.\nFigure 7.4: bar chart\nconclusion can draw bar chart straightforward: twice many data points dinner lunch.","code":"$ rush plot --x time tips.csv           \n         ********************************                                       \n         ********************************                                       \n  150    ********************************                                       \n         ********************************                                       \n         ********************************                                       \n         ********************************                                       \n  100    ********************************                                       \n         ********************************                                       \n         ********************************  ********************************     \n         ********************************  ********************************     \n   50    ********************************  ********************************     \n         ********************************  ********************************     \n         ********************************  ********************************     \n         ********************************  ********************************     \n    0    ********************************  ********************************     \n                      Dinner                            Lunch                   \n                                        time                                    $ rush plot --x time tips.csv > plot-bar.png\n \n$ display plot-bar.png"},{"path":"chapter-7-exploring-data.html","id":"creating-histograms","chapter":"7 Exploring Data","heading":"7.4.4 Creating Histograms","text":"counts continuous variable can visualized histogram.\n, used time feature set fill color.\nresult, rush plot conveniently creates stacked histogram.Figure 7.5 shows graphical visualization.\nFigure 7.5: histogram\nhistogram reveals tips around 2,5 USD.\ntwo groups dinner lunch groups stacked top show absolute counts, ’s difficult compare .\nPerhaps density plot can help .","code":"$ rush plot --x tip --fill time tips.csv\n             ===                                                                \n             ===     ===                                                        \n  40         ===     ===                                                        \n             ===     ===                                                        \n             ===     ===                                                        \n  30         ===     ===                                                        \n             ===     ===                                                 time   \n           =====     ===                                                 Dinner \n  20       ==+++ === =====                                               Lunch  \n           ==+++==== =====  ===                                          +      \n           ==+++==== =====  ===   ===                                           \n  10       +++++==========  ===   ===                                           \n       ====+++++++++=+++====+++== =====                                         \n       ==+++++++++++++++++==+++++=+++====== ======                              \n   0   ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++         \n                 2.5              5.0              7.5             10.0         \n                                     tip                                        $ !! > plot-histogram.png\nrush plot --x tip --fill time tips.csv > plot-histogram.png\n \n$ display !$\ndisplay plot-histogram.png"},{"path":"chapter-7-exploring-data.html","id":"creating-density-plots","chapter":"7 Exploring Data","heading":"7.4.5 Creating Density Plots","text":"density plot useful visualizing distribution continuous variable.\nrush plot uses heuristics determine appropriate geometry, can override geom option:case, textual representation really shows limitations compared visual representation Figure 7.6.\nFigure 7.6: density plot\n","code":"$ rush plot --x tip --fill time --geom density tips.csv                         \n  0.5        @@@                                                                \n            @@+@@                                                               \n  0.4       @+++@@                                                              \n           @@++++@                                                              \n           @+++++@@  @@                                                         \n  0.3      @++++++@@@@=@@                                                       \n          @++++++++@@===@@                                               time   \n          @+++++++++@@====@                                              Dinner \n  0.2    @+++++++++++@@@===@@                                            Lunch  \n         @+++++++++++++@@@==@@                                           @      \n        @++++++++++++++++@@@@@@@                                                \n  0.1  @@+++++++++++++++++++++@@@@@@@                                           \n       ++++++++++++++++++++++++++@++@@@@                                        \n       ++++++++++++++++++++++++++++++++@@@@@@@@@@@                              \n  0.0  ++++++++++++++++++++++++++++++++++++++++++@@@@@@@@@@@@@@@@@@@@@@         \n                 2.5              5.0              7.5              10.0        \n                                     tip                                        $ rush plot --x tip --fill time --geom density tips.csv > plot-density.png\n \n$ display plot-density.png"},{"path":"chapter-7-exploring-data.html","id":"happy-little-accidents","chapter":"7 Exploring Data","heading":"7.4.6 Happy Little Accidents","text":"’ve already seen three types visualizations.\nggplot2, correspond functions geom_bar, geom_histogram, geom_density.\ngeom short geometry dictates actually plotted.\ncheat sheet ggplot2 provides good overview available geometry types.\ngeometry types can use depend columns (types) specify.\nevery combination makes sense.\nTake line plot example.happy little accident becomes clearer visual representation Figure 7.7.\nFigure 7.7: happy little accident\nrows tips.csv independent observations, whereas drawing line data points assumes connected.\n’s better visualize relationship tip bill scatter plot.","code":"$ rush plot --x tip --y bill --color size --size day --geom path tips.csv       \n  50                                           #*               *   #####       \n                         #        ==        #***            ****#####           \n                 #        ###    =**+    #***          *****####                \n  40              #  ##**   ####***+======         *****###                     \n                   #*# ###******###+#**   ======****###                    day  \n        ###################*****##*+*##*****####=#                         Fri  \nb 30    #    *+++########**##==****+****#######                            Sat  \ni       # #** *##++####**#===*%=====####*##**#                             Sun  \nl      #   #  ######***#=####==##########    **                            Thur \nl 20   # #########**#####****#####++                                            \n       ########*########*###### #  *                                       size \n       ######################    ## *                                      5    \n  10    ###########      ####      ##*                                          \n       %%##  ##                      #                                          \n       %                                                                        \n                2.5               5.0               7.5               10.0      \n                                      tip                                       $ rush plot --x tip --y bill --color size --size day --geom path tips.csv > plot\n-accident.png\n \n$ display plot-accident.png"},{"path":"chapter-7-exploring-data.html","id":"creating-scatter-plots","chapter":"7 Exploring Data","heading":"7.4.7 Creating Scatter Plots","text":"scatter plot, geometry point, happens default specifying two continuous features:Note color point specified --color option (--fill option).\nSee Figure 7.8 visual representation.\nFigure 7.8: scatter plot\nscatter plot may conclude ’s relationship amount bill tip.\nPerhaps ’s useful examine data higher level creating trend lines.","code":"$ rush plot --x bill --y tip --color time tips.csv                              \n  10.0                                                                =         \n                                                                   =            \n                                                                                \n                                                                                \n   7.5                                                 =                        \n                                  =      =       +                =             \nt                                   +     =   =                          time   \ni             =                   =   =    ==    +                       Dinner \np  5.0                         = =   = +     =+   =     =+  +     =      Lunch  \n                 =     = =+==+++= =  = +=  +         =                          \n                   =  += =++= ======== =     =   =             =                \n                  == =====+=====+=++     ===   =     ==  =   =                  \n   2.5        ++=++++=+=+==== ==  ===  = = ==                                   \n              =+ ===+ + == =++        =                                         \n        =   = =      =    =                    =                                \n                 10          20           30           40           50          \n                                     bill                                       $ rush plot --x bill --y tip --color time tips.csv > plot-scatter.png\n \n$ display plot-scatter.png"},{"path":"chapter-7-exploring-data.html","id":"creating-trend-lines","chapter":"7 Exploring Data","heading":"7.4.8 Creating Trend Lines","text":"override default geometry smooth, can visualize trend lines.\nuseful seeing bigger picture.rush plot handle transparency, visual representation (see Figure 7.9 much better case.\nFigure 7.9: Trend lines\nlike visualize original points along trend lines, need resort writing ggplot2 code rush run (see Figure 7.10).\nFigure 7.10: Trend lines original points combined\n","code":"$ rush plot --x bill --y tip --color time --geom smooth tips.csv                \n                                                                     ==         \n                                                                   ====         \n  7.5                                                            ======         \n                                                               ========         \n                                                     ==================         \n                                             =======+++++++++======             \nt 5.0                                  ====+++++++++==========           time   \ni                                 ====++++++=================            Dinner \np                           ===+++++++=============                      Lunch  \n       ==              ==+++++++=====                                    =      \n  2.5  ==============++++====                                                   \n       ======++++++++===                                                        \n       ==========                                                               \n       =====                                                                    \n  0.0  ==                                                                       \n                10           20           30           40           50          \n                                     bill                                       $ rush plot --x bill --y tip --color time --geom smooth tips.csv > plot-trend.pn\ng\n \n$ display plot-trend.png$ rush run --library ggplot2 'ggplot(df, aes(x = bill, y = tip, color = time)) +\n geom_point() + geom_smooth()' tips.csv > plot-trend-points.png\n \n$ display plot-trend-points.png"},{"path":"chapter-7-exploring-data.html","id":"creating-box-plots","chapter":"7 Exploring Data","heading":"7.4.9 Creating Box Plots","text":"box plot visualizes, one features, five-number summary: minimum, maximum, sample median, first third quartiles.\ncase need convert size feature categorical one using factor() function, otherwise values bill feature lumped together.textual representation bad, visual one much clearer (see Figure 7.11).\nFigure 7.11: box plot\nUnsurprisingly, box plot shows , average, larger party size leads higher bill.","code":"$ rush plot --x 'factor(size)' --y bill --geom boxplot tips.csv                 \n  50                               %                                            \n                                   %           %                        %       \n                                   %           %                        %       \n  40                   %                       %           %            %       \n                                   %       %%%%%%%%%%              %%%%%%%%%%   \n                       %           %       %        %              %%%%%%%%%%   \nb 30                               %       %        %  %%%%%%%%%%  %%%%%%%%%%   \ni                      %      %%%%%%%%%%%  %%%%%%%%%%                           \nl                      %      %         %  %%%%%%%%%%                           \nl 20              %%%%%%%%%%  %%%%%%%%%%%      %           %                    \n                  %%%%%%%%%%  %%%%%%%%%%%                                       \n                  %%%%%%%%%%       %                                            \n  10  %%%%%%%%%%       %                                                        \n      %%%%%%%%%%                                                                \n                                                                                \n          1            2           3           4           5           6        \n                                   factor(size)                                 $ rush plot --x 'factor(size)' --y bill --geom boxplot tips.csv > plot-boxplot.p\nng\n \n$ display plot-boxplot.png"},{"path":"chapter-7-exploring-data.html","id":"adding-labels","chapter":"7 Exploring Data","heading":"7.4.10 Adding Labels","text":"default labels based column names (specification).\nprevious image, label factor(size) improved.\nUsing --xlab --ylab options can override labels x y axes.\ntitle can added --title option.\n’s violin plot (mashup box plot density plot) demonstrating (see also Figure 7.12.\nFigure 7.12: violin plot title labels\nAnnotating visualization proper labels title especially useful want share others (future self) ’s easier understand ’s shown.","code":"$ rush plot --x 'factor(size)' --y bill --geom violin --title 'Distribution of b\nill amount per party size' --xlab 'Party size' --ylab 'Bill (USD)' tips.csv     \n   Distribution of bill amount per party size                                   \n  50                               %                                            \n                                   %           %%                      %%       \n                                   %           %%                      %%       \n  40                   %           %          %%%        %%%%%%        %%       \nB                      %           %          % %          %          %%%%      \ni                      %          %%%         % %     %%%%%%%%%%%%    %  %%     \nl 30                  %%          % %         % %%    %%%%% %%%%%%    %%%%%     \nl                     %%%         % %         %  %         %%                   \n                     %% %        %% %%        %  %       %%%%%%                 \n( 20                %%  %%       %   %        %%%%                              \nU                   %    %%      %% %%                                          \nS 10 %%%%%%%%%%%%   %%% %%        %%%                                           \nD    %%%%%  %%%%%     %%%                                                       \n)       %%%%%%                                                                  \n          1            2           3           4           5           6        \n                                    Party size                                  $ rush plot --x 'factor(size)' --y bill --geom violin --title 'Distribution of b\nill amount per party size' --xlab 'Party size' --ylab 'Bill (USD)' tips.csv > pl\not-labels.png\n \n$ display plot-labels.png"},{"path":"chapter-7-exploring-data.html","id":"going-beyond-basic-plots","chapter":"7 Exploring Data","heading":"7.4.11 Going Beyond Basic Plots","text":"Although rush plot suitable creating basic plots ’re exploring data, certainly limitations.\nSometimes need flexibility sophisticated options multiple geometries, coordinate transformations, theming.\ncase might worthwhile learn underlying package rush plot draws capabilities, namely ggplot2 package R.\n’re Python R, ’s plotnine package, reimplementation ggplot2 Python.","code":""},{"path":"chapter-7-exploring-data.html","id":"summary-6","chapter":"7 Exploring Data","heading":"7.5 Summary","text":"chapter ’ve looked various ways explore data.\ntextual graphical data visualizations pros cons.\ngraphical ones obviously much higher quality, can tricky view command line.\ntextual visualizations come handy.\nleast rush , thanks R ggplot2, consistent syntax creating types.next chapter , , intermezzo chapter discuss can speed commands pipelines.\nFeel free read chapter later can’t wait start modeling data Chapter 9.","code":""},{"path":"chapter-7-exploring-data.html","id":"for-further-exploration-6","chapter":"7 Exploring Data","heading":"7.6 For Further Exploration","text":"proper ggplot2 tutorial unfortunately beyond scope book. want get better visualizing data strongly recommend invest time understanding power beauty grammar graphics. Chapters 3 28 book R Data Science Hadley Wickham Garrett Grolemund excellent resource.Speaking Chapters 3 28, translated Python using Plotnine Pandas case ’re Python R.","code":""},{"path":"chapter-8-parallel-pipelines.html","id":"chapter-8-parallel-pipelines","chapter":"8 Parallel Pipelines","heading":"8 Parallel Pipelines","text":"previous chapters, ’ve dealing commands pipelines take care entire task .\npractice, however, may find facing task requires command pipeline run multiple times.\n, example, may need :Scrape hundreds web pagesMake dozens API calls transform outputTrain classifier range parameter valuesGenerate scatter plots every pair features datasetIn examples, ’s certain form repetition involved.\nfavorite scripting programming language, take care loop loop.\ncommand line, first thing might inclined press bring back previous command, modify necessary, press Enter run command .\nfine two three times, imagine dozens times.\napproach quickly becomes cumbersome, inefficient, prone errors.\ngood news can write loops command line well.\n’s chapter .Sometimes, repeating fast command one (serial manner) sufficient.\nmultiple cores (perhaps even multiple machines) nice make use , especially ’re faced data-intensive task.\nusing multiple cores machines, total running time may reduced significantly.\nchapter introduce powerful tool called parallel95 can take care exactly . enables apply command pipeline range arguments numbers, lines, files.\nPlus, name implies, allows run commands parallel.","code":""},{"path":"chapter-8-parallel-pipelines.html","id":"overview-5","chapter":"8 Parallel Pipelines","heading":"8.1 Overview","text":"intermezzo chapter discusses several approaches speed tasks require commands pipelines run many times.\nmain goal demonstrate flexibility power parallel.\ntool can combined tool discussed book, positively change way use command line data science.\nchapter, ’ll learn :Running commands serial range numbers, lines, filesBreaking large task several smaller tasksRunning pipelines parallelDistributing pipelines multiple machinesThis chapter starts following files:instructions get files Chapter 2.\nfiles either downloaded generated using command-line tools.","code":"$ cd /data/ch08\n \n$ l\ntotal 20K\n-rw-r--r-- 1 dst dst  126 Dec 14 11:54 emails.txt\n-rw-r--r-- 1 dst dst   61 Dec 14 11:54 movies.txt\n-rwxr-xr-x 1 dst dst  125 Dec 14 11:54 slow.sh*\n-rw-r--r-- 1 dst dst 5.1K Dec 14 11:54 users.json"},{"path":"chapter-8-parallel-pipelines.html","id":"serial-processing","chapter":"8 Parallel Pipelines","heading":"8.2 Serial Processing","text":"dive parallelization, ’ll briefly discuss looping serial fashion.\n’s worthwhile know functionality always available, syntax closely resembles looping programming languages, really make appreciate parallel.examples provided introduction chapter, can distill three types items loop : numbers, lines, files.\nthree types items discussed next three subsections, respectively.","code":""},{"path":"chapter-8-parallel-pipelines.html","id":"looping-over-numbers","chapter":"8 Parallel Pipelines","heading":"8.2.1 Looping Over Numbers","text":"Imagine need compute square every even integer 0 100. ’s tool called bc96, basic calculator can pipe equation .\ncommand compute square 4 looks follows:one-calculation, .\nHowever, mentioned introduction, need crazy press , change number, press Enter 50 times!\ncase ’s better let shell hard work using loop:➊ Z shell feature called brace expansion, transforms {0..100..2} list separated spaces: 0 2 4 … 98 100. variable assigned value “0” first iteration, “1” second iteration, forth.\n➌ value variable can used prefixing dollar sign ($). shell replace $value echo executed. Note can one command done.Although syntax may appear bit odd compared favorite programming language, ’s worth remembering ’s always available shell.\n’ll introduce better flexible way repeating commands moment.","code":"$ echo \"4^2\" | bc\n16$ for i in {0..100..2}  ➊\n> do\n> echo \"$i^2\" | bc      ➋\n> done | trim\n0\n4\n16\n36\n64\n100\n144\n196\n256\n324\n… with 41 more lines"},{"path":"chapter-8-parallel-pipelines.html","id":"looping-over-lines","chapter":"8 Parallel Pipelines","heading":"8.2.2 Looping Over Lines","text":"second type items can loop lines.\nlines can come either file standard input.\ngeneric approach lines can contain anything, including: numbers, dates, email addresses.Imagine ’d want send email contacts.\nLet’s first generate fake users using free Random User Generator API:can loop lines emails loop:➊ case need use loop Z shell know beforehand many lines input consists .\n➋ Although curly braces around line variable necessary case (since variable names contain periods), ’s still good practice.\n➌ redirection can also placed .can also provide input loop interactively specifying special file standard input /dev/stdin. Press Ctrl-D done.method, however, disadvantage , press Enter, commands done run immediately line input. ’s turning back.","code":"$ curl -s \"https://randomuser.me/api/1.2/?results=5&seed=dsatcl2e\" > users.json\n \n$ < users.json jq -r '.results[].email' > emails\n \n$ bat emails\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: emails\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ selma.andersen@example.com\n   2   │ kent.clark@example.com\n   3   │ ditmar.niehaus@example.com\n   4   │ benjamin.robinson@example.com\n   5   │ paulo.muller@example.com\n───────┴────────────────────────────────────────────────────────────────────────$ while read line                         ➊\n> do\n> echo \"Sending invitation to ${line}.\"   ➋\n> done < emails                           ➌\nSending invitation to selma.andersen@example.com.\nSending invitation to kent.clark@example.com.\nSending invitation to ditmar.niehaus@example.com.\nSending invitation to benjamin.robinson@example.com.\nSending invitation to paulo.muller@example.com.$ while read line; do echo \"You typed: ${line}.\"; done < /dev/stdin\none\nYou typed: one.\ntwo\nYou typed: two.\nthree\nYou typed: three."},{"path":"chapter-8-parallel-pipelines.html","id":"looping-over-files","chapter":"8 Parallel Pipelines","heading":"8.2.3 Looping Over Files","text":"section discuss third type item often need loop : files.handle special characters, use globbing (.e., pathname expansion) instead ls97:Just brace expansion, expression /data/* first expanded list Z shell ’s processed loop.elaborate alternative listing files find98, :Can traverse directoriesAllows elaborate searching properties size, access time, permissionsHandles special characters spaces newlinesFor example, following find invocation lists files located directory /data csv extension smaller 2 kilobyte:","code":"$ for chapter in /data/*\n> do\n> echo \"Processing Chapter ${chapter}.\"\n> done\nProcessing Chapter /data/ch02.\nProcessing Chapter /data/ch03.\nProcessing Chapter /data/ch04.\nProcessing Chapter /data/ch05.\nProcessing Chapter /data/ch06.\nProcessing Chapter /data/ch07.\nProcessing Chapter /data/ch08.\nProcessing Chapter /data/ch09.\nProcessing Chapter /data/ch10.\nProcessing Chapter /data/csvconf.$ find /data -type f -name '*.csv' -size -2k\n/data/ch03/tmnt-basic.csv\n/data/ch03/tmnt-missing-newline.csv\n/data/ch03/tmnt-with-header.csv\n/data/ch05/irismeta.csv\n/data/ch05/names-comma.csv\n/data/ch05/names.csv\n/data/ch07/datatypes.csv"},{"path":"chapter-8-parallel-pipelines.html","id":"parallel-processing","chapter":"8 Parallel Pipelines","heading":"8.3 Parallel Processing","text":"Let’s say long running tool, one shown :➊ ts99 adds timestamp.\n➋ magic variable RANDOM calls internal Bash function returns pseudorandom integer 0 32767. Taking remainder division integer 5 adding 1 ensures duration 1 5.\n➌ sleep pauses execution given number seconds.process probably doesn’t take available resources.\nhappens need run command lot times.\nexample, need download whole sequence files.naive way parallelize run commands background.\nLet’s run slow.sh three times:➊ ampersand (&) sends command background, allowing loop continue immediately next iteration.\n➋ line shows job number given Z shell process ID, can used fine-grained job control. topic, powerful, beyond scope book.Figure 8.1 illustrates, conceptual level, difference serial processing, naive parallel processing, parallel processing GNU Parallel terms number concurrent processes total amount time takes run everything.\nFigure 8.1: Serial processing, naive parallel processing, parallel processing GNU Parallel\ntwo problems naive approach.\nFirst, ’s way control many processes running concurrently.\nstart many jobs , competing resources CPU, memory, disk access, network bandwidth.\nlead longer time run everything.\nSecond, ’s difficult tell output belongs input.\nLet’s look better approach.","code":"$ bat slow.sh\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: slow.sh\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ #!/bin/bash\n   2   │ echo \"Starting job $1\" | ts ➊\n   3   │ duration=$((1+RANDOM%5)) ➋\n   4   │ sleep $duration ➌\n   5   │ echo \"Job $1 took ${duration} seconds\" | ts\n───────┴────────────────────────────────────────────────────────────────────────$ for i in {A..C}; do\n> ./slow.sh $i & ➊\n> done\n[2] 386 ➋\n[3] 388\n[4] 391\n \n$ Dec 14 11:54:18 Starting job A\nDec 14 11:54:18 Starting job B\nDec 14 11:54:18 Starting job C\nDec 14 11:54:20Dec 14 11:54:20 Job B took 2 seconds\n Job C took 2 seconds\n \n[3]  - done       ./slow.sh $i\n$\n[4]  + done       ./slow.sh $i\n$ Dec 14 11:54:23 Job A took 5 seconds\n \n[2]  + done       ./slow.sh $i\n$"},{"path":"chapter-8-parallel-pipelines.html","id":"introducing-gnu-parallel","chapter":"8 Parallel Pipelines","heading":"8.3.1 Introducing GNU Parallel","text":"Allow introduce parallel, command-line tool allows parallelize distribute commands pipelines.\nbeauty tool existing tools can used ; need modified.go details parallel, ’s little teaser show easy replace -loop earlier:parallel simplest form: items loop passed via standard input aren’t arguments command parallel needs run.\nSee Figure 8.2 illustration parallel concurrently distributes input among processes collects outputs.\nFigure 8.2: GNU Parallel concurrently distributes input among processes collects outputs\ncan see basically acts loop.\n’s another teaser, replaces loop previous section., using --jobs option, specify parallel can run two jobs concurrently. arguments slow.sh specified argument instead via standard input.whopping 159 different options, parallel offers lot functionality.\n(Perhaps much.)\nLuckily need know handful order effective.\nmanual page quite informative case need use less common option.","code":"$ seq 0 2 100 | parallel \"echo {}^2 | bc\" | trim\n0\n16\n4\n36\n64\n100\n144\n196\n324\n400\n… with 41 more lines$ parallel --jobs 2 ./slow.sh ::: {A..C}\nDec 14 11:54:28 Starting job B\nDec 14 11:54:31 Job B took 3 seconds\nDec 14 11:54:28 Starting job A\nDec 14 11:54:33 Job A took 5 seconds\nDec 14 11:54:32 Starting job C\nDec 14 11:54:37 Job C took 5 seconds"},{"path":"chapter-8-parallel-pipelines.html","id":"specifying-input","chapter":"8 Parallel Pipelines","heading":"8.3.2 Specifying Input","text":"important argument parallel, command pipeline ’d like run every input.\nquestion : input item inserted command line?\ndon’t specify anything, input item appended end pipeline.running:➊ output , redirect /dev/null suppress .Although often works, advise explicit input item inserted command using placeholders.\ncase, want use entire input line (number) , need one placeholder.\nspecify placeholder, words, put input item, pair curly braces ({}):input items filenames, couple modifiers can use parts filename.\nexample, {/}, basename filename used:➊ Characters parentheses ()) quotes (\") special meaning shell. use literally put backslash \\ front . called escaping.input line multiple parts separated delimiter can add numbers placeholders.example:, can apply placeholder modifiers.\nalso possible reuse input item.\ninput parallel CSV file header, can use column names placeholders:","code":"$ seq 3 | parallel cowsay\n \n ___\n< 1 >\n ---\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n ___\n< 2 >\n ---\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n ___\n< 3 >\n ---\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||$ cowsay 1 > /dev/null ➊\n \n$ cowsay 2 > /dev/null\n \n$ cowsay 3 > /dev/null$ seq 3 | parallel cowsay {} > /dev/null$ find /data/ch03 -type f | parallel echo '{#}\\) \\\"{}\\\" has basename \\\"{/}\\\"' ➊\n1) \"/data/ch03/tmnt-basic.csv\" has basename \"tmnt-basic.csv\"\n2) \"/data/ch03/logs.tar.gz\" has basename \"logs.tar.gz\"\n3) \"/data/ch03/tmnt-missing-newline.csv\" has basename \"tmnt-missing-newline.csv\"\n4) \"/data/ch03/r-datasets.db\" has basename \"r-datasets.db\"\n5) \"/data/ch03/top2000.xlsx\" has basename \"top2000.xlsx\"\n6) \"/data/ch03/tmnt-with-header.csv\" has basename \"tmnt-with-header.csv\"$ < input.csv parallel --colsep , \"mv {2} {1}\" > /dev/null$ < input.csv parallel -C, --header : \"invite {name} {email}\""},{"path":"chapter-8-parallel-pipelines.html","id":"controlling-the-number-of-concurrent-jobs","chapter":"8 Parallel Pipelines","heading":"8.3.3 Controlling the Number of Concurrent Jobs","text":"default, parallel runs one job per CPU core.\ncan control number jobs run concurrently --jobs -j option.\nSpecifying number means many jobs run concurrently.\nput plus sign front number parallel run N jobs plus number CPU cores. put minus sign front number parallel run N-M jobs.\nN number CPU cores.\ncan also specify percentage, default 100% number CPU cores.\noptimal number jobs run concurrently depends actual commands running.specify -j1, commands run serial. Even though doesn’t name tool justice, still uses. example, need access API allows one connection time. specify -j0, parallel run many jobs parallel possible. can compared loop ampersand. advised.","code":"$ seq 5 | parallel -j0 \"echo Hi {}\"\nHi 1\nHi 2\nHi 3\nHi 4\nHi 5$ seq 5 | parallel -j200% \"echo Hi {}\"\nHi 1\nHi 2\nHi 3\nHi 4\nHi 5"},{"path":"chapter-8-parallel-pipelines.html","id":"logging-and-output","chapter":"8 Parallel Pipelines","heading":"8.3.4 Logging and Output","text":"save output command, might tempted following:save output individual files.\n, want save everything one big file following:However, parallel offers --results option, stores output separate files.\njob, parallel creates three files: seq, holds job number, stdout contains output produced job, stderr contains errors produced job.\nthree files placed subdirectories based input values.parallel still prints output, redundant case.\ncan redirect standard input standard output /dev/null follows:See Figure 8.3 pictorial overview --results option works.\nFigure 8.3: GNU Parallel stores output separate files --results option\n’re running multiple jobs parallel, order jobs run may correspond order input.\noutput jobs therefore also mixed .\nkeep order, specify --keep-order option -k option.Sometimes ’s useful record input generated output.\nparallel allows tag output --tag option, prepends line input item.","code":"$ seq 5 | parallel \"echo \\\"Hi {}\\\" > hi-{}.txt\"$ seq 5 | parallel \"echo Hi {}\" >> one-big-file.txt$ seq 10 | parallel --results outdir \"curl 'https://anapioficeandfire.com/api/ch\naracters/{}' | jq -r '.aliases[0]'\" 2>/dev/null 1>&2\n \n$ tree outdir | trim\noutdir\n└── 1\n    ├── 1\n    │   ├── seq\n    │   ├── stderr\n    │   └── stdout\n    ├── 10\n    │   ├── seq\n    │   ├── stderr\n    │   └── stdout\n… with 34 more lines$ seq 5 | parallel --tag \"echo 'sqrt({})' | bc -l\"\n1       1\n3       1.73205080756887729352\n4       2.00000000000000000000\n2       1.41421356237309504880\n5       2.23606797749978969640\n \n$ parallel --tag --keep-order \"echo '{1}*{2}' | bc -l\" ::: 3 4 ::: 5 6 7\n3 5     15\n3 6     18\n3 7     21\n4 5     20\n4 6     24\n4 7     28"},{"path":"chapter-8-parallel-pipelines.html","id":"creating-parallel-tools","chapter":"8 Parallel Pipelines","heading":"8.3.5 Creating Parallel Tools","text":"bc tool, used beginning chapter, parallel .\nHowever, can parallelize using parallel.\nDocker image contains tool called pbc100.\ncode shown :tool allows us simplify code used beginning chapter .\ncan process comma-separated values simultaneously:","code":"$ bat $(which pbc)\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: /usr/bin/dsutils/pbc\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ #!/bin/bash\n   2   │ # pbc: parallel bc. First column of input CSV is mapped to {1}, second\n       │ to {2}, and so forth.\n   3   │ #\n   4   │ # Example usage: paste -d, <(seq 100) <(seq 100 -1 1) | ./pbc 'sqrt({1}\n       │ *{2})'\n   5   │ #\n   6   │ # Dependency: GNU parallel\n   7   │ #\n   8   │ # Author: http://jeroenjanssens.com\n   9   │\n  10   │ parallel -C, -k -j100% \"echo '$1' | bc -l\"\n───────┴────────────────────────────────────────────────────────────────────────$ seq 100 | pbc '{1}^2' | trim\n1\n4\n9\n16\n25\n36\n49\n64\n81\n100\n… with 90 more lines\n \n$ paste -d, <(seq 4) <(seq 4) <(seq 4) | pbc 'sqrt({1}+{2})^{3}'\n1.41421356237309504880\n4.00000000000000000000\n14.69693845669906858905\n63.99999999999999999969"},{"path":"chapter-8-parallel-pipelines.html","id":"distributed-processing","chapter":"8 Parallel Pipelines","heading":"8.4 Distributed Processing","text":"Sometimes need power local machine, even cores, can offer.\nLuckily, parallel can also leverage power remote machines, really allows speed pipeline.’s great parallel doesn’t installed remote machine.\n’s required can connect remote machine Secure Shell protocol (SSH), also parallel uses distribute pipeline.\n(parallel installed helpful can determine many cores employ remote machine; later.)First, ’m going obtain list running AWS EC2 instances.\nDon’t worry don’t remote machines, can replace occurrence --slf hostnames, tells parallel remote machines use, --sshlogin :.\nway, can still follow along examples section.know remote machines take , ’re going consider three flavors distributed processing:Running ordinary commands remote machinesDistributing local data directly among remote machinesSending files remote machines, process , retrieve results","code":""},{"path":"chapter-8-parallel-pipelines.html","id":"get-list-of-running-aws-ec2-instances","chapter":"8 Parallel Pipelines","heading":"8.4.1 Get List of Running AWS EC2 Instances","text":"section ’re creating file named hostnames contain one hostname remote machine per line.\n’m using Amazon Web Services (AWS) example.\nassume AWS account know launch instances.\n’re using different cloud computing service (Google Cloud Platform Microsoft Azure), servers, please make sure create hostnames file continuing next section.can obtain list running AWS EC2 instances using aws101, command-line interface AWS API.\naws, can almost everything can online AWS Management Console.command aws ec2 describe-instances returns lot information EC2 instances JSON format (see online documentation information).\ncan extract relevant fields using jq:possible states EC2 instance : pending, running, shutting-, terminated, stopping, stopped.\ncan distribute pipeline running instances, filter non-running instances follows:(Without -r --raw-output option, hostnames surrounded double quotes.)\noutput saved hostnames, can pass parallel later.mentioned, parallel employs ssh102 connect remote machines.\nwant connect EC2 instances without typing credentials every time, can add something like following text file ~/.ssh/config.Depending distribution running, user name may different ubuntu.","code":"$ aws ec2 describe-instances | jq '.Reservations[].Instances[] | {public_dns: .P\nublicDnsName, state: .State.Name}'> aws ec2 describe-instances | jq -r '.Reservations[].Instances[] | select(.Stat\ne.Name==\"running\") | .PublicDnsName' | tee hostnames\nec2-54-88-122-140.compute-1.amazonaws.com\nec2-54-88-89-208.compute-1.amazonaws.com$ bat ~/.ssh/config\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: /home/dst/.ssh/config\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ Host *.amazonaws.com\n   2   │         IdentityFile ~/.ssh/MyKeyFile.pem\n   3   │         User ubuntu\n───────┴────────────────────────────────────────────────────────────────────────"},{"path":"chapter-8-parallel-pipelines.html","id":"running-commands-on-remote-machines","chapter":"8 Parallel Pipelines","heading":"8.4.2 Running Commands on Remote Machines","text":"first flavor distributed processing run ordinary commands remote machines.\nLet’s first double check parallel working running tool hostname103 EC2 instance:, --sshloginfile --slf option used refer file hostnames.\n--nonall option instructs parallel execute command every remote machine hostnames file without using parameters.\nRemember, don’t remote machines utilize, can replace --slf hostnames --sshlogin : command run local machine:Running command every remote machine requires one core per machine. wanted distribute list arguments passed parallel potentially use one core. number cores specified explicitly, parallel try determine .case, parallel installed one two remote machines.\n’m getting warning message indicating parallel found one .\nresult, parallel determine number cores default using one core.\nreceive warning message, can one following four things:Don’t worry, happy using one core per machineSpecify number jobs machine via --jobs -j optionSpecify number cores use per machine putting, example, 2/ want two cores, front hostname hostnames fileInstall parallel using package manager. example, remote machines run Ubuntu:","code":"$ parallel --nonall --sshloginfile hostnames hostname\nip-172-31-23-204\nip-172-31-23-205$ parallel --nonall --sshlogin : hostname\ndata-science-toolbox$ seq 2 | parallel --slf hostnames echo 2>&1\nbash: parallel: command not found\nparallel: Warning: Could not figure out number of cpus on ec2-54-88-122-140.comp\nute-1.amazonaws.com (). Using 1.\n1\n2$ parallel --nonall --slf hostnames \"sudo apt-get install -y parallel\""},{"path":"chapter-8-parallel-pipelines.html","id":"distributing-local-data-among-remote-machines","chapter":"8 Parallel Pipelines","heading":"8.4.3 Distributing Local Data among Remote Machines","text":"second flavor distributed processing distribute local data directly among remote machines.\nImagine one large dataset want process using multiple remote machines.\nsimplicity, let’s sum integers 1 1000.\nFirst, let’s double check input actually distributed printing hostname remote machine length input received using wc:Excellent. can see 1000 numbers get distributed evenly subsets 100 (specified -N100).\nNow, ’re ready sum numbers:, immediately also sum ten sums get back remote machines.\nLet’s check answer correct calculation without parallel:Good, works.\nlarger pipeline want execute remote machines, can also put separate script upload parallel.\n’ll demonstrate creating simple command-line tool called add:Using --basefile option, parallel first uploads file add remote machines running jobs:Summing 1000 numbers course toy example.\nPlus, ’ve much faster locally.\nStill, hope ’s clear parallel can incredibly powerful.","code":"$ seq 1000 | parallel -N100 --pipe --slf hostnames \"(hostname; wc -l) | paste -s\nd:\"\nip-172-31-23-204:100\nip-172-31-23-205:100\nip-172-31-23-205:100\nip-172-31-23-204:100\nip-172-31-23-205:100\nip-172-31-23-204:100\nip-172-31-23-205:100\nip-172-31-23-204:100\nip-172-31-23-205:100\nip-172-31-23-204:100$ seq 1000 | parallel -N100 --pipe --slf hostnames \"paste -sd+ | bc\" | paste -sd\n \n500500$ seq 1000 | paste -sd+ | bc\n500500$ echo '#!/usr/bin/env bash' > add\n \n$ echo 'paste -sd+ | bc' >> add\n \n$ bat add\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: add\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ #!/usr/bin/env bash\n   2   │ paste -sd+ | bc\n───────┴────────────────────────────────────────────────────────────────────────\n \n$ chmod u+x add\n \n$ seq 1000 | ./add\n500500$ seq 1000 |\n> parallel -N100 --basefile add --pipe --slf hostnames './add' |\n> ./add\n500500"},{"path":"chapter-8-parallel-pipelines.html","id":"processing-files-on-remote-machines","chapter":"8 Parallel Pipelines","heading":"8.4.4 Processing Files on Remote Machines","text":"third flavor distributed processing send files remote machines, process , retrieve results.\nImagine want count borough New York City, often receive service calls 311.\ndon’t data local machine yet, let’s first obtain free NYC Open Data API:now 10 files containing compressed JSON data:Note jq -c '.[]' used flatten array JSON objects ’s one object per line, total 100 lines per file.\nUsing zcat104, directly print contents compress file:Let’s see one line JSON looks like using:get total number service calls per borough local machine, run following command:➊ Expand compressed files using zcat.\n➋ call, extract name borough using jq.\n➌ Convert borough names lowercase replace spaces underscores (awk splits whitespace default).\n➍ Count occurrences borough using sort uniq.\n➎ Reverse two columns delimit comma delimited using awk.\n➏ Add header using header.Imagine, moment, machine slow simply perform pipeline locally.\ncan use parallel distribute local files among remote machines, let processing, retrieve results:➊ Print list files pipe parallel\n➋ Transmit jq binary remote machine. Luckily, jq dependencies. file removed remote machines afterwards specified --trc option (implies --cleanup option). Note pipeline uses ./jq instead just jq. ’s pipeline needs use version uploaded version may may search path.\n➌ command-line argument --trc {.}.csv short --transfer --return {.}.csv --cleanup. (replacement string {.} gets replaced input filename without last extension.) , means JSON file gets transferred remote machine, CSV file gets returned local machine, files removed job remote machine\n➍ Specify list hostnames. Remember, want try locally, can specify --sshlogin : instead --slf hostnames\n➎ Note escaping awk expression. Quoting can sometimes tricky. , dollar signs double quotes escaped. quoting ever gets confusing, remember put pipeline separate command-line tool just addIf , process, run ls one remote machines, see parallel indeed transfers (cleans ) binary jq, JSON files, CSV files:CSV file looks something like :can sum counts CSV file using rush105 tidyverse:, prefer use SQL aggregate results, can use csvsql discussed Chapter 5:","code":"$ seq 0 100 900 | parallel  \"curl -sL 'http://data.cityofnewyork.us/resource/erm\n2-nwe9.json?\\$limit=100&\\$offset={}' | jq -c '.[]' | gzip > nyc-{#}.json.gz\"$ l nyc*json.gz\n-rw-r--r-- 1 dst dst 16K Dec 14 11:55 nyc-10.json.gz\n-rw-r--r-- 1 dst dst 13K Dec 14 11:56 nyc-1.json.gz\n-rw-r--r-- 1 dst dst 14K Dec 14 11:55 nyc-2.json.gz\n-rw-r--r-- 1 dst dst 14K Dec 14 11:55 nyc-3.json.gz\n-rw-r--r-- 1 dst dst 14K Dec 14 11:55 nyc-4.json.gz\n-rw-r--r-- 1 dst dst 16K Dec 14 11:55 nyc-5.json.gz\n-rw-r--r-- 1 dst dst 16K Dec 14 11:55 nyc-6.json.gz\n-rw-r--r-- 1 dst dst 16K Dec 14 11:55 nyc-7.json.gz\n-rw-r--r-- 1 dst dst 16K Dec 14 11:56 nyc-8.json.gz\n-rw-r--r-- 1 dst dst 15K Dec 14 11:56 nyc-9.json.gz$ zcat nyc-1.json.gz | trim\n{\"unique_key\":\"52779474\",\"created_date\":\"2021-12-13T02:10:31.000\",\"agency\":\"NYP…\n{\"unique_key\":\"52776058\",\"created_date\":\"2021-12-13T02:09:50.000\",\"agency\":\"NYP…\n{\"unique_key\":\"52775678\",\"created_date\":\"2021-12-13T02:08:53.000\",\"agency\":\"NYP…\n{\"unique_key\":\"52782776\",\"created_date\":\"2021-12-13T02:07:37.000\",\"closed_date\"…\n{\"unique_key\":\"52778629\",\"created_date\":\"2021-12-13T02:07:32.000\",\"agency\":\"NYP…\n{\"unique_key\":\"52776019\",\"created_date\":\"2021-12-13T02:07:23.000\",\"agency\":\"NYP…\n{\"unique_key\":\"52776002\",\"created_date\":\"2021-12-13T02:04:07.000\",\"agency\":\"NYP…\n{\"unique_key\":\"52775975\",\"created_date\":\"2021-12-13T02:02:46.000\",\"agency\":\"NYP…\n{\"unique_key\":\"52776757\",\"created_date\":\"2021-12-13T02:01:36.000\",\"agency\":\"NYP…\n{\"unique_key\":\"52780492\",\"created_date\":\"2021-12-13T02:01:35.000\",\"agency\":\"NYP…\n… with 90 more lines$ zcat nyc-1.json.gz | head -n 1\n{\"unique_key\":\"52779474\",\"created_date\":\"2021-12-13T02:10:31.000\",\"agency\":\"NYPD\n\",\"agency_name\":\"New York City Police Department\",\"complaint_type\":\"Encampment\",\n\"descriptor\":\"N/A\",\"location_type\":\"Subway\",\"status\":\"In Progress\",\"community_bo\nard\":\"Unspecified QUEENS\",\"borough\":\"QUEENS\",\"x_coordinate_state_plane\":\"1039396\n\",\"y_coordinate_state_plane\":\"195150\",\"open_data_channel_type\":\"MOBILE\",\"park_fa\ncility_name\":\"Unspecified\",\"park_borough\":\"QUEENS\",\"bridge_highway_name\":\"E\",\"br\nidge_highway_segment\":\"Mezzanine\",\"latitude\":\"40.702146602995356\",\"longitude\":\"-\n73.80111202259863\",\"location\":{\"latitude\":\"40.702146602995356\",\"longitude\":\"-73.\n80111202259863\",\"human_address\":\"{\\\"address\\\": \\\"\\\", \\\"city\\\": \\\"\\\", \\\"state\\\":\n\\\"\\\", \\\"zip\\\": \\\"\\\"}\"},\":@computed_region_efsh_h5xi\":\"24340\",\":@computed_region_\nf5dn_yrer\":\"41\",\":@computed_region_yeji_bk3q\":\"3\",\":@computed_region_92fq_4b7q\":\n\"6\",\":@computed_region_sbqj_enih\":\"61\"}$ zcat nyc*json.gz | ➊\n> jq -r '.borough' | ➋\n> tr '[A-Z] ' '[a-z]_' | ➌\n> sort | uniq -c | sort -nr | ➍\n> awk '{print $2\",\"$1}' | ➎\n> header -a borough,count | ➏\n> csvlook\n│ borough       │ count │\n├───────────────┼───────┤\n│ brooklyn      │   285 │\n│ queens        │   271 │\n│ manhattan     │   226 │\n│ bronx         │   200 │\n│ staten_island │    18 │$ ls *.json.gz | ➊\n> parallel -v --basefile jq \\ ➋\n> --trc {.}.csv \\ ➌\n> --slf hostnames \\ ➍\n> \"zcat {} | ./jq -r '.borough' | tr '[A-Z] ' '[a-z]_' | sort | uniq -c | awk '{\nprint \\$2\\\",\\\"\\$1}' > {.}.csv\" ➎$ ssh $(head -n 1 hostnames) ls> cat nyc-1.json.csv\nbronx,3\nbrooklyn,5\nmanhattan,24\nqueens,3\nstaten_island,2$ cat nyc*csv | header -a borough,count |\n> rush run -t 'group_by(df, borough) %>% summarize(count = sum(count))' - |\n> csvsort -rc count | csvlook\n│ borough       │ count │\n├───────────────┼───────┤\n│ brooklyn      │   285 │\n│ queens        │   271 │\n│ manhattan     │   226 │\n│ bronx         │   200 │\n│ staten_island │    18 │$ cat nyc*csv | header -a borough,count |\n> csvsql --query 'SELECT borough, SUM(count) AS count FROM stdin GROUP BY boroug\nh ORDER BY count DESC' |\n> csvlook\n│ borough       │ count │\n├───────────────┼───────┤\n│ brooklyn      │   285 │\n│ queens        │   271 │\n│ manhattan     │   226 │\n│ bronx         │   200 │\n│ staten_island │    18 │"},{"path":"chapter-8-parallel-pipelines.html","id":"summary-7","chapter":"8 Parallel Pipelines","heading":"8.5 Summary","text":"data scientist, work data–occasionally lot data.\nmeans sometimes need run command multiple times distribute data-intensive commands multiple cores.\nchapter shown easy parallelize commands.\nparallel powerful flexible tool speed ordinary command-line tools distribute .\noffers lot functionality chapter ’ve able scratch surface.\nnext chapter ’m going cover fourth step OSEMN model: modeling data.","code":""},{"path":"chapter-8-parallel-pipelines.html","id":"for-further-exploration-7","chapter":"8 Parallel Pipelines","heading":"8.6 For Further Exploration","text":"basic understanding parallel important options, recommend take look online tutorial. ’ll learn, among things, specify different ways specifying input, keep log jobs, timeout, resume, retry jobs. creator parallel Ole Tange tutorial says, “command line love .”","code":""},{"path":"chapter-9-modeling-data.html","id":"chapter-9-modeling-data","chapter":"9 Modeling Data","heading":"9 Modeling Data","text":"chapter ’re going perform fourth step OSEMN model: modeling data.\nGenerally speaking, model abstract higher-level description data.\nModeling bit like creating visualizations sense ’re taking step back individual data points see bigger picture.Visualizations characterized shapes, positions, colors: can interpret looking .\nModels, hand, internally characterized numbers, means computers can use things like make predictions new data points.\n(can still visualize models can try understand see performing.)chapter ’ll consider three types algorithms commonly used model data:Dimensionality reductionRegressionClassificationThese algorithms come field statistics machine learning, ’m going change vocabulary bit.\nLet’s assume CSV file, also known dataset.\nrow, except header, considered data point.\ndata point one features, properties measured.\nSometimes, data point also label, , generally speaking, judgment outcome.\nbecomes concrete introduce wine dataset .first type algorithm (dimensionality reduction) often unsupervised, means create model based features dataset .\nlast two types algorithms (regression classification) definition supervised algorithms, means also incorporate labels model.","code":""},{"path":"chapter-9-modeling-data.html","id":"overview-6","chapter":"9 Modeling Data","heading":"9.1 Overview","text":"chapter, ’ll learn :Reduce dimensionality dataset using tapkee106.Predict quality white wine using vw107.Classify wine red white using skll108.chapter starts following file:instructions get files Chapter 2.\nfiles either downloaded generated using command-line tools.","code":"$ cd /data/ch09\n \n$ l\ntotal 4.0K\n-rw-r--r-- 1 dst dst 503 Dec 14 11:57 classify.cfg"},{"path":"chapter-9-modeling-data.html","id":"more-wine-please","chapter":"9 Modeling Data","heading":"9.2 More Wine Please!","text":"Throughout chapter, ’ll using dataset wine tasters’ notes red white varieties Portuguese wine called vinho verde.\ndata point represents wine. wine rated 11 physicochemical properties: (1) fixed acidity, (2) volatile acidity, (3) citric acid, (4) residual sugar, (5) chlorides, (6) free sulfur dioxide, (7) total sulfur dioxide, (8) density, (9) pH, (10) sulphates, (11) alcohol.\nalso overall quality score 0 (bad) 10 (excellent), median least three evaluation wine experts. information dataset available UCI Machine Learning Repository.dataset split two files: one white wine one red wine.\nfirst step obtain two files using curl (course parallel haven’t got day):triple colon just another way pass data parallel.Let’s inspect files count number lines:➊ clarity use nl add line numbers.\n➋ see entire header, use fold.first sight data appears quite clean.\nStill, let’s scrub conforms command-line tools expect.\nSpecifically, ’ll:Convert header lowercase.Replace semi-colons commas.Replace spaces underscores.Remove unnecessary quotes.tool tr can take care things.\nLet’s use loop time—old times’ sake—process files:Let’s also create single dataset combining two files.\n’ll use csvstack109 add column named type, “red” rows first file, “white” rows second file:➊ new column type placed beginning csvstack.\n➋ algorithms assume label last column, use xsv move column type end.’s good practice check whether missing values dataset, machine learning algorithms can’t handle :Excellent!\nmissing values, fill , say, average common value feature.\nalternative, less subtle approach remove data points least one missing value.\nJust curiosity, let’s see distribution quality looks like red white wines.\n(#fig:plot_wine_quality)Comparing quality red white wines using density plot\ndensity plot can see quality white wine distributed towards higher values.\nmean white wines overall better red wines, white wine experts easily give higher scores red wine experts?\n’s something data doesn’t tell us.\nperhaps relationship alcohol quality?\nLet’s use rush find :\n(#fig:plot_wine_alchohol_vs_quality)Relationship alcohol contents wine quality\nEureka! Ahem, let’s carry modeling, shall ?","code":"$ parallel \"curl -sL http://archive.ics.uci.edu/ml/machine-learning-databases/wi\nne-quality/winequality-{}.csv > wine-{}.csv\" ::: red white$ cp /data/.cache/wine-*.csv .$ < wine-red.csv nl | ➊\n> fold | ➋\n> trim\n     1  \"fixed acidity\";\"volatile acidity\";\"citric acid\";\"residual sugar\";\"chlor\nides\";\"free sulfur dioxide\";\"total sulfur dioxide\";\"density\";\"pH\";\"sulphates\";\"a\nlcohol\";\"quality\"\n     2  7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5\n     3  7.8;0.88;0;2.6;0.098;25;67;0.9968;3.2;0.68;9.8;5\n     4  7.8;0.76;0.04;2.3;0.092;15;54;0.997;3.26;0.65;9.8;5\n     5  11.2;0.28;0.56;1.9;0.075;17;60;0.998;3.16;0.58;9.8;6\n     6  7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5\n     7  7.4;0.66;0;1.8;0.075;13;40;0.9978;3.51;0.56;9.4;5\n     8  7.9;0.6;0.06;1.6;0.069;15;59;0.9964;3.3;0.46;9.4;5\n… with 1592 more lines\n \n$ < wine-white.csv nl | fold | trim\n     1  \"fixed acidity\";\"volatile acidity\";\"citric acid\";\"residual sugar\";\"chlor\nides\";\"free sulfur dioxide\";\"total sulfur dioxide\";\"density\";\"pH\";\"sulphates\";\"a\nlcohol\";\"quality\"\n     2  7;0.27;0.36;20.7;0.045;45;170;1.001;3;0.45;8.8;6\n     3  6.3;0.3;0.34;1.6;0.049;14;132;0.994;3.3;0.49;9.5;6\n     4  8.1;0.28;0.4;6.9;0.05;30;97;0.9951;3.26;0.44;10.1;6\n     5  7.2;0.23;0.32;8.5;0.058;47;186;0.9956;3.19;0.4;9.9;6\n     6  7.2;0.23;0.32;8.5;0.058;47;186;0.9956;3.19;0.4;9.9;6\n     7  8.1;0.28;0.4;6.9;0.05;30;97;0.9951;3.26;0.44;10.1;6\n     8  6.2;0.32;0.16;7;0.045;30;136;0.9949;3.18;0.47;9.6;6\n… with 4891 more lines\n \n$ wc -l wine-{red,white}.csv\n  1600 wine-red.csv\n  4899 wine-white.csv\n  6499 total$ for COLOR in red white; do\n> < wine-$COLOR.csv tr '[A-Z]; ' '[a-z],_' | tr -d \\\" > wine-${COLOR}-clean.csv\n> done$ csvstack -g red,white -n type wine-{red,white}-clean.csv | ➊\n> xsv select 2-,1 > wine.csv ➋$ csvstat wine.csv --nulls\n  1. fixed_acidity: False\n  2. volatile_acidity: False\n  3. citric_acid: False\n  4. residual_sugar: False\n  5. chlorides: False\n  6. free_sulfur_dioxide: False\n  7. total_sulfur_dioxide: False\n  8. density: False\n  9. ph: False\n 10. sulphates: False\n 11. alcohol: False\n 12. quality: False\n 13. type: False$ rush run -t 'ggplot(df, aes(x = quality, fill = type)) + geom_density(adjust =\n 3, alpha = 0.5)' wine.csv > wine-quality.png\n \n$ display wine-quality.png$ rush plot --x alcohol --y quality --color type --geom smooth wine.csv > wine-a\nlcohol-vs-quality.png\n \n$ display wine-alcohol-vs-quality.png"},{"path":"chapter-9-modeling-data.html","id":"dimensionality-reduction-with-tapkee","chapter":"9 Modeling Data","heading":"9.3 Dimensionality Reduction with Tapkee","text":"goal dimensionality reduction map high-dimensional data points onto lower dimensional mapping.\nchallenge keep similar data points close together lower-dimensional mapping.\n’ve seen previous section, wine dataset contains 13 features.\n’ll stick two dimensions ’s straightforward visualize.Dimensionality reduction often regarded part exploration.\n’s useful many features plotting.\nscatter-plot matrix, shows two features time.\n’s also useful pre-processing step machine learning algorithms.dimensionality reduction algorithms unsupervised.\nmeans don’t employ labels data points order construct lower-dimensional mapping.section ’ll look two techniques: PCA, stands Principal Components Analysis110 t-SNE, stands t-distributed Stochastic Neighbor Embedding111.","code":""},{"path":"chapter-9-modeling-data.html","id":"introducing-tapkee","chapter":"9 Modeling Data","heading":"9.3.1 Introducing Tapkee","text":"Tapkee C++ template library dimensionality reduction112.\nlibrary contains implementations many dimensionality reduction algorithms, including:Locally Linear EmbeddingIsomapMultidimensional ScalingPCAt-SNEMore information algorithms can found Tapkee’s website.\nAlthough Tapkee mainly library can included applications, also offers command-line tool tapkee.\n’ll use perform dimensionality reduction wine dataset.","code":""},{"path":"chapter-9-modeling-data.html","id":"linear-and-non-linear-mappings","chapter":"9 Modeling Data","heading":"9.3.2 Linear and Non-linear Mappings","text":"First, ’ll scale features using standardization feature equally important.\ngenerally leads better results applying machine learning algorithms.scale use rush tidyverse package.➊ need temporary remove column type scale() works numerical columns.\n➋ scale() function accepts data frame, returns matrix.\n➌ function as_tibble() converts matrix back data frame.\n➍ Finally, add back type column.Now apply dimensionality reduction techniques visualize mapping using Rio-scatter:➊ Deselect column type\n➋ Remove header\n➌ Apply PCA➊ Add back header columns pc1 pc2\n➋ Add back column typeNow can create scatter plot:\nFigure 9.1: Linear dimensionality reduction PCA\nLet’s perform t-SNE approach:➊ Deselect column type\n➋ Remove header\n➌ Apply t-SNE\n➍ Add back header columns x y\n➎ Add back column type\n➏ Create scatter plot\nFigure 9.2: Non-linear dimensionality reduction t-SNE\ncan see t-SNE better job PCA separating red white wines based physicochemical properties.\nscatter plots verify dataset certain structure; ’s relationship features labels.\nKnowing , ’m comfortable moving forward applying supervised machine learning.\n’ll start regression task continue classification task.","code":"$ rush run --tidyverse --output wine-scaled.csv \\\n> 'select(df, -type) %>% ➊\n> scale() %>% ➋\n> as_tibble() %>% ➌\n> mutate(type = df$type)' wine.csv ➍\n \n$ csvlook wine-scaled.csv\n│ fixed_acidity │ volatile_acidity │ citric_acid │ residual_sugar │ chlorides │…\n├───────────────┼──────────────────┼─────────────┼────────────────┼───────────┤…\n│        0.142… │           2.189… │     -2.193… │        -0.745… │    …\n│        0.451… │           3.282… │     -2.193… │        -0.598… │    …\n│        0.451… │           2.553… │     -1.917… │        -0.661… │    …\n│        3.074… │          -0.362… │      1.661… │        -0.745… │    …\n│        0.142… │           2.189… │     -2.193… │        -0.745… │    …\n│        0.142… │           1.946… │     -2.193… │        -0.766… │    …\n│        0.528… │           1.581… │     -1.780… │        -0.808… │    …\n│        0.065… │           1.885… │     -2.193… │        -0.892… │    …\n… with 6489 more lines$ xsv select '!type' wine-scaled.csv | ➊\n> header -d | ➋\n> tapkee --method pca | ➌\n> tee wine-pca.txt | trim\n-0.568882,3.34818\n-1.19724,3.22835\n-0.952507,3.23722\n-1.60046,1.67243\n-0.568882,3.34818\n-0.556231,3.15199\n-0.53894,2.28288\n1.104,2.56479\n0.231315,2.86763\n-1.18363,1.81641\n… with 6487 more lines$ < wine-pca.txt header -a pc1,pc2 | ➊\n> paste -d, - <(xsv select type wine-scaled.csv) | ➋\n> tee wine-pca.csv | csvlook\n│      pc1 │     pc2 │ type  │\n├──────────┼─────────┼───────┤\n│  -0.569… │  3.348… │ red   │\n│  -1.197… │  3.228… │ red   │\n│  -0.953… │  3.237… │ red   │\n│  -1.600… │  1.672… │ red   │\n│  -0.569… │  3.348… │ red   │\n│  -0.556… │  3.152… │ red   │\n│  -0.539… │  2.283… │ red   │\n│   1.104… │  2.565… │ red   │\n… with 6489 more lines$ rush plot --x pc1 --y pc2 --color type --shape type wine-pca.csv > wine-pca.pn\ng\n \n$ display wine-pca.png$ xsv select '!type' wine-scaled.csv | ➊\n> header -d | ➋\n> tapkee --method t-sne | ➌\n> header -a x,y | ➍\n> paste -d, - <(xsv select type wine-scaled.csv) | ➎\n> rush plot --x x --y y --color type --shape type > wine-tsne.png ➏$ display wine-tsne.png"},{"path":"chapter-9-modeling-data.html","id":"regression-with-vowpal-wabbit","chapter":"9 Modeling Data","heading":"9.4 Regression with Vowpal Wabbit","text":"section, ’m going create model predicts quality white wine, based physicochemical properties.\nquality number 0 10, can consider regression task.’ll using Vowpal Wabbit, vw.","code":""},{"path":"chapter-9-modeling-data.html","id":"preparing-the-data","chapter":"9 Modeling Data","heading":"9.4.1 Preparing the Data","text":"Instead working CSV, vw data format.\ntool csv2vw113 can, name implies, convert CSV format.\n--label option used indicate column contains labels.\nLet’s examine result:format, line one data point.\nline starts label, followed pipe symbol feature name/value pairs separated spaces.\nformat may seem overly verbose compared CSV format, offer flexibility weights, tags, namespaces, sparse feature representation.\nwine dataset don’t need flexibility, might useful applying vw complicated problems.\narticle explains vw format detail.One ’ve created, trained regression model, can used make predictions new, unseen data points.\nwords, give model wine hasn’t seen , can predict, test, quality.\nproperly evaluate accuracy predictions, need set aside data used training.\n’s common use 80% complete dataset training remaining 20% testing.can first splitting complete dataset five equal parts using split114.\nverify number data points part using wc.➊ tool shuf115 randomizes dataset ensure training test similar quality distribution.Now can use first part (20%) testing set wine-test.vw combine four remaining parts (80%) training set wine-train.vw:Now ’re ready train model using vw.","code":"$ csv2vw wine-white-clean.csv --label quality | trim\n6 | alcohol:8.8 chlorides:0.045 citric_acid:0.36 density:1.001 fixed_acidity:7 …\n6 | alcohol:9.5 chlorides:0.049 citric_acid:0.34 density:0.994 fixed_acidity:6.…\n6 | alcohol:10.1 chlorides:0.05 citric_acid:0.4 density:0.9951 fixed_acidity:8.…\n6 | alcohol:9.9 chlorides:0.058 citric_acid:0.32 density:0.9956 fixed_acidity:7…\n6 | alcohol:9.9 chlorides:0.058 citric_acid:0.32 density:0.9956 fixed_acidity:7…\n6 | alcohol:10.1 chlorides:0.05 citric_acid:0.4 density:0.9951 fixed_acidity:8.…\n6 | alcohol:9.6 chlorides:0.045 citric_acid:0.16 density:0.9949 fixed_acidity:6…\n6 | alcohol:8.8 chlorides:0.045 citric_acid:0.36 density:1.001 fixed_acidity:7 …\n6 | alcohol:9.5 chlorides:0.049 citric_acid:0.34 density:0.994 fixed_acidity:6.…\n6 | alcohol:11 chlorides:0.044 citric_acid:0.43 density:0.9938 fixed_acidity:8.…\n… with 4888 more lines$ csv2vw wine-white-clean.csv --label quality |\n> shuf | ➊\n> split -d -n r/5 - wine-part-\n \n$ wc -l wine-part-*\n   980 wine-part-00\n   980 wine-part-01\n   980 wine-part-02\n   979 wine-part-03\n   979 wine-part-04\n  4898 total$ mv wine-part-00 wine-test.vw\n \n$ cat wine-part-* > wine-train.vw\n \n$ rm wine-part-*\n \n$ wc -l wine-*.vw\n   980 wine-test.vw\n  3918 wine-train.vw\n  4898 total"},{"path":"chapter-9-modeling-data.html","id":"training-the-model","chapter":"9 Modeling Data","heading":"9.4.2 Training the Model","text":"tool vw accepts many different options (nearly 400!).\nLuckily, don’t need order effective.\nannotate options use , ’ll put one separate line:➊ file wine-train.vw used train model.\n➋ model, regressor, stored file wine.model.\n➌ Number training passes.\n➍ Caching needed making multiple passes.\n➎ Use neural network 3 hidden units.\n➏ Create use quadratic features, based input features. duplicates removed vw.\n➐ Use l2 regularization.\n➑ Use 25 bits store features.Now trained regression model, let’s use make predictions.","code":"$ vw \\\n> --data wine-train.vw \\ ➊\n> --final_regressor wine.model \\ ➋\n> --passes 10 \\ ➌\n> --cache_file wine.cache \\ ➍\n> --nn 3 \\ ➎\n> --quadratic :: \\ ➏\n> --l2 0.000005 \\ ➐\n> --bit_precision 25 ➑\ncreating quadratic features for pairs: ::\nWARNING: any duplicate namespace interactions will be removed\nYou can use --leave_duplicate_interactions to disable this behaviour.\nusing l2 regularization = 5e-06\nfinal_regressor = wine.model\nNum weight bits = 25\nlearning rate = 0.5\ninitial_t = 0\npower_t = 0.5\ndecay_learning_rate = 1\ncreating cache_file = wine.cache\nReading datafile = wine-train.vw\nnum sources = 1\nEnabled reductions: gd, generate_interactions, nn, scorer\naverage  since         example        example  current  current  current\nloss     last          counter         weight    label  predict features\n25.000000 25.000000            1            1.0   5.0000   0.0000       78\n21.514251 18.028502            2            2.0   5.0000   0.7540       78\n23.981016 26.447781            4            4.0   6.0000   1.5814       78\n21.543597 19.106179            8            8.0   7.0000   2.1586       78\n16.715053 11.886508           16           16.0   7.0000   2.8977       78\n12.412012 8.108970           32           32.0   6.0000   3.8832       78\n7.698827 2.985642           64           64.0   8.0000   4.8759       78\n4.547053 1.395279          128          128.0   7.0000   5.7022       78\n2.780491 1.013930          256          256.0   6.0000   5.9425       78\n1.797196 0.813900          512          512.0   7.0000   5.9101       78\n1.292476 0.787756         1024         1024.0   4.0000   5.8295       78\n1.026469 0.760462         2048         2048.0   6.0000   5.9139       78\n0.945076 0.945076         4096         4096.0   6.0000   6.1987       78 h\n0.792362 0.639647         8192         8192.0   6.0000   6.2091       78 h\n0.690935 0.589508        16384        16384.0   5.0000   5.5898       78 h\n0.643649 0.596364        32768        32768.0   6.0000   6.1262       78 h\n \nfinished run\nnumber of examples per pass = 3527\npasses used = 10\nweighted example sum = 35270.000000\nweighted label sum = 206890.000000\naverage loss = 0.585270 h\nbest constant = 5.865891\ntotal feature number = 2749380"},{"path":"chapter-9-modeling-data.html","id":"testing-the-model","chapter":"9 Modeling Data","heading":"9.4.3 Testing the Model","text":"model stored file wine.model.\nuse model make predictions, run vw , now different set options:➊ file wine-test.vw used test model.\n➋ Use model stored file wine.model.\n➌ Ignore label information just test.\n➍ predictions stored file called predictions.\n➎ Don’t output diagnostics progress updates.Let’s use paste combine predictions file predictions true, observed, values file wine-test.vw.\nUsing awk, can compare predicted values observed values compute mean absolute error (MAE).\nMAE tells us far vw average, comes predicting quality white wine., predictions average 0.6 points .\nLet’s visualize relationship observed values predicted values using rush plot:\nFigure 9.3: Regression Vowpal Wabbit\ncan imagine options used train model can bit overwhelming.\nLet’s see vw performs use default values:➊ Train regression model\n➋ Test regression model\n➌ Compute mean absolute errorApparently, default values, MAE 0.04 higher, meaning predictions slightly worse.section, ’ve able scratch surface vw can .\n’s reason accepts many options.\nBesides regression, also supports, among things, binary classification, multi-class classification, reinforcement learning, Latent Dirichlet Allocation.\nwebsite contains many tutorials articles learn .","code":"$ vw \\\n> --data wine-test.vw \\ ➊\n> --initial_regressor wine.model \\ ➋\n> --testonly \\ ➌\n> --predictions predictions \\ ➍\n> --quiet ➎\n \n$ bat predictions | trim\n6.702528\n6.537283\n5.633761\n6.569905\n5.934127\n5.485150\n5.768181\n6.452881\n4.978302\n5.834136\n… with 970 more lines$ paste -d, predictions <(cut -d '|' -f 1 wine-test.vw) |\n> tee results.csv |\n> awk -F, '{E+=sqrt(($1-$2)^2)} END {print \"MAE: \" E/NR}' |\n> cowsay ➊\n _______________\n< MAE: 0.586385 >\n ---------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||$ < results.csv header -a \"predicted,observed\" |\n> rush plot --x observed --y predicted --geom jitter > wine-regression.png\n \n$ display wine-regression.png$ vw -d wine-train.vw -f wine2.model --quiet ➊\n \n$ vw -data wine-test.vw -i wine2.model -t -p predictions --quiet ➋\n \n$ paste -d, predictions <(cut -d '|' -f 1 wine-test.vw) | ➌\n> awk -F, '{E+=sqrt(($1-$2)^2)} END {print \"MAE: \" E/NR}'\nMAE: 0.61905"},{"path":"chapter-9-modeling-data.html","id":"classification-with-scikit-learn-laboratory","chapter":"9 Modeling Data","heading":"9.5 Classification with SciKit-Learn Laboratory","text":"section ’m going train classification model, classifier, predicts whether wines either red white.\nuse vw , ’d like demonstrate another tool: SciKit-Learn Laboratory (SKLL).\nname implies, ’s built top SciKit-Learn, popular machine learning package Python.\nSKLL, Python package, provides run_experiment tool, makes possible use SciKit-Learn command line.\nInstead run_experiment, use alias skll find easier remember corresponds package name:","code":"$ alias skll=run_experiment\n \n$ skll\nusage: run_experiment [-h] [-a NUM_FEATURES] [-A] [-k] [-l] [-m MACHINES]\n                      [-q QUEUE] [-r] [-v] [--version]\n                      config_file [config_file ...]\nrun_experiment: error: the following arguments are required: config_file"},{"path":"chapter-9-modeling-data.html","id":"preparing-the-data-1","chapter":"9 Modeling Data","heading":"9.5.1 Preparing the Data","text":"skll expects training test dataset filenames, located separate directories.\npredictions necessarily order original dataset, add column, id, contains unique identifier can match predictions correct data points.\nLet’s create balanced dataset:➊ Store number red wines variable NUM_RED.\n➋ Combine red wines random sample white wines.\n➌ Add “line numbers” using nl front line.\n➍ Replace “0” first line “id” ’s proper column name.Let’s split balanced dataset training set test set:Now balanced training dataset balanced test dataset, can continue building classifier.","code":"$ NUM_RED=\"$(< wine-red-clean.csv wc -l)\" ➊\n \n$ csvstack -n type -g red,white \\ ➋\n> wine-red-clean.csv \\\n> <(< wine-white-clean.csv body shuf | head -n $NUM_RED) |\n> body shuf |\n> nl -s, -w1 -v0 | ➌\n> sed '1s/0,/id,/' | ➍\n> tee wine-balanced.csv | csvlook\n│    id │ type  │ fixed_acidity │ volatile_acidity │ citric_acid │ residual_sug…\n├───────┼───────┼───────────────┼──────────────────┼─────────────┼─────────────…\n│     1 │ white │          7.30 │            0.300 │        0.42 │           7.…\n│     2 │ white │          6.90 │            0.210 │        0.81 │           1.…\n│     3 │ red   │          7.80 │            0.760 │        0.04 │           2.…\n│     4 │ red   │          7.90 │            0.300 │        0.68 │           8.…\n│     5 │ red   │          8.80 │            0.470 │        0.49 │           2.…\n│     6 │ white │          6.40 │            0.150 │        0.29 │           1.…\n│     7 │ white │          7.80 │            0.210 │        0.34 │          11.…\n│     8 │ white │          7.00 │            0.130 │        0.37 │          12.…\n… with 3190 more lines$ mkdir -p {train,test}\n \n$ HEADER=\"$(< wine-balanced.csv header)\"\n\n$ < wine-balanced.csv header -d | shuf | split -d -n r/5 - wine-part-\n \n$ wc -l wine-part-*\n   640 wine-part-00\n   640 wine-part-01\n   640 wine-part-02\n   639 wine-part-03\n   639 wine-part-04\n  3198 total\n \n$ cat wine-part-00 | header -a $HEADER > test/features.csv && rm wine-part-00\n \n$ cat wine-part-* | header -a $HEADER > train/features.csv && rm wine-part-*\n \n$ wc -l t*/features.csv\n   641 test/features.csv\n  2559 train/features.csv\n  3200 total"},{"path":"chapter-9-modeling-data.html","id":"running-the-experiment","chapter":"9 Modeling Data","heading":"9.5.2 Running the Experiment","text":"Training classifier skll done defining experiment configuration file.\nconsists several sections specify, example, look datasets, classifiers\n’s configuration file classify.cfg ’ll use:run experiment using skll:option-l specifies run local mode.\nskll also offers possibility run experiments clusters.\ntime takes run experiment depends complexity chosen algorithms size data.","code":"$ bat classify.cfg\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: classify.cfg\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ [General]\n   2   │ experiment_name = wine\n   3   │ task = evaluate\n   4   │\n   5   │ [Input]\n   6   │ train_directory = train\n   7   │ test_directory = test\n   8   │ featuresets = [[\"features\"]]\n   9   │ feature_scaling = both\n  10   │ label_col = type\n  11   │ id_col = id\n  12   │ shuffle = true\n  13   │ learners = [\"KNeighborsClassifier\", \"LogisticRegression\", \"DecisionTree\n       │ Classifier\", \"RandomForestClassifier\"]\n  14   │ suffix = .csv\n  15   │\n  16   │ [Tuning]\n  17   │ grid_search = false\n  18   │ objectives = [\"neg_mean_squared_error\"]\n  19   │ param_grids = [{}, {}, {}, {}]\n  20   │\n  21   │ [Output]\n  22   │ logs = output\n  23   │ results = output\n  24   │ predictions = output\n  25   │ models = output\n───────┴────────────────────────────────────────────────────────────────────────$ skll -l classify.cfg 2>/dev/null"},{"path":"chapter-9-modeling-data.html","id":"parsing-the-results","chapter":"9 Modeling Data","heading":"9.5.3 Parsing the Results","text":"classifiers trained tested, results can found directory output:skll generates four files classifier: one log, two results, one predictions.\nextract algorithm names sort accuracies using following SQL query:relevant column accuracy, indicates percentage data points classified correctly.\nsee actually algorithms performing really well.\nRandomForestClassifier comes best performing algorithm, closely followed KNeighborsClassifier.JSON file contains confusion matrix, giving additional insight performance classifier.\nconfusion matrix table columns refer true labels (red white) rows refer predicted labels.\nHigher numbers diagonal mean correct predictions.\njq can print name classifier extract associated confusion matrix:confusion matrix especially helpful two classes, can see kind misclassifications happen, cost incorrect classification class.usage perspective, ’s interesting consider vw skll take two different approaches.\nvw uses command-line options, whereas skll requires separate file.\napproaches advantages disadvantages.\ncommand-line options enable ad-hoc usage, configuration file perhaps easier reproduce.\n, ’ve seen, invoking vw number options can easily placed script Makefile.\nopposite, making skll accept options doesn’t need configuration file, less straightforward.","code":"$ ls -1 output\nwine_features_DecisionTreeClassifier.log\nwine_features_DecisionTreeClassifier.model\nwine_features_DecisionTreeClassifier_predictions.tsv\nwine_features_DecisionTreeClassifier.results\nwine_features_DecisionTreeClassifier.results.json\nwine_features_KNeighborsClassifier.log\nwine_features_KNeighborsClassifier.model\nwine_features_KNeighborsClassifier_predictions.tsv\nwine_features_KNeighborsClassifier.results\nwine_features_KNeighborsClassifier.results.json\nwine_features_LogisticRegression.log\nwine_features_LogisticRegression.model\nwine_features_LogisticRegression_predictions.tsv\nwine_features_LogisticRegression.results\nwine_features_LogisticRegression.results.json\nwine_features_RandomForestClassifier.log\nwine_features_RandomForestClassifier.model\nwine_features_RandomForestClassifier_predictions.tsv\nwine_features_RandomForestClassifier.results\nwine_features_RandomForestClassifier.results.json\nwine.log\nwine_summary.tsv$ < output/wine_summary.tsv csvsql --query \"SELECT learner_name, accuracy FROM s\ntdin ORDER BY accuracy DESC\" | csvlook -I\n│ learner_name           │ accuracy  │\n├────────────────────────┼───────────┤\n│ RandomForestClassifier │ 0.9921875 │\n│ LogisticRegression     │ 0.990625  │\n│ KNeighborsClassifier   │ 0.9890625 │\n│ DecisionTreeClassifier │ 0.984375  │$ jq -r '.[] | \"\\(.learner_name):\\n\\(.result_table)\\n\"' output/*.json\nDecisionTreeClassifier:\n+-------+-------+---------+-------------+----------+-------------+\n|       |   red |   white |   Precision |   Recall |   F-measure |\n+=======+=======+=========+=============+==========+=============+\n|   red | [313] |       7 |       0.991 |    0.978 |       0.984 |\n+-------+-------+---------+-------------+----------+-------------+\n| white |     3 |   [317] |       0.978 |    0.991 |       0.984 |\n+-------+-------+---------+-------------+----------+-------------+\n(row = reference; column = predicted)\n \nKNeighborsClassifier:\n+-------+-------+---------+-------------+----------+-------------+\n|       |   red |   white |   Precision |   Recall |   F-measure |\n+=======+=======+=========+=============+==========+=============+\n|   red | [314] |       6 |       0.997 |    0.981 |       0.989 |\n+-------+-------+---------+-------------+----------+-------------+\n| white |     1 |   [319] |       0.982 |    0.997 |       0.989 |\n+-------+-------+---------+-------------+----------+-------------+\n(row = reference; column = predicted)\n \nLogisticRegression:\n+-------+-------+---------+-------------+----------+-------------+\n|       |   red |   white |   Precision |   Recall |   F-measure |\n+=======+=======+=========+=============+==========+=============+\n|   red | [315] |       5 |       0.997 |    0.984 |       0.991 |\n+-------+-------+---------+-------------+----------+-------------+\n| white |     1 |   [319] |       0.985 |    0.997 |       0.991 |\n+-------+-------+---------+-------------+----------+-------------+\n(row = reference; column = predicted)\n \nRandomForestClassifier:\n+-------+-------+---------+-------------+----------+-------------+\n|       |   red |   white |   Precision |   Recall |   F-measure |\n+=======+=======+=========+=============+==========+=============+\n|   red | [315] |       5 |       1.000 |    0.984 |       0.992 |\n+-------+-------+---------+-------------+----------+-------------+\n| white |     0 |   [320] |       0.985 |    1.000 |       0.992 |\n+-------+-------+---------+-------------+----------+-------------+\n(row = reference; column = predicted)\n "},{"path":"chapter-9-modeling-data.html","id":"summary-8","chapter":"9 Modeling Data","heading":"9.6 Summary","text":"chapter ’ve looked modeling data.\nexamples dived three different machine learning tasks namely dimensionality reduction unsupervised regression classification supervised.\nproper machine learning tutorial unfortunately beyond scope book.\nnext section couple recommendations case want learn machine learning.\nfourth last step OSEMN model data science ’m covering book.\nnext chapter last intermezzo chapter leveraging command line elsewhere.","code":""},{"path":"chapter-9-modeling-data.html","id":"for-further-exploration-8","chapter":"9 Modeling Data","heading":"9.7 For Further Exploration","text":"book Python Machine Learning Sebastian Raschka Vahid Mirjalili offers comprehensive overview machine learning apply using Python.later chapters R Everyone Jared Lander explain accomplish various machine learning tasks using R.want get deeper understanding machine learning, highly recommend pick Pattern Recognition Machine Learning Christopher Bishop Information Theory, Inference, Learning Algorithms David MacKay.’re interested learning t-SNE algorithm, recommend original article : Visualizing Data Using T-SNE Laurens van der Maaten Geoffrey Hinton.","code":""},{"path":"chapter-10-polyglot-data-science.html","id":"chapter-10-polyglot-data-science","chapter":"10 Polyglot Data Science","heading":"10 Polyglot Data Science","text":"polyglot someone speaks multiple languages.\npolyglot data scientist, see , someone uses multiple programming languages, tools, techniques obtain, scrub, explore, model data.command line stimulates polyglot approach.\ncommand line doesn’t care programming language tool written, long adhere Unix philosophy.\nsaw clearly Chapter 4, created command-line tools Bash, Python, R.\nMoreover, executed SQL queries directly CSV files executed R expressions command line.\nshort, already polyglot data science without fully realizing !chapter ’m going take flipping around.\n’m going show leverage command line various programming languages environments.\nlet’s honest, ’re going spend entire data science careers command line.\n, ’m analyzing data often use RStudio IDE ’m implementing something, often use Python.\nuse whatever helps get job done.find comforting know command line often within arm’s reach, without switch different application.\nallows quickly run command without switching separate application break workflow.\nExamples downloading files curl, inspecting piece data head, creating backup git, compiling website make.\nGenerally speaking, tasks normally require lot code simply done without command line.","code":""},{"path":"chapter-10-polyglot-data-science.html","id":"overview-7","chapter":"10 Polyglot Data Science","heading":"10.1 Overview","text":"chapter, ’ll learn :Run terminal within JupyterLab RStudio IDEInteract arbitrary command-line tools Python RTransform data using shell commands Apache SparkThis chapter starts following files:instructions get files Chapter 2.\nfiles either downloaded generated using command-line tools.","code":"$ cd /data/ch10\n \n$ l\ntotal 180K\ndrwxr-xr-x 2 dst dst 4.0K Dec 14 12:03 __pycache__/\n-rw-r--r-- 1 dst dst 164K Dec 14 12:03 alice.txt\n-rwxr--r-- 1 dst dst  408 Dec 14 12:03 count.py*\n-rw-r--r-- 1 dst dst  460 Dec 14 12:03 count.R\n-rw-r--r-- 1 dst dst 1.7K Dec 14 12:03 Untitled1337.ipynb"},{"path":"chapter-10-polyglot-data-science.html","id":"jupyter","chapter":"10 Polyglot Data Science","heading":"10.2 Jupyter","text":"Project Jupyter open-source project, born IPython Project 2014 evolved support interactive data science scientific computing across programming languages.\nJupyter supports 40 programming languages, including Python, R, Julia, Scala.\nsection ’ll focus Python.project includes JupyterLab, Jupyter Notebook, Jupyter Console.\n’ll start Jupyter Console, basic one work Python interactive way.\n’s Jupyter Console session illustrating couple ways leverage command line.➊ can run arbitrary shell commands pipelines date pip install Python package.\n➋ Compare line Pyton code count number lines alice.txt invocation wc .\n➌ Note standard output returned list strings, order use value total_lines, get first item cast integer.\n➍ Compare cell next download file invocation curl .\n➎ can use Python variables part shell command using curly braces.\n➏ want use literal curly braces, type twice.\n➐ Using Python variable standard input can done, gets quite tricky can see.Jupyter Notebook , essence, browser-based version Jupyter Console.\nsupports ways leverage command line, including exclamation mark bash magic.\nbiggest difference notebook contain code, also marked-text, equations, data visualizations.\n’s popular among data scientists reason.\nJupyter Notebook separate project environment, ’d like use JupyterLab work notebooks, offers complete IDE.Figure Figure 10.1 screenshot JupyterLab, showing file explorer (left), code editor (middle), notebook (right), terminal (bottom). latter three show ways leverage command line.\ncode something get back next section.\nparticular notebook quite similar console session just discussed.\nterminal offers complete shell run command line tools.\naware ’s interactivity possible terminal, code, notebook.\nterminal really different separate terminal application open, ’s still helpful ’re working inside Docker container remote server.\nFigure 10.1: JupyterLab file explorer, code editor, notebook, terminal\n","code":"$ jupyter console\nJupyter console 6.4.0\n \nPython 3.9.4 (default, Apr  4 2021, 19:38:44)\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.23.0 -- An enhanced Interactive Python. Type '?' for help.\n \nIn [1]: ! date ➊\nSun May  2 01:45:06 PM CEST 2021\n \nIn [2]: ! pip install --upgrade requests\nRequirement already satisfied: requests in /home/dst/.local/lib/python3.9/site-p\nackages (2.25.1)\nCollecting requests\n  Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)\n  Downloading requests-2.25.0-py2.py3-none-any.whl (61 kB)\n     |████████████████████████████████| 61 kB 2.1 MB/s\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/dst/.local/lib/pyt\nhon3.9/site-packages (from requests) (1.26.4)\nRequirement already satisfied: certifi>=2017.4.17 in /home/dst/.local/lib/python\n3.9/site-packages (from requests) (2020.12.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /usr/lib/python3/dist-packag\nes (from requests) (4.0.0)\nRequirement already satisfied: idna<3,>=2.5 in /home/dst/.local/lib/python3.9/si\nte-packages (from requests) (2.10)\n \nIn [3]: ! head alice.txt\n﻿Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\n \nThis eBook is for the use of anyone anywhere at no cost and with\nalmost no restrictions whatsoever.  You may copy it, give it away or\nre-use it under the terms of the Project Gutenberg License included\nwith this eBook or online at www.gutenberg.org\n \n \nTitle: Alice's Adventures in Wonderland\n \n \nIn [4]: len(open(\"alice.txt\").read().strip().split(\"\\n\")) ➋\nOut[4]: 3735\n \nIn [5]: total_lines = ! < alice.txt wc -l\n \nIn [6]: total_lines\nOut[6]: ['3735']\n \nIn [7]: int(total_lines[0]) ➌\nOut[7]: 3735\n \nIn [8]: url = \"https://www.gutenberg.org/files/11/old/11.txt\"\n \nIn [9]: import requests ➍\n \nIn [10]: with open(\"alice2.txt\", \"wb\") as f:\n    ...:     response = requests.get(url)\n    ...:     f.write(response.content)\n    ...: \n \nIn [11]: ! curl '{url}' > alice3.txt ➎\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  163k  100  163k    0     0   211k      0 --:--:-- --:--:-- --:--:--  211k\n \nIn [12]: ! ls alice*txt\nalice2.txt  alice3.txt  alice.txt\n \nIn [13]: ! rm -v alice{2,3}.txt ➏\nzsh:1: no matches found: alice(2, 3).txt\n \nIn [14]: ! rm -v alice{{2,3}}.txt\nremoved 'alice2.txt'\nremoved 'alice3.txt'\n \nIn [15]: lower = [\"foo\", \"bar\", \"baz\"]\n \nIn [16]: upper = ! echo '{\"\\n\".join(lower)}' | tr '[a-z]' '[A-Z]' ➐\n \nIn [17]: upper\nOut[17]: ['FOO', 'BAR', 'BAZ']\n \nIn [18]: exit\nShutting down kernel\n "},{"path":"chapter-10-polyglot-data-science.html","id":"python","chapter":"10 Polyglot Data Science","heading":"10.3 Python","text":"subprocess module allows run command-line tools Python connect standard input output.\nmodule recommended older os.system() function.\n’s run shell default, ’s possible change shell argument run() function.➊ recommended way leverage command line use run() function subprocess module.\n➋ Open file filename\n➌ Split entire text words\n➍ Run command-line tool grep, words passed standard input.\n➎ standard output available one long string. , split newline character count number occurrences pattern.command-line tools used follows:Notice first argument run call line 15 list strings, first item name command-line tool, remaining items arguments.\ndifferent passing single string.\nalso means don’t shell syntax available allow things redirection piping.","code":"$ bat count.py\n───────┬────────────────────────────────────────────────────────────────────────\n       │ File: count.py\n───────┼────────────────────────────────────────────────────────────────────────\n   1   │ #!/usr/bin/env python\n   2   │\n   3   │ from subprocess import run ➊\n   4   │ from sys import argv\n   5   │\n   6   │ if __name__ == \"__main__\":\n   7   │\n   8   │     _, filename, pattern = argv\n   9   │\n  10   │     with open(filename) as f: ➋\n  11   │         alice = f.read()\n  12   │\n  13   │     words = \"\\n\".join(alice.split()) ➌\n  14   │\n  15   │     grep = run([\"grep\", \"-i\", pattern], ➍\n  16   │                input = words,\n  17   │                capture_output=True,\n  18   │                text=True)\n  19   │\n  20   │     print(len(grep.stdout.strip().split(\"\\n\"))) ➎\n───────┴────────────────────────────────────────────────────────────────────────$ ./count.py alice.txt alice\n403"},{"path":"chapter-10-polyglot-data-science.html","id":"r","chapter":"10 Polyglot Data Science","heading":"10.4 R","text":"R, several ways leverage command line.example , start R session count number occurrences string alice book Alice’s Adventures Wonderland using system2() function.➊ Read file alice.txt\n➋ Split text words\n➌ Invoke command-line tool grep keep lines match string alice. character vector words passed standard input.\n➍ Count number elements character vector aliceA disadvantage system2() first writes character vector file passing standard input command-line tool.\ncan problematic dealing lot data lot invocations.’s better use named pipe, data written disk, much efficient.\ncan done pipe() fifo() functions.\nThanks Jim Hester suggesting .\ncode demonstrates :➊ function fifo() creates special first--first-file called . just reference pipe connection (like stdin stdout ). data actually written disk.\n➋ tool grep keep lines contain b write named pipe .\n➌ Write two values standard input shell command.\n➍ Read standard output produces grep character vector.\n➎ Clean connections delete special file.requires quite bit boilerplate code (creating connections, writing, reading, cleaning ), written helper function sh().\nUsing pipe operator (%>%) magrittr package chain together multiple shell commands.","code":"$ R --quiet\n> lines <- readLines(\"alice.txt\") ➊\n> head(lines)\n[1] \"Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\"\n[2] \"\"\n[3] \"This eBook is for the use of anyone anywhere at no cost and with\"\n[4] \"almost no restrictions whatsoever.  You may copy it, give it away or\"\n[5] \"re-use it under the terms of the Project Gutenberg License included\"\n[6] \"with this eBook or online at www.gutenberg.org\"\n> words <- unlist(strsplit(lines, \" \")) ➋\n> head(words)\n[1] \"Project\"     \"Gutenberg's\" \"Alice's\"     \"Adventures\"  \"in\"\n[6] \"Wonderland,\"\n> alice <- system2(\"grep\", c(\"-i\", \"alice\"), input = words, stdout = TRUE) ➌\n> head(alice)\n[1] \"Alice's\" \"Alice's\" \"ALICE'S\" \"ALICE'S\" \"Alice\"   \"Alice\"\n> length(alice) ➍> out_con <- fifo(\"out\", \"w+\") ➊\n> in_con <- pipe(\"grep b > out\") ➋\n> writeLines(c(\"foo\", \"bar\"), in_con) ➌\n> readLines(out_con) ➍\n[1] \"bar\"> library(magrittr)\n>\n> sh <- function(.data, command) {\n+   temp_file <- tempfile()\n+   out_con <- fifo(temp_file, \"w+\")\n+   in_con <- pipe(paste0(command, \" > \", temp_file))\n+   writeLines(as.character(.data), in_con)\n+   result <- readLines(out_con)\n+   close(out_con)\n+   close(in_con)\n+   unlink(temp_file)\n+   result\n+ }\n>\n> lines <- readLines(\"alice.txt\")\n> words <- unlist(strsplit(lines, \" \"))\n>\n> sh(words, \"grep -i alice\") %>%\n+   sh(\"wc -l\") %>%\n+   sh(\"cowsay\") %>%\n+   cli::cat_boxx()\n┌──────────────────────────────────┐\n│                                  │\n│    _____                         │\n│   < 403 >                        │\n│    -----                         │\n│           \\   ^__^               │\n│            \\  (oo)\\_______       │\n│               (__)\\       )\\/\\   │\n│                   ||----w |      │\n│                   ||     ||      │\n│                                  │\n└──────────────────────────────────┘\n>\n> q(\"no\")"},{"path":"chapter-10-polyglot-data-science.html","id":"rstudio","chapter":"10 Polyglot Data Science","heading":"10.5 RStudio","text":"RStudio IDE arguably popular environment working R.\nopen RStudio, first see console tab:\nFigure 10.2: RStudio IDE console tab open\nterminal tab right next console tab.\noffers complete shell:\nFigure 10.3: RStudio IDE terminal tab open\nNote , just JupyterLab, terminal connected console R scripts.","code":""},{"path":"chapter-10-polyglot-data-science.html","id":"apache-spark","chapter":"10 Polyglot Data Science","heading":"10.6 Apache Spark","text":"Apache Spark cluster-computing framework.\n’s 800-pound gorilla turn ’s impossible fit data memory.\nSpark written Scala, can also interact Python using PySpark R using SparkR sparklyr.Data processing machine learning pipelines defined series transformations one final action.\nOne transformation pipe() transformation, allows run entire dataset shell command Bash Perl script.\nitems dataset written standard input standard output returned RDD strings.session , start Spark shell count number occurrences alice book Alice’s Adventures Wonderland.➊ Read alice.txt line element.\n➋ Split element spaces. words, line split words.\n➌ Pipe partition grep keep elements match string alice.\n➍ Pipe partition wc count number elements.\n➎ ’s one count partition.\n➏ Sum counts get final count. Note elements first need converted strings integers.\n➐ steps combined single command.want use custom command-line tool pipeline, need make sure ’s present nodes cluster (known executors).\nOne way specify filename(s) --files option ’re submitting Spark applications using spark-submit.Matei Zaharia Bill Chambers (original author Apache Spark) mention book Spark: Definitive Guide “[t]pipe method probably one Spark’s interesting methods.”\n’s quite compliment!\nthink ’s fantastic developers Apache Spark added ability leverage 50-year old technology.","code":"$ spark-shell --master local[6]\nSpark context Web UI available at http://3d1bec8f2543:4040\nSpark context available as 'sc' (master = local[6], app id = local-16193763).\nSpark session available as 'spark'.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.1.1\n      /_/\n \nUsing Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 11.0.10)\nType in expressions to have them evaluated.\nType :help for more information.\n \nscala> val lines = sc.textFile(\"alice.txt\") ➊\nlines: org.apache.spark.rdd.RDD[String] = alice.txt MapPartitionsRDD[1] at textF\nile at <console>:24\n \nscala> lines.first()\nres0: String = Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Ca\nrroll\n \nscala> val words = lines.flatMap(line => line.split(\" \")) ➋\nwords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <con\nsole>:25\n \nscala> words.take(5)\nres1: Array[String] = Array(Project, Gutenberg's, Alice's, Adventures, in)\n \nscala> val alice = words.pipe(\"grep -i alice\") ➌\nalice: org.apache.spark.rdd.RDD[String] = PipedRDD[3] at pipe at <console>:25\n \nscala> alice.take(5)\nres2: Array[String] = Array(Alice's, Alice's, ALICE'S, ALICE'S, Alice)\n \nscala> val counts = alice.pipe(\"wc -l\") ➍\ncounts: org.apache.spark.rdd.RDD[String] = PipedRDD[4] at pipe at <console>:25\n \nscala> counts.collect()\nres3: Array[String] = Array(64, 72, 94, 93, 67, 13) ➎\n \nscala> counts.map(x => x.toInt).reduce(_ + _) ➏\nres4: Int = 403\n \nscala> sc.textFile(\"alice.txt\").flatMap(line => line.split(\" \")).pipe(\"grep -i a\nlice\").pipe(\"wc -l\").map(x => x.toInt).reduce(_ + _)\nres5: Int = 403 ➐\n "},{"path":"chapter-10-polyglot-data-science.html","id":"summary-9","chapter":"10 Polyglot Data Science","heading":"10.7 Summary","text":"chapter learned several ways leverage command line situations, including programming languages environments.\n’s important realize command line doesn’t exist vacuum.\nmatters use tools, sometimes combination, reliably get job done.Now ’ve four OSEMN chapters four intermezzo chapters, ’s time wrap conclude final chapter.","code":""},{"path":"chapter-10-polyglot-data-science.html","id":"for-further-exploration-9","chapter":"10 Polyglot Data Science","heading":"10.8 For Further Exploration","text":"also ways integrating two programming languages directly, without use command line. example reticulate package R allows interface Python directly.","code":""},{"path":"chapter-11-conclusion.html","id":"chapter-11-conclusion","chapter":"11 Conclusion","heading":"11 Conclusion","text":"final chapter, book comes close.\n’ll first recap ’ve discussed previous ten chapters, offer three pieces advice provide resources explore related topics touched upon.\nFinally, case questions, comments, new command-line tools share, provide ways get touch .","code":""},{"path":"chapter-11-conclusion.html","id":"lets-recap","chapter":"11 Conclusion","heading":"11.1 Let’s Recap","text":"book explored power using command line data science.\nfind interesting observation challenges posed relatively young field can tackled time-tested technology.\nhope now see command line capable .\nmany command-line tools offer sorts possibilities well suited variety tasks encompassing data science.many definitions data science available.\nChapter 1, introduced OSEMN model defined Mason Wiggins, practical one translates specific tasks.\nacronym OSEMN stands obtaining, scrubbing, exploring, modeling, interpreting data. Chapter 1 also explained command line suitable data science tasks.Chapter 2, explained can get tools used book. Chapter 2 also provided introduction essential tools concepts command line.four OSEMN model chapters focused performing practical tasks using command line.\nhaven’t devoted chapter fifth step, interpreting data, , quite frankly, computer, let alone command line, little use .\n, however, provided pointers reading topic.four intermezzo chapters, looked broader topics data science command line, topics really specific one particular step.\nChapter 4, explained can turn one-liners existing code reusable command-line tools.\nChapter 6, described can manage data workflow using tool called make.\nChapter 8, demonstrated ordinary command-line tools pipelines can run parallel using GNU Parallel.\nChapter 10, showed command line doesn’t exist vacuum can leveraged programming languages environments.\ntopics discussed intermezzo chapters can applied point data workflow.’s impossible demonstrate command-line tools available relevant data science.\nNew tools created daily basis.\nmay come understand now, book idea using command line, rather giving exhaustive list tools.","code":""},{"path":"chapter-11-conclusion.html","id":"three-pieces-of-advice","chapter":"11 Conclusion","heading":"11.2 Three Pieces of Advice","text":"probably spent quite time reading chapters perhaps also following along code examples.\nhope maximizes return investment increases probability ’ll continue incorporate command line data science workflow, like offer three pieces advice: (1) patient, (2) creative, (3) practical. next three subsections elaborate piece advice.","code":""},{"path":"chapter-11-conclusion.html","id":"be-patient","chapter":"11 Conclusion","heading":"11.2.1 Be Patient","text":"first piece advice can give patient.\nWorking data command line different using programming language, therefore requires different mindset.Moreover, command-line tools without quirks inconsistencies.\npartly developed many different people, course multiple decades.\never find loss regarding mind-dazzling options, don’t forget use --help, man, tldr, favorite search engine learn .Still, especially beginning, can frustrating experience.\nTrust , ’ll become proficient practice using command line tools.\ncommand line around many decades, around many come.\n’s worthwhile investment.","code":""},{"path":"chapter-11-conclusion.html","id":"be-creative","chapter":"11 Conclusion","heading":"11.2.2 Be Creative","text":"second, related piece advice creative.\ncommand line flexible.\ncombining command-line tools, can accomplish might think.encourage immediately fall back onto programming language.\nuse programming language, think whether code can generalized reused way.\n, consider creating command-line tool code using steps discussed Chapter 4.\nbelieve tool may beneficial others, even go one step making open source.\nMaybe ’s step know perform command line, rather leave comfort main programming language environment ’re working .\nPerhaps can use one approaches listed Chapter 10.","code":""},{"path":"chapter-11-conclusion.html","id":"be-practical","chapter":"11 Conclusion","heading":"11.2.3 Be Practical","text":"third piece advice practical.\npractical related creative, deserves separate explanation.\nprevious subsection, mentioned immediately fall back programming language.\ncourse, command line limits.\nThroughout book, emphasized command line regarded companion approach data science.’ve discussed four steps data science command line.\npractice, applicability command line higher step 1 step 4.\nuse whatever approach works best task hand.\n’s perfectly fine mix match approaches point workflow.\n’ve shown Chapter 10, command line wonderful integrated approaches, programming languages, statistical environments.\n’s certain trade-approach, part becoming proficient command line learn use .conclusion, ’re patient, creative, practical, command line make efficient productive data scientist.","code":""},{"path":"chapter-11-conclusion.html","id":"where-to-go-from-here","chapter":"11 Conclusion","heading":"11.3 Where To Go From Here?","text":"book intersection command line data science, many related topics touched upon.\nNow, ’s explore topics.\nfollowing subsections provide list topics suggested resources consult.","code":""},{"path":"chapter-11-conclusion.html","id":"the-command-line","chapter":"11 Conclusion","heading":"11.4 The Command Line","text":"Linux Command Line: Complete Introduction, 2nd Edition William E. Shotts, Jr. (Starch Press, 2019)Unix Power Tools, 3rd Edition Jerry Peek, Shelley Powers, Tim O’Reilly, Mike Loukides (O’Reilly Media, 2002)Learning Vi Vim Editors, 7th Edition Arnold Robbins, Elbert Hannah, Linda Lamb (O’Reilly Media, 2008)","code":""},{"path":"chapter-11-conclusion.html","id":"shell-programming","chapter":"11 Conclusion","heading":"11.4.1 Shell Programming","text":"Classic Shell Scripting Arnold Robbins Nelson H.F. Beebe (O’Reilly Media, 2005)Wicked Cool Shell Scripts, 2nd Edition Dave Taylor Brandon Perry (Starch Press, 2017)Bash Cookbook Carl Albing JP Vossen (O’Reilly Media, 2018)","code":""},{"path":"chapter-11-conclusion.html","id":"python-r-and-sql","chapter":"11 Conclusion","heading":"11.4.2 Python, R, and SQL","text":"Learn Python 3 Hard Way Zed . Shaw (Addison-Wesley Professional, 2017)Python Data Analysis, 2nd Edition Wes McKinney (O’Reilly Media, 2017)Data Science Scratch, 2nd Edition Joel Grus (O’Reilly Media, 2019)R Data Science Garrett Grolemund Hadley Wickham (O’Reilly Media, 2016)R Everyone, 2nd edition Jared Lander (Addison-Wesley Professional, 2017)Sams Teach SQL 10 Minutes Day, 5th Edition Ben Forta (Sams, 2020)","code":""},{"path":"chapter-11-conclusion.html","id":"apis","chapter":"11 Conclusion","heading":"11.4.3 APIs","text":"Mining Social Web, 3rd Edition Matthew . Russell Mikhail Klassen (O’Reilly Media, 2019)Data Source Handbook Pete Warden (O’Reilly Media, 2011)","code":""},{"path":"chapter-11-conclusion.html","id":"machine-learning","chapter":"11 Conclusion","heading":"11.4.4 Machine Learning","text":"Python Machine Learning, 3rd Edition Sebastian Raschka Vahid Mirjalili (Packt Publishing, 2019)Pattern Recognition Machine Learning Christopher M. Bishop (Springer, 2006)Information Theory, Inference, Learning Algorithms David MacKay (Cambridge University Press, 2003)","code":""},{"path":"chapter-11-conclusion.html","id":"getting-in-touch","chapter":"11 Conclusion","heading":"11.5 Getting in Touch","text":"book possible without many people created command line numerous tools.\n’s safe say current ecosystem command-line tools data science community effort.\nable give glimpse many command-line tools available.\nNew ones created everyday, perhaps day create one .\ncase, love hear .\n’d also appreciate drop line whenever question, comment, suggestion.\ncouple ways get touch:Email: jeroen@jeroenjanssens.comTwitter: @jeroenhjanssensBook website: https://datascienceatthecommandline.com/Book GitHub repository: https://github.com/jeroenjanssens/data-science---command-lineThank .","code":""},{"path":"list-of-command-line-tools.html","id":"list-of-command-line-tools","chapter":"List of Command-Line Tools","heading":"List of Command-Line Tools","text":"overview command-line tools discussed book.\nincludes binary executables, interpreted scripts, Z Shell builtins keywords.\ncommand-line tool, following information, available appropriate, provided:actual command type command lineA descriptionThe version used bookThe year version releasedThe primary author(s)website find informationHow obtain helpAn example usageAll command-line tools listed included Docker image.\nSee Chapter 2 instructions set .\nPlease note citing open source software trivial, information may missing incorrect.","code":""},{"path":"list-of-command-line-tools.html","id":"alias","chapter":"List of Command-Line Tools","heading":"alias","text":"Define display aliases.\nalias\nZ shell builtin.","code":"$ type alias\nalias is a shell builtin\n \n$ man zshbuiltins | grep -A 10 alias\n \n$ alias l\nl='ls --color -lhF --group-directories-first'\n \n$ alias python=python3"},{"path":"list-of-command-line-tools.html","id":"awk","chapter":"List of Command-Line Tools","heading":"awk","text":"Pattern scanning text processing language.\nawk\n(version 1.3.4)\nMike D. Brennan Thomas E. Dickey (2019).\ninformation: https://invisible-island.net/mawk.","code":"$ type awk\nawk is /usr/bin/awk\n \n$ man awk\n \n$ seq 5 | awk '{sum+=$1} END {print sum}'\n15"},{"path":"list-of-command-line-tools.html","id":"aws","chapter":"List of Command-Line Tools","heading":"aws","text":"Unified tool manage AWS services.\naws\n(version 2.1.32)\nAmazon Web Services (2021).\ninformation: https://aws.amazon.com/cli.","code":"$ type aws\naws is /usr/local/bin/aws\n \n$ aws --help"},{"path":"list-of-command-line-tools.html","id":"bash","chapter":"List of Command-Line Tools","heading":"bash","text":"GNU Bourne-SHell.\nbash\n(version 5.0.17)\nBrian Fox Chet Ramey (2019).\ninformation: https://www.gnu.org/software/bash.","code":"$ type bash\nbash is /usr/bin/bash\n \n$ man bash"},{"path":"list-of-command-line-tools.html","id":"bat","chapter":"List of Command-Line Tools","heading":"bat","text":"cat clone syntax highlighting Git integration.\nbat\n(version 0.18.0)\nDavid Peter (2021).\ninformation: https://github.com/sharkdp/bat.","code":"$ type bat\nbat is an alias for bat --tabs 8 --paging never\n \n$ man bat"},{"path":"list-of-command-line-tools.html","id":"bc","chapter":"List of Command-Line Tools","heading":"bc","text":"arbitrary precision calculator language.\nbc\n(version 1.07.1)\nPhilip . Nelson (2017).\ninformation: https://www.gnu.org/software/bc.","code":"$ type bc\nbc is /usr/bin/bc\n \n$ man bc\n \n$ bc -l <<< 'e(1)'\n2.71828182845904523536"},{"path":"list-of-command-line-tools.html","id":"body","chapter":"List of Command-Line Tools","heading":"body","text":"Apply command first line.\nbody\n(version 0.1)\nJeroen Janssens (2021).\ninformation: https://github.com/jeroenjanssens/dsutils.","code":"$ type body\nbody is /usr/bin/dsutils/body\n \n$ seq 10 | header -a 'values' | body shuf\nvalues\n3\n7\n10\n9\n5\n4\n6\n8\n2\n1"},{"path":"list-of-command-line-tools.html","id":"cat","chapter":"List of Command-Line Tools","heading":"cat","text":"Concatenate files print standard output.\ncat\n(version 8.30)\nTorbjorn Granlund Richard M. Stallman (2018).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type cat\ncat is /usr/bin/cat\n \n$ man cat\n \n$ cat *.log > all.log"},{"path":"list-of-command-line-tools.html","id":"cd","chapter":"List of Command-Line Tools","heading":"cd","text":"Change shell working directory.\ncd\nZ shell builtin.","code":"$ type cd\ncd is a shell builtin\n \n$ man zshbuiltins | grep -A 10 cd\n \n$ cd ~\n \n$ pwd\n/home/dst\n \n$ cd ..\n \n$ pwd\n/home\n \n$ cd /data/ch01\ncd: no such file or directory: /data/ch01"},{"path":"list-of-command-line-tools.html","id":"chmod","chapter":"List of Command-Line Tools","heading":"chmod","text":"Change file mode bits.\nchmod\n(version 8.30)\nDavid MacKenzie Jim Meyering (2018).\nuse chmod Chapter 4 make tool executable.\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type chmod\nchmod is /usr/bin/chmod\n \n$ man chmod\n \n$ chmod u+x script.sh"},{"path":"list-of-command-line-tools.html","id":"cols","chapter":"List of Command-Line Tools","heading":"cols","text":"Apply command subset columns.\ncols\n(version 0.1)\nJeroen Janssens (2021).\ninformation: https://github.com/jeroenjanssens/dsutils.","code":"$ type cols\ncols is /usr/bin/dsutils/cols"},{"path":"list-of-command-line-tools.html","id":"column","chapter":"List of Command-Line Tools","heading":"column","text":"Columnate lists.\ncolumn\n(version 2.36.1)\nKarel Zak (2021).\ninformation: https://www.kernel.org/pub/linux/utils/util-linux.","code":"$ type column\ncolumn is /usr/bin/column"},{"path":"list-of-command-line-tools.html","id":"cowsay","chapter":"List of Command-Line Tools","heading":"cowsay","text":"Configurable speaking cow.\ncowsay\n(version 3.0.3)\nTony Monroe (1999).\ninformation: https://github.com/tnalpgge/rank-amateur-cowsay.","code":"$ type cowsay\ncowsay is /usr/bin/cowsay\n \n$ man cowsay\n \n$ echo 'The command line is awesome!' | cowsay\n ______________________________\n< The command line is awesome! >\n ------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||"},{"path":"list-of-command-line-tools.html","id":"cp","chapter":"List of Command-Line Tools","heading":"cp","text":"Copy files directories.\ncp\n(version 8.30)\nTorbjorn Granlund, David MacKenzie, Jim Meyering (2018).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type cp\ncp is /usr/bin/cp\n \n$ man cp\n \n$ cp -r ~/Downloads/*.xlsx /data"},{"path":"list-of-command-line-tools.html","id":"csv2vw","chapter":"List of Command-Line Tools","heading":"csv2vw","text":"Convert CSV Vowpal Wabbit format.\ncsv2vw\n(version 0.1)\nJeroen Janssens (2021).\ninformation: https://github.com/jeroenjanssens/dsutils.","code":"$ type csv2vw\ncsv2vw is /usr/bin/dsutils/csv2vw"},{"path":"list-of-command-line-tools.html","id":"csvcut","chapter":"List of Command-Line Tools","heading":"csvcut","text":"Filter truncate CSV files.\ncsvcut\n(version 1.0.5)\nChristopher Groskopf (2020).\ninformation: https://csvkit.rtfd.org.","code":"$ type csvcut\ncsvcut is /usr/bin/csvcut\n \n$ csvcut --help\n \n$ csvcut -c bill,tip /data/ch05/tips.csv | trim\nbill,tip\n16.99,1.01\n10.34,1.66\n21.01,3.5\n23.68,3.31\n24.59,3.61\n25.29,4.71\n8.77,2.0\n26.88,3.12\n15.04,1.96\n… with 235 more lines"},{"path":"list-of-command-line-tools.html","id":"csvgrep","chapter":"List of Command-Line Tools","heading":"csvgrep","text":"Search CSV files.\ncsvgrep\n(version 1.0.5)\nChristopher Groskopf (2020).\ninformation: https://csvkit.rtfd.org.","code":"$ type csvgrep\ncsvgrep is /usr/bin/csvgrep\n \n$ csvgrep --help"},{"path":"list-of-command-line-tools.html","id":"csvjoin","chapter":"List of Command-Line Tools","heading":"csvjoin","text":"Execute SQL-like join merge CSV files specified column columns.\ncsvjoin\n(version 1.0.5)\nChristopher Groskopf (2020).\ninformation: https://csvkit.rtfd.org.","code":"$ type csvjoin\ncsvjoin is /usr/bin/csvjoin\n \n$ csvjoin --help"},{"path":"list-of-command-line-tools.html","id":"csvlook","chapter":"List of Command-Line Tools","heading":"csvlook","text":"Render CSV file console Markdown-compatible, fixed-width table.\ncsvlook\n(version 1.0.5)\nChristopher Groskopf (2020).\ninformation: https://csvkit.rtfd.org.","code":"$ type csvlook\ncsvlook is a shell function\n \n$ csvlook --help\n \n$ csvlook /data/ch05/tips.csv\n│  bill │   tip │ sex    │ smoker │ day  │ time   │ size │\n├───────┼───────┼────────┼────────┼──────┼────────┼──────┤\n│ 16.99 │  1.01 │ Female │  False │ Sun  │ Dinner │    2 │\n│ 10.34 │  1.66 │ Male   │  False │ Sun  │ Dinner │    3 │\n│ 21.01 │  3.50 │ Male   │  False │ Sun  │ Dinner │    3 │\n│ 23.68 │  3.31 │ Male   │  False │ Sun  │ Dinner │    2 │\n│ 24.59 │  3.61 │ Female │  False │ Sun  │ Dinner │    4 │\n│ 25.29 │  4.71 │ Male   │  False │ Sun  │ Dinner │    4 │\n│  8.77 │  2.00 │ Male   │  False │ Sun  │ Dinner │    2 │\n│ 26.88 │  3.12 │ Male   │  False │ Sun  │ Dinner │    4 │\n… with 236 more lines"},{"path":"list-of-command-line-tools.html","id":"csvquote","chapter":"List of Command-Line Tools","heading":"csvquote","text":"Enable common unix utlities work correctly CSV data.\ncsvquote\n(version 0.1)\nDan Brown (2018).\ninformation: https://github.com/dbro/csvquote.","code":"$ type csvquote\ncsvquote is /usr/local/bin/csvquote"},{"path":"list-of-command-line-tools.html","id":"csvsort","chapter":"List of Command-Line Tools","heading":"csvsort","text":"Sort CSV files.\ncsvsort\n(version 1.0.5)\nChristopher Groskopf (2020).\ninformation: https://csvkit.rtfd.org.","code":"$ type csvsort\ncsvsort is /usr/bin/csvsort\n \n$ csvsort --help"},{"path":"list-of-command-line-tools.html","id":"csvsql","chapter":"List of Command-Line Tools","heading":"csvsql","text":"Execute SQL statements CSV files.\ncsvsql\n(version 1.0.5)\nChristopher Groskopf (2020).\ninformation: https://csvkit.rtfd.org.","code":"$ type csvsql\ncsvsql is /usr/bin/csvsql\n \n$ csvsql --help"},{"path":"list-of-command-line-tools.html","id":"csvstack","chapter":"List of Command-Line Tools","heading":"csvstack","text":"Stack rows multiple CSV files.\ncsvstack\n(version 1.0.5)\nChristopher Groskopf (2020).\ninformation: https://csvkit.rtfd.org.","code":"$ type csvstack\ncsvstack is /usr/bin/csvstack\n \n$ csvstack --help"},{"path":"list-of-command-line-tools.html","id":"csvstat","chapter":"List of Command-Line Tools","heading":"csvstat","text":"Print descriptive statistics column CSV file.\ncsvstat\n(version 1.0.5)\nChristopher Groskopf (2020).\ninformation: https://csvkit.rtfd.org.","code":"$ type csvstat\ncsvstat is /usr/bin/csvstat\n \n$ csvstat --help"},{"path":"list-of-command-line-tools.html","id":"curl","chapter":"List of Command-Line Tools","heading":"curl","text":"Transfer URL.\ncurl\n(version 7.68.0)\nDaniel Stenberg (2016).\ninformation: https://curl.haxx.se.","code":"$ type curl\ncurl is /usr/bin/curl\n \n$ man curl"},{"path":"list-of-command-line-tools.html","id":"cut","chapter":"List of Command-Line Tools","heading":"cut","text":"Remove sections line files.\ncut\n(version 8.30)\nDavid M. Ihnat, David MacKenzie, Jim Meyering (2019).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type cut\ncut is /usr/bin/cut\n \n$ man cut"},{"path":"list-of-command-line-tools.html","id":"display","chapter":"List of Command-Line Tools","heading":"display","text":"Displays image image sequence X server.\ndisplay\n(version 6.9.10-23)\nImageMagick Studio LLC (2019).\ninformation: https://imagemagick.org.","code":"$ type display\ndisplay is a shell function"},{"path":"list-of-command-line-tools.html","id":"dseq","chapter":"List of Command-Line Tools","heading":"dseq","text":"Generate sequence dates.\ndseq\n(version 0.1)\nJeroen Janssens (2021).\ninformation: https://github.com/jeroenjanssens/dsutils.","code":"$ type dseq\ndseq is /usr/bin/dsutils/dseq\n \n$ dseq 3\n2021-12-15\n2021-12-16\n2021-12-17"},{"path":"list-of-command-line-tools.html","id":"echo","chapter":"List of Command-Line Tools","heading":"echo","text":"Display line text.\necho\n(version 8.30)\nBrian Fox Chet Ramey (2019).\nUseful using literal text standard input next tool.\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type echo\necho is a shell builtin\n \n$ man echo\n \n$ echo Hippopotomonstrosesquippedaliophobia | cowsay\n ______________________________________\n< Hippopotomonstrosesquippedaliophobia >\n --------------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n \n$ echo -n Hippopotomonstrosesquippedaliophobia | wc -c\n36"},{"path":"list-of-command-line-tools.html","id":"env","chapter":"List of Command-Line Tools","heading":"env","text":"Run program modified environment.\nenv\n(version 8.30)\nRichard Mlynarik, David MacKenzie, Assaf Gordon (2018).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type env\nenv is /usr/bin/env\n \n$ man env"},{"path":"list-of-command-line-tools.html","id":"export","chapter":"List of Command-Line Tools","heading":"export","text":"Set export attribute shell variables. Useful making shell variables available command-line tools..\nexport\nZ shell builtin.","code":"$ type export\nexport is a reserved word\n \n$ man zshbuiltins | grep -A 10 export\n \n$ export PATH=\"$PATH:$HOME/bin\""},{"path":"list-of-command-line-tools.html","id":"fc","chapter":"List of Command-Line Tools","heading":"fc","text":"Control interactive history mechanism.\nfc\nZ shell builtin.\nuse fc Chapter 4 edit command nano.","code":"$ type fc\nfc is a shell builtin\n \n$ man zshbuiltins | grep -A 10 '^ *fc '"},{"path":"list-of-command-line-tools.html","id":"find","chapter":"List of Command-Line Tools","heading":"find","text":"Search files directory hierarchy.\nfind\n(version 4.7.0)\nEric B. Decker, James Youngman, Kevin Dalley (2019).\ninformation: https://www.gnu.org/software/findutils.","code":"$ type find\nfind is /usr/bin/find\n \n$ man find\n \n$ find /data -type f -name '*.csv' -size -3\n/data/ch03/tmnt-basic.csv\n/data/ch03/tmnt-missing-newline.csv\n/data/ch03/tmnt-with-header.csv\n/data/ch05/irismeta.csv\n/data/ch05/names-comma.csv\n/data/ch05/names.csv\n/data/ch07/datatypes.csv"},{"path":"list-of-command-line-tools.html","id":"fold","chapter":"List of Command-Line Tools","heading":"fold","text":"Wrap input line fit specified width.\nfold\n(version 8.30)\nDavid MacKenzie (2020).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type fold\nfold is /usr/bin/fold\n \n$ man fold"},{"path":"list-of-command-line-tools.html","id":"for","chapter":"List of Command-Line Tools","heading":"for","text":"Execute commands member list.\n\nZ shell builtin.\nChapter 8, discuss advantages using parallel instead .","code":"$ type for\nfor is a reserved word\n \n$ man zshmisc | grep -EA 10 '^ *for '\n \n$ for i in {A..C} \"It's easy as\" {1..3}; do echo $i; done\nA\nB\nC\nIt's easy as\n1\n2\n3"},{"path":"list-of-command-line-tools.html","id":"fx","chapter":"List of Command-Line Tools","heading":"fx","text":"Interactive JSON viewer.\nfx\n(version 20.0.2)\nAnton Medvedev (2020).\ninformation: https://github.com/antonmedv/fx.","code":"$ type fx\nfx is /usr/local/bin/fx\n \n$ fx --help\n \n$ echo '[1,2,3]' | fx 'this.map(x => x * 2)'\n[\n  2,\n  4,\n  6\n]"},{"path":"list-of-command-line-tools.html","id":"git","chapter":"List of Command-Line Tools","heading":"git","text":"stupid content tracker.\ngit\n(version 2.25.1)\nLinus Torvalds Junio C. Hamano (2021).\ninformation: https://git-scm.com.","code":"$ type git\ngit is /usr/bin/git\n \n$ man git"},{"path":"list-of-command-line-tools.html","id":"grep","chapter":"List of Command-Line Tools","heading":"grep","text":"Print lines match patterns.\ngrep\n(version 3.4)\nJim Meyering (2019).\ninformation: https://www.gnu.org/software/grep.","code":"$ type grep\ngrep is /usr/bin/grep\n \n$ man grep\n \n$ seq 100 | grep 3 | wc -l\n19"},{"path":"list-of-command-line-tools.html","id":"gron","chapter":"List of Command-Line Tools","heading":"gron","text":"Make JSON greppable.\ngron\n(version 0.6.1)\nTom Hudson (2021).\ninformation: https://github.com/TomNomNom/gron.","code":"$ type gron\ngron is /usr/bin/gron\n \n$ man gron"},{"path":"list-of-command-line-tools.html","id":"head","chapter":"List of Command-Line Tools","heading":"head","text":"Output first part files.\nhead\n(version 8.30)\nDavid MacKenzie Jim Meyering (2019).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type head\nhead is /usr/bin/head\n \n$ man head\n \n$ seq 100 | head -n 5\n1\n2\n3\n4\n5"},{"path":"list-of-command-line-tools.html","id":"header","chapter":"List of Command-Line Tools","heading":"header","text":"Add, replace, delete header lines.\nheader\n(version 0.1)\nJeroen Janssens (2021).\ninformation: https://github.com/jeroenjanssens/dsutils.","code":"$ type header\nheader is /usr/bin/dsutils/header"},{"path":"list-of-command-line-tools.html","id":"history","chapter":"List of Command-Line Tools","heading":"history","text":"GNU History Library.\nhistory\n(version 8.1)\nBrian Fox Chet Ramey (2020).\ninformation: https://www.gnu.org/software/bash.","code":"$ type history\nhistory is a shell builtin"},{"path":"list-of-command-line-tools.html","id":"hostname","chapter":"List of Command-Line Tools","heading":"hostname","text":"Show set system’s host name.\nhostname\n(version 3.23)\nPeter Tobias, Bernd Eckenfels, Michael Meskes (2021).\ninformation: https://sourceforge.net/projects/net-tools/.","code":"$ type hostname\nhostname is /usr/bin/hostname\n \n$ man hostname\n \n$ hostname\n2c3edb64827b\n \n$ hostname -i\n172.17.0.2"},{"path":"list-of-command-line-tools.html","id":"in2csv","chapter":"List of Command-Line Tools","heading":"in2csv","text":"Convert common, less awesome, tabular data formats CSV.\nin2csv\n(version 1.0.5)\nChristopher Groskopf (2020).\ninformation: https://csvkit.rtfd.org.","code":"$ type in2csv\nin2csv is /usr/bin/in2csv\n \n$ in2csv --help"},{"path":"list-of-command-line-tools.html","id":"jq","chapter":"List of Command-Line Tools","heading":"jq","text":"Command-line JSON processor.\njq\n(version 1.6)\nStephen Dolan (2021).\ninformation: https://stedolan.github.com/jq.","code":"$ type jq\njq is /usr/bin/jq\n \n$ man jq"},{"path":"list-of-command-line-tools.html","id":"json2csv","chapter":"List of Command-Line Tools","heading":"json2csv","text":"Convert JSON CSV.\njson2csv\n(version 1.2.1)\nJehiah Czebotar (2019).\ninformation: https://github.com/jehiah/json2csv.","code":"$ type json2csv\njson2csv is /usr/bin/json2csv\n \n$ json2csv --help"},{"path":"list-of-command-line-tools.html","id":"l","chapter":"List of Command-Line Tools","heading":"l","text":"List directory contents long format directories grouped files, human readable file sizes, access permissions.\nl\nUnknown (1999).","code":"$ type l\nl is an alias for ls --color -lhF --group-directories-first\n \n$ cd /data/ch03\n \n$ ls\nlogs.tar.gz    tmnt-basic.csv            tmnt-with-header.csv\nr-datasets.db  tmnt-missing-newline.csv  top2000.xlsx\n \n$ l\ntotal 924K\n-rw-r--r-- 1 dst dst 627K Dec 14 12:03 logs.tar.gz\n-rw-r--r-- 1 dst dst 189K Dec 14 12:03 r-datasets.db\n-rw-r--r-- 1 dst dst  149 Dec 14 12:03 tmnt-basic.csv\n-rw-r--r-- 1 dst dst  148 Dec 14 12:03 tmnt-missing-newline.csv\n-rw-r--r-- 1 dst dst  181 Dec 14 12:03 tmnt-with-header.csv\n-rw-r--r-- 1 dst dst  91K Dec 14 12:03 top2000.xlsx"},{"path":"list-of-command-line-tools.html","id":"less","chapter":"List of Command-Line Tools","heading":"less","text":"opposite .\nless\n(version 551)\nMark Nudelman (2019).\ninformation: https://www.greenwoodsoftware.com/less.","code":"$ type less\nless is an alias for less -R\n \n$ man less\n \n$ less README"},{"path":"list-of-command-line-tools.html","id":"ls","chapter":"List of Command-Line Tools","heading":"ls","text":"List directory contents.\nls\n(version 8.30)\nRichard M. Stallman David MacKenzie (2019).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type ls\nls is /usr/bin/ls\n \n$ man ls\n \n$ ls /data\nch02  ch03  ch04  ch05  ch06  ch07  ch08  ch09  ch10  csvconf"},{"path":"list-of-command-line-tools.html","id":"make","chapter":"List of Command-Line Tools","heading":"make","text":"Program Maintaining Computer Programs.\nmake\n(version 4.3)\nStuart . Feldman (2020).\ninformation: https://www.gnu.org/software/make.","code":"$ type make\nmake is /usr/bin/make\n \n$ man make\n \n$ make sandwich"},{"path":"list-of-command-line-tools.html","id":"man","chapter":"List of Command-Line Tools","heading":"man","text":"interface system reference manuals.\nman\n(version 2.9.1)\nJohn W. Eaton Colin Watson (2020).\ninformation: https://nongnu.org/man-db.","code":"$ type man\nman is /usr/bin/man\n \n$ man man\n \n$ man excel\nNo manual entry for excel"},{"path":"list-of-command-line-tools.html","id":"mkdir","chapter":"List of Command-Line Tools","heading":"mkdir","text":"Make directories.\nmkdir\n(version 8.30)\nDavid MacKenzie (2019).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type mkdir\nmkdir is /usr/bin/mkdir\n \n$ man mkdir\n \n$ mkdir -p /data/ch{01..10}"},{"path":"list-of-command-line-tools.html","id":"mv","chapter":"List of Command-Line Tools","heading":"mv","text":"Move (rename) files.\nmv\n(version 8.30)\nMike Parker, David MacKenzie, Jim Meyering (2020).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type mv\nmv is /usr/bin/mv\n \n$ man mv\n \n$ mv results{,.bak}"},{"path":"list-of-command-line-tools.html","id":"nano","chapter":"List of Command-Line Tools","heading":"nano","text":"Nano’s ANOther editor, inspired Pico.\nnano\n(version 5.4)\nBenno Schulenberg, David Lawrence Ramsey, Jordi Mallach, Chris Allegretta, Robert Siemborski, Adam Rogoyski (2020).\ninformation: https://nano-editor.org.","code":"$ type nano\nnano is /usr/bin/nano"},{"path":"list-of-command-line-tools.html","id":"nl","chapter":"List of Command-Line Tools","heading":"nl","text":"Number lines files.\nnl\n(version 8.30)\nScott Bartram David MacKenzie (2020).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type nl\nnl is /usr/bin/nl\n \n$ man nl\n \n$ nl /data/ch05/alice.txt | head\n     1  ﻿Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\n     2\n     3  This eBook is for the use of anyone anywhere at no cost and with\n     4  almost no restrictions whatsoever.  You may copy it, give it away or\n     5  re-use it under the terms of the Project Gutenberg License included\n     6  with this eBook or online at www.gutenberg.org\n     7\n     8\n     9  Title: Alice's Adventures in Wonderland\n    10"},{"path":"list-of-command-line-tools.html","id":"parallel","chapter":"List of Command-Line Tools","heading":"parallel","text":"Build execute shell command lines standard input parallel.\nparallel\n(version 20161222)\nOle Tange (2016).\ninformation: https://www.gnu.org/software/parallel.","code":"$ type parallel\nparallel is /usr/bin/parallel\n \n$ man parallel\n \n$ seq 3 | parallel \"echo Processing file {}.csv\"\nProcessing file 1.csv\nProcessing file 2.csv\nProcessing file 3.csv"},{"path":"list-of-command-line-tools.html","id":"paste","chapter":"List of Command-Line Tools","heading":"paste","text":"Merge lines files.\npaste\n(version 8.30)\nDavid M. Ihnat David MacKenzie (2019).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type paste\npaste is /usr/bin/paste\n \n$ man paste\n \n$ paste -d, <(seq 5) <(dseq 5)\n1,2021-12-15\n2,2021-12-16\n3,2021-12-17\n4,2021-12-18\n5,2021-12-19\n \n$ seq 5 | paste -sd+\n1+2+3+4+5"},{"path":"list-of-command-line-tools.html","id":"pbc","chapter":"List of Command-Line Tools","heading":"pbc","text":"Parallel bc.\npbc\nJeroen Janssens (2021).\ninformation: https://github.com/jeroenjanssens/dsutils.","code":"$ type pbc\npbc is /usr/bin/dsutils/pbc\n \n$ seq 3 | pbc '{1}^2'\n1\n4\n9"},{"path":"list-of-command-line-tools.html","id":"pip","chapter":"List of Command-Line Tools","heading":"pip","text":"tool installing managing Python packages.\npip\n(version 20.0.2)\nPyPA (2020).\ninformation: https://pip.pypa.io.","code":"$ type pip\npip is /usr/bin/pip\n \n$ man pip\n \n$ pip install pandas\n \n$ pip freeze | grep sci\nscikit-learn==0.24.2\nscipy==1.7.0"},{"path":"list-of-command-line-tools.html","id":"pup","chapter":"List of Command-Line Tools","heading":"pup","text":"Parsing HTML command line.\npup\n(version 0.4.0)\nEric Chiang (2016).\ninformation: https://github.com/EricChiang/pup.","code":"$ type pup\npup is /usr/bin/pup\n \n$ pup --help"},{"path":"list-of-command-line-tools.html","id":"pwd","chapter":"List of Command-Line Tools","heading":"pwd","text":"Print name working directory.\npwd\n(version 8.30)\nJim Meyering (2019).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type pwd\npwd is a shell builtin\n \n$ man pwd\n \n$ cd ~\n \n$ pwd\n/home/dst"},{"path":"list-of-command-line-tools.html","id":"python-1","chapter":"List of Command-Line Tools","heading":"python","text":"interpreted, interactive, object-oriented programming language.\npython\n(version 3.8.5)\nPython Software Foundation (2021).\ninformation: https://www.python.org.","code":"$ type python\npython is an alias for python3\n \n$ man python"},{"path":"list-of-command-line-tools.html","id":"r-1","chapter":"List of Command-Line Tools","heading":"R","text":"Language Environment Statistical Computing.\nR\n(version 4.0.4)\nR Foundation Statistical Computing (2021).\ninformation: https://www.r-project.org.","code":"$ type R\nR is /usr/bin/R\n \n$ man R"},{"path":"list-of-command-line-tools.html","id":"rev","chapter":"List of Command-Line Tools","heading":"rev","text":"Reverse lines characterwise.\nrev\n(version 2.36.1)\nKarel Zak (2021).\ninformation: https://www.kernel.org/pub/linux/utils/util-linux.","code":"$ type rev\nrev is /usr/bin/rev\n \n$ echo 'Satire: Veritas' | rev\nsatireV :eritaS\n \n$ echo 'Ça va?' | rev | cut -c 2- | rev\nÇa va"},{"path":"list-of-command-line-tools.html","id":"rm","chapter":"List of Command-Line Tools","heading":"rm","text":"Remove files directories.\nrm\n(version 8.30)\nPaul Rubin, David MacKenzie, Richard M. Stallman, Jim Meyering (2019).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type rm\nrm is /usr/bin/rm\n \n$ man rm\n \n$ rm *.old"},{"path":"list-of-command-line-tools.html","id":"rush","chapter":"List of Command-Line Tools","heading":"rush","text":"R One-Liners Shell.\nrush\n(version 0.1)\nJeroen Janssens (2021).\ninformation: https://github.com/jeroenjanssens/rush.","code":"$ type rush\nrush is /usr/local/lib/R/site-library/rush/exec/rush\n \n$ rush --help\n \n$ rush run '6*7'\n42\n \n$ rush run --tidyverse 'filter(starwars, species == \"Human\") %>% select(name)'\n# A tibble: 35 x 1\n   name\n   <chr>\n 1 Luke Skywalker\n 2 Darth Vader\n 3 Leia Organa\n 4 Owen Lars\n 5 Beru Whitesun lars\n 6 Biggs Darklighter\n 7 Obi-Wan Kenobi\n 8 Anakin Skywalker\n 9 Wilhuff Tarkin\n10 Han Solo\n# … with 25 more rows"},{"path":"list-of-command-line-tools.html","id":"sample","chapter":"List of Command-Line Tools","heading":"sample","text":"Filter lines standard input according probability, given delay, certain duration.\nsample\n(version 0.2.4)\nJeroen Janssens (2021).\ninformation: https://github.com/jeroenjanssens/sample.","code":"$ type sample\nsample is /usr/local/bin/sample\n \n$ sample --help\n \n$ seq 1000 | sample -r 0.01 | trim 5\n160\n300\n337\n340\n434\n… with 6 more lines"},{"path":"list-of-command-line-tools.html","id":"scp","chapter":"List of Command-Line Tools","heading":"scp","text":"OpenSSH secure file copy.\nscp\n(version 1:8.2p1-4ubuntu0.2)\nTimo Rinne Tatu Ylonen (2019).\ninformation: https://www.openssh.com.","code":"$ type scp\nscp is /usr/bin/scp\n \n$ man scp"},{"path":"list-of-command-line-tools.html","id":"sed","chapter":"List of Command-Line Tools","heading":"sed","text":"Stream editor filtering transforming text.\nsed\n(version 4.7)\nJay Fenlason, Tom Lord, Ken Pizzini, Paolo Bonzini (2018).\ninformation: https://www.gnu.org/software/sed.","code":"$ type sed\nsed is /usr/bin/sed\n \n$ man sed"},{"path":"list-of-command-line-tools.html","id":"seq","chapter":"List of Command-Line Tools","heading":"seq","text":"Print sequence numbers.\nseq\n(version 8.30)\nUlrich Drepper (2019).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type seq\nseq is /usr/bin/seq\n \n$ man seq\n \n$ seq 3\n1\n2\n3\n \n$ seq 10 5 20\n10\n15\n20"},{"path":"list-of-command-line-tools.html","id":"servewd","chapter":"List of Command-Line Tools","heading":"servewd","text":"Serve current working directory using simple HTTP server.\nservewd\n(version 0.1)\nJeroen Janssens (2021).\ninformation: https://github.com/jeroenjanssens/dsutils.","code":"$ type servewd\nservewd is /usr/bin/dsutils/servewd\n \n$ servewd --help\n \n$ cd /data && servewd 8000"},{"path":"list-of-command-line-tools.html","id":"shuf","chapter":"List of Command-Line Tools","heading":"shuf","text":"Generate random permutations.\nshuf\n(version 8.30)\nPaul Eggert (2019).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type shuf\nshuf is /usr/bin/shuf\n \n$ man shuf\n \n$ echo {a..z} | tr ' ' '\\n' | shuf | trim 5\ns\na\nk\nx\ny\n… with 21 more lines\n \n$ shuf -i 1-100 | trim 5\n61\n60\n5\n92\n10\n… with 95 more lines"},{"path":"list-of-command-line-tools.html","id":"skll","chapter":"List of Command-Line Tools","heading":"skll","text":"SciKit-Learn Laboratory.\nskll\n(version 2.5.0)\nEducational Testing Service (2021).\nactual tool run_experiment. use alias skll find easier remember.\ninformation: https://skll.readthedocs.org.","code":"$ type skll\nskll is an alias for run_experiment\n \n$ skll --help"},{"path":"list-of-command-line-tools.html","id":"sort","chapter":"List of Command-Line Tools","heading":"sort","text":"Sort lines text files.\nsort\n(version 8.30)\nMike Haertel Paul Eggert (2019).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type sort\nsort is /usr/bin/sort\n \n$ man sort\n \n$ echo '3\\n7\\n1\\n3' | sort\n1\n3\n3\n7"},{"path":"list-of-command-line-tools.html","id":"split","chapter":"List of Command-Line Tools","heading":"split","text":"Split file pieces.\nsplit\n(version 8.30)\nTorbjorn Granlund Richard M. Stallman (2019).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type split\nsplit is /usr/bin/split\n \n$ man split"},{"path":"list-of-command-line-tools.html","id":"sponge","chapter":"List of Command-Line Tools","heading":"sponge","text":"Soak standard input write file.\nsponge\n(version 0.65)\nColin Watson Tollef Fog Heen (2021).\nUseful want read write file single pipeline.\ninformation: https://joeyh.name/code/moreutils.","code":"$ type sponge\nsponge is /usr/bin/sponge"},{"path":"list-of-command-line-tools.html","id":"sql2csv","chapter":"List of Command-Line Tools","heading":"sql2csv","text":"Execute SQL query database output result CSV file.\nsql2csv\n(version 1.0.5)\nChristopher Groskopf (2020).\ninformation: https://csvkit.rtfd.org.","code":"$ type sql2csv\nsql2csv is /usr/bin/sql2csv\n \n$ sql2csv --help"},{"path":"list-of-command-line-tools.html","id":"ssh","chapter":"List of Command-Line Tools","heading":"ssh","text":"OpenSSH remote login client.\nssh\n(version 1:8.2p1-4ubuntu0.2)\nTatu Ylonen, Aaron Campbell, Bob Beck, Markus Friedl, Niels Provos, Theo Raadt, Dug Song, Markus Friedl (2020).\ninformation: https://www.openssh.com.","code":"$ type ssh\nssh is /usr/bin/ssh\n \n$ man ssh"},{"path":"list-of-command-line-tools.html","id":"sudo","chapter":"List of Command-Line Tools","heading":"sudo","text":"Execute command another user.\nsudo\n(version 1.8.31)\nTodd C. Miller (2019).\ninformation: https://www.sudo.ws.","code":"$ type sudo\nsudo is /usr/bin/sudo\n \n$ man sudo"},{"path":"list-of-command-line-tools.html","id":"tail","chapter":"List of Command-Line Tools","heading":"tail","text":"Output last part files.\ntail\n(version 8.30)\nPaul Rubin, David MacKenzie, Ian Lance Taylor, Jim Meyering (2019).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type tail\ntail is /usr/bin/tail\n \n$ man tail"},{"path":"list-of-command-line-tools.html","id":"tapkee","chapter":"List of Command-Line Tools","heading":"tapkee","text":"efficient dimension reduction library.\ntapkee\n(version 1.2)\nSergey Lisitsyn, Christian Widmer, Fernando J. Iglesias Garcia (2013).\ninformation: http://tapkee.lisitsyn..","code":"$ type tapkee\ntapkee is /usr/bin/tapkee\n \n$ tapkee --help"},{"path":"list-of-command-line-tools.html","id":"tar","chapter":"List of Command-Line Tools","heading":"tar","text":"archiving utility.\ntar\n(version 1.30)\nJohn Gilmore Jay Fenlason (2014).\ninformation: https://www.gnu.org/software/tar.","code":"$ type tar\ntar is /usr/bin/tar\n \n$ man tar"},{"path":"list-of-command-line-tools.html","id":"tee","chapter":"List of Command-Line Tools","heading":"tee","text":"Read standard input write standard output files.\ntee\n(version 8.30)\nMike Parker, Richard M. Stallman, David MacKenzie (2019).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type tee\ntee is /usr/bin/tee\n \n$ man tee"},{"path":"list-of-command-line-tools.html","id":"telnet","chapter":"List of Command-Line Tools","heading":"telnet","text":"User interface TELNET protocol.\ntelnet\n(version 0.17)\nMats Erik Andersson, Andreas Henriksson, Christoph Biedl (1999).\ninformation: http://www.hcs.harvard.edu/~dholland/computers/netkit.html.","code":"$ type telnet\ntelnet is /usr/bin/telnet"},{"path":"list-of-command-line-tools.html","id":"tldr","chapter":"List of Command-Line Tools","heading":"tldr","text":"Collaborative cheatsheets console commands.\ntldr\n(version 3.3.7)\nOwen Voke (2021).\ninformation: https://tldr.sh.","code":"$ type tldr\ntldr is /usr/local/bin/tldr\n \n$ tldr --help\n \n$ tldr tar | trim\n✔ Page not found. Updating cache...\n✔ Creating index...\n \n  tar\n \n  Archiving utility.\n  Often combined with a compression method, such as gzip or bzip2.\n  More information: https://www.gnu.org/software/tar.\n \n  - [c]reate an archive and write it to a [f]ile:\n    tar cf target.tar file1 file2 file3\n \n… with 22 more lines"},{"path":"list-of-command-line-tools.html","id":"tr","chapter":"List of Command-Line Tools","heading":"tr","text":"Translate delete characters.\ntr\n(version 8.30)\nJim Meyering (2018).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type tr\ntr is /usr/bin/tr\n \n$ man tr"},{"path":"list-of-command-line-tools.html","id":"tree","chapter":"List of Command-Line Tools","heading":"tree","text":"List contents directories tree-like format.\ntree\n(version 1.8.0)\nSteve Baker (2018).\ninformation: https://launchpad.net/ubuntu/+source/tree.","code":"$ type tree\ntree is /usr/bin/tree\n \n$ man tree\n \n$ tree / | trim\n/\n├── bin -> usr/bin\n├── boot\n├── data\n│   ├── ch02\n│   │   ├── fac.py\n│   │   └── movies.txt\n│   ├── ch03\n│   │   ├── logs.tar.gz\n│   │   ├── r-datasets.db\n… with 121572 more lines"},{"path":"list-of-command-line-tools.html","id":"trim","chapter":"List of Command-Line Tools","heading":"trim","text":"Trim output given height width.\ntrim\nJeroen Janssens (2021).\ninformation: https://github.com/jeroenjanssens/dsutils.","code":"$ type trim\ntrim is /usr/bin/dsutils/trim\n \n$ echo {a..z}-{0..9} | fold | trim 5 60\na-0 a-1 a-2 a-3 a-4 a-5 a-6 a-7 a-8 a-9 b-0 b-1 b-2 b-3 b-4…\nc-0 c-1 c-2 c-3 c-4 c-5 c-6 c-7 c-8 c-9 d-0 d-1 d-2 d-3 d-4…\ne-0 e-1 e-2 e-3 e-4 e-5 e-6 e-7 e-8 e-9 f-0 f-1 f-2 f-3 f-4…\ng-0 g-1 g-2 g-3 g-4 g-5 g-6 g-7 g-8 g-9 h-0 h-1 h-2 h-3 h-4…\ni-0 i-1 i-2 i-3 i-4 i-5 i-6 i-7 i-8 i-9 j-0 j-1 j-2 j-3 j-4…\n… with 8 more lines"},{"path":"list-of-command-line-tools.html","id":"ts","chapter":"List of Command-Line Tools","heading":"ts","text":"Timestamp input.\nts\n(version 0.65)\nJoey Hess (2021).\ninformation: https://joeyh.name/code/moreutils.","code":"$ type ts\nts is /usr/bin/ts\n \n$ echo seq 5 | sample -d 500 | ts\nDec 14 12:07:20 seq 5"},{"path":"list-of-command-line-tools.html","id":"type","chapter":"List of Command-Line Tools","heading":"type","text":"Show type location command-line tool.\ntype\nZ shell builtin.","code":"$ type type\ntype is a shell builtin\n \n$ man zshbuiltins | grep -A 10 '^ *type '"},{"path":"list-of-command-line-tools.html","id":"uniq","chapter":"List of Command-Line Tools","heading":"uniq","text":"Report omit repeated lines.\nuniq\n(version 8.30)\nRichard M. Stallman David MacKenzie (2019).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type uniq\nuniq is /usr/bin/uniq\n \n$ man uniq"},{"path":"list-of-command-line-tools.html","id":"unpack","chapter":"List of Command-Line Tools","heading":"unpack","text":"Extract common file formats.\nunpack\n(version 0.1)\nPatrick Brisbin (2013).\ninformation: https://github.com/jeroenjanssens/dsutils.","code":"$ type unpack\nunpack is /usr/bin/dsutils/unpack"},{"path":"list-of-command-line-tools.html","id":"unrar","chapter":"List of Command-Line Tools","heading":"unrar","text":"Extract files rar archives.\nunrar\n(version 0.0.1)\nBen Asselstine, Christian Scheurer, Johannes Winkelmann (2014).\ninformation: http://home.gna.org/unrar.","code":"$ type unrar\nunrar is /usr/bin/unrar\n \n$ man unrar"},{"path":"list-of-command-line-tools.html","id":"unzip","chapter":"List of Command-Line Tools","heading":"unzip","text":"List, test extract compressed files ZIP archive.\nunzip\n(version 6.0)\nSamuel H. Smith, Ed Gordon, Christian Spieler, Onno Linden, Mike White, Kai Uwe Rommel, Steven M. Schweda, Paul Kienitz, Chris Herborth, Jonathan Hudson, Sergio Monesi, Harald Denker, John Bush, Hunter Goatley, Steve Salisbury, Steve Miller, Dave Smith (2009).\ninformation: http://www.info-zip.org/pub/infozip.","code":"$ type unzip\nunzip is /usr/bin/unzip\n \n$ man unzip"},{"path":"list-of-command-line-tools.html","id":"vw","chapter":"List of Command-Line Tools","heading":"vw","text":"Fast machine learning library online learning.\nvw\n(version 8.10.1)\nJohn Langford (2021).\ninformation: https://vowpalwabbit.org.","code":"$ type vw\nvw is /usr/local/bin/vw\n \n$ vw --help --quiet"},{"path":"list-of-command-line-tools.html","id":"wc","chapter":"List of Command-Line Tools","heading":"wc","text":"Print newline, word, byte counts file.\nwc\n(version 8.30)\nPaul Rubin David MacKenzie (2019).\ninformation: https://www.gnu.org/software/coreutils.","code":"$ type wc\nwc is /usr/bin/wc\n \n$ man wc"},{"path":"list-of-command-line-tools.html","id":"which","chapter":"List of Command-Line Tools","heading":"which","text":"Locate command.\n\n(version 0.1)\nUnknown (2016).\ninformation: .","code":"$ type which\nwhich is a shell builtin\n \n$ man which"},{"path":"list-of-command-line-tools.html","id":"xml2json","chapter":"List of Command-Line Tools","heading":"xml2json","text":"Convert XML input JSON output, using xml-mapping.\nxml2json\n(version 0.0.3)\nFrançois Parmentier (2016).\ninformation: https://github.com/parmentf/xml2json.","code":"$ type xml2json\nxml2json is /usr/local/bin/xml2json"},{"path":"list-of-command-line-tools.html","id":"xmlstarlet","chapter":"List of Command-Line Tools","heading":"xmlstarlet","text":"Command line XML/XSLT toolkit.\nxmlstarlet\n(version 1.6.1)\nDagobert Michelsen, Noam Postavsky, Mikhail Grushinskiy (2019).\ninformation: https://sourceforge.net/projects/xmlstar.","code":"$ type xmlstarlet\nxmlstarlet is /usr/bin/xmlstarlet\n \n$ man xmlstarlet"},{"path":"list-of-command-line-tools.html","id":"xsv","chapter":"List of Command-Line Tools","heading":"xsv","text":"fast CSV command line toolkit written Rust.\nxsv\n(version 0.13.0)\nAndrew Gallant (2018).\ninformation: https://github.com/BurntSushi/xsv.","code":"$ type xsv\nxsv is /usr/bin/xsv\n \n$ xsv --help"},{"path":"list-of-command-line-tools.html","id":"zcat","chapter":"List of Command-Line Tools","heading":"zcat","text":"Decompress concatenate files standard output.\nzcat\n(version 1.10)\nPaul Eggert (2021).\ninformation: https://www.nongnu.org/zutils/zutils.html.","code":"$ type zcat\nzcat is /usr/bin/zcat\n \n$ man zcat"},{"path":"list-of-command-line-tools.html","id":"zsh","chapter":"List of Command-Line Tools","heading":"zsh","text":"Z shell.\nzsh\n(version 5.8)\nPaul Falstad Peter Stephenson (2020).\ninformation: https://www.zsh.org.","code":"$ type zsh\nzsh is /usr/bin/zsh\n \n$ man zsh"}]
